%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[a4paper,12pt]{report}
%%%%%% 12 times romanm A4 paper and equations left aligned       %%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\renewcommand{\baselinestretch}{1.5}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%       page setting     %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%-------------------A4
%\hsize=6.25in
%\vsize=9.65in
%-----------------------------------------------------------------------------------------
%--------------------- NOT RECOMMENDED  --------------------------------------------------
%Directives which define an amount of space to be skipped specify the ideal amount of space %to be skipped, and 'plus' and 'minus' values specify the permitted limits.
%\hsize=6.5in
%\vsize=8.9in
%\pretolerance=100
%\tolerance=200
%\parindent=20pt
%\parskip=0pt plus 1pt
%\hbadness=1000
%\vbadness=1000
%\baselineskip=12pt
%---------------------------------------------------------------------------------------
\textwidth  6.0in
\textheight 8.5in
\topmargin  0.0in
\oddsidemargin 0.0in
\evensidemargin 0.0in
\setlength{\parindent}{12pt}
%-------------------------- Packages Used ------------------------------------------
\hyphenation{}
\usepackage{subfigure}
\usepackage{latexsym}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{mathrsfs}
\usepackage{epsfig}
\usepackage{float}
\usepackage{graphicx}
\usepackage{inconsolata}
\usepackage{graphics}
\usepackage{caption}
\usepackage{rotating}
\usepackage{psfrag}
\usepackage{lscape}
\usepackage{longtable}
\usepackage{color}
\usepackage{epstopdf}
\usepackage{listings}
\usepackage{enumerate}
\usepackage{pdfpages}
\usepackage{amsthm}
\usepackage{amscd}
\usepackage{Sweave}
\usepackage[tableposition=top]{caption}
\usepackage{ifthen}
\usepackage[utf8]{inputenc}
%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[svgnames]{xcolor}
\usepackage{listings}
\lstset{language=R,
    basicstyle=\small\ttfamily,
    stringstyle=\color{DarkGreen},
    otherkeywords={0,1,2,3,4,5,6,7,8,9},
    morekeywords={TRUE,FALSE},
    deletekeywords={data,frame,length,as,character},
    keywordstyle=\color{blue},
    commentstyle=\color{DarkGreen},
}
%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{bm}
\usepackage{url}
\usepackage{longtable}

\setlength\topmargin{-1.5cm}
\setlength\oddsidemargin{0.0cm}
\setlength\evensidemargin{0.0cm}
\setlength{\textwidth}{16cm}
\setlength{\textheight}{25cm}
\newcommand{\blue} {\textcolor{blue}}
\newcommand{\red} {\textcolor{red}}

\newcommand{\tetn}{{\mbox{\boldmath $\theta$}}}
%\usepackage[T1]{fontenc}
%\usepackage[sc]{mathpazo}
%-----------------------------------------------------------------------------------
%              THEOREM Environments (Examples)
%-----------------------------------------------------------------------------------
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\theoremstyle{remark}
\newtheorem{remark}{Remark}%-----------------------------------------
%\newtheorem{defn}[thm]{Definition}
\newtheorem{prop}[theorem]{Proposition}
%\newtheorem{ex}[thm]{Example}
%\newtheorem{rem}[thm]{Remark}
%\newtheorem{cor}[thm]{Corollary}
%\newtheorem{ax}{Axiom}
%\theoremstyle{definition}
%\newtheorem{defn}{Definition}[section]
%\newtheorem{defn}{Definition}[section]
%-----------------------------------------
%\theoremstyle{remark}
%\newtheorem{rem}[thm]{Remark}
%\newtheorem{rem}{Remark}[section]
%-----------------------------------------
%\theoremstyle{example}
%\newtheorem{ex}{Example}[section]
%-----------------------------------------
%\newtheorem*{notation}{Notation}
%\numberwithin{equation}{section}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%     Start of Document           %%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\SweaveOpts{concordance=TRUE}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%     Title, author  page 1        %%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{\textbf{\Huge{Implementing R to Statistics
}}}
\author{By\\
\textbf{Muhammad Imran}\\
}
\date{}
\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%          Copy Rights            %%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Page setting counter in roman  %%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcounter{page}{0}
\pagenumbering{roman}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%            Declaration          %%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%            Approval             %%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%            Certificate          %%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%            Certificate          %%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%            Preface             %%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{Preface}
\addcontentsline{toc}{section}{Preface}{}
R is evolving into a vital tool for data science, machine learning, and statistical computing. It is popular among statisticians, data scientists, and applied researchers of different fields including biostatistics, healthcare, finance, risk management, social sciences, market research, environmental and climate research. R is an open-source programming language that is available for free and supports different operating systems. It offers an extensive range of packages that are useful for handling, analyzing, and data wrangling. R makes it possible to obtain sophisticated and elegant data visualization specially based on the ggplot2 R package.

\noindent R is inspired by the programming language S. Initially, R was developed as a computer language to teach basic statistics at the University of Auckland by professors Ross Ihaka and Robert Gentleman. It generally comes with the command line interface. However, there are several third-party graphical user interfaces available but RStudio, an integrated development environment (IDE) for R, is the most widely used. In addition, R Studio is accessible for several operating systems, including Windows, Linux, and macOS, and comes in desktop and server versions.

\noindent RStudio permits to work and operate different file formats including R script, C++ file, Python script, R Sweave and R Markdown, etc. R Markdown is a popular tool for producing well-structured, reproducible documents in formats including HTML, PDF, Microsoft Word, etc. It elegantly combines the text and code fragments into a single reproducible document.

\noindent The remarkable thing about R's flexibility is the immense amount of computational packages that are available to software repositories such as CRAN (the Comprehensive R Archive Network). These packages are simple to use and install because they include the necessary documentation in the form of a PDF reference manual.

\noindent This book helps in understanding the R fundamentals as well as implementing and practicing it in statistics. Even for those without extensive programming knowledge, it is fairly simple to use. Moreover, the book provides useful insights into the process of developing R packages.

\noindent  Chapter 1 covers the steps involved in the installation of R and RStudio. A basic introduction to RStudio, its console structure, its uses, data import, opening and manipulating new files in various formats, package installation, loading, version, and citation, including BibTeX style, installed package removal, package-related help, details on all installed packages, and library location path are also provided. The chapter ends with some basic illustrations of the R Markdown.

\noindent  To gain further understanding of R, Chapter 2 expands on a few core functions, such as vectors and their types, basic mathematical operations, data frames, and their structures, as well as the internal mathematical operations and handling they undergo, a descriptive overview of a data frame, sorting data frames, rounding numbers, the basic matrix and its structure, several internal operations, Rloops, and data import.

\noindent  Some significant positioning and variability measures form the foundation of Chapter 3.  We provide both manual and R scripts to compute a wide range of measures, such as descriptive statistics, mean, median, mode, geometric mean, harmonic mean, quartiles, standard deviation, variance, real moments, skewness, kurtosis, and the combined mean and variance.

\noindent  Chapter 4 is based on the fitting probability distributions, graphical representations of PDF and CDF and unknown parameter estimation, variance co-variance matrix, ordinary moments, mean moments, median and quartile deviation, distributional properties, Lorenz and Bonferroni, applications to real data, Kaplan-Maier plot, probability-probability plot, fitting of discrete probability distributions, binomial distribution, Poisson distribution, parameter estimation of discrete distribution, zero-inflated model, and zero-one-inflated models.

\noindent  Chapter 5 comprises time series analysis, time series data-based R packages, frequency of time series data, autocorrelation and partial autocorrelation, time series display, simple moving average with forecast and accuracy measures, graphical representation, ARIMA, SARIMA, ETS, ANN, and model comparison models.

\noindent  Simple regression model fitting, Pearson's correlation, multiple regression model and comprehending its output, multiple regression analysis assumptions, predicting particular values, deleting intercept from model, binary logistic regression fitting, and multiple logistic regression are all covered in Chapter 6.

\noindent Creating an R package is covered in Chapter 7. These include installing the roxygen2 R package, creating a NAMESPACE file based on roxygen2, resolving problems, warnings, and notes, testing and installing the package into the library, and, finally, creating a PDF reference.

\noindent The R package, metadata, and authoring of the DESCRIPTION file are covered in detail in Chapter 8. Chapter 9 is finally moderately grounded in R package development.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%     MSC 2000                    %%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%     Keywords                    %%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%     ACKNOWLEDGMENTS             %%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%     Dedication                 %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{Dedication}
\addcontentsline{toc}{section}{Dedication}{}

To,\\

\begin{center}
\indent\qquad\qquad {{\bf \huge{My Parents}}} \\
\end{center}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%     Appreciation               %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%     table of contents          %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%     add contents               %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\chapter*{Table of Contents}
\tableofcontents
\addcontentsline{toc}{section}{Contents}{}
\contentsline{chapter}{\numberline}{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%     List of Abbreviations      %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%     List of Symbols            %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%       Extras                   %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%     For Tables and Figures     %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%         Chapter 1              %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter*{About the Authors}

\textbf{Muhammad Imran}

\textbf{Michail Tsagris}

\textbf{Christophe Chesneau}

\textbf{Farrukh Jamal}

\addcontentsline{toc}{section}{About the Authors}{}

\pagenumbering{arabic}
\setcounter{page}{1}

\chapter{RStudio: A quick overview}
This chapter covers the steps involved in the installation of R and RStudio. It also provides a basic overview of RStudio, its console structure, and its uses. Additionally, it also shows how to assess the current working directory and how to change it. Moreover, it provides the file's details in the working directory as well as the total length of the files. Importing data, opening and working with new files of different formats, package installation, package loading, package version, package citation including BibTeX style, installed package removal, package related help, information of all installed packages, and library location path all are discussed for developing better understanding regarding RStudio and its further uses. Moreover, the opening of a R Markdown file, YAML header related settings, adding a table of contents with a section number, importing figures, and importing cross references are all shown. Finally, it is also demonstrated how to add citations to the R Markdown text by producing a new ref.bib file.

\section{Installing R and RStudio}

The R can be downloaded freely. To this end, go to \textbf{cran.r-project.org} and download it but take care of your operating system. R is available for three operating systems such as Linux, macOS, and Windows. After clicking the \textbf{Downloading R for Windows} (because in our case, we have Windows operating system) and then click on \textbf{install R for the first time}. To the upper left, click on \textbf{downloading R} and will start the downloading, it will take a few minutes depending upon your internet speed. Once the downloading is completed, open the downloaded file select the language (by default is English), and click \textbf{OK}. The new wizard is open with some important information and instructions after going through click \textbf{Next}. If the user is not interested in changing the default location then click \textbf{Next}. After that, the new wizard shows the components which you want to install select \textbf{64-bits User installation} (from the drop-down menu) and unchecked only the \textbf{32-bit Files} (because we are using 64-bit window operating system), then click \textbf{Next}. After that a new wizard is open with two options click on NO and then click Next. Again click Next, after selecting the additional shortcuts then click Next and finally, click \textbf{Finish}. Similarly, the RStudio can be downloaded using the following web address \textbf{www.rstudio.com}. After that click on the top right tab of \textbf{DOWNLOAD RSTUDIO} and then click \textbf{DOWNLOAD RSTUDIO} for desktop, once again click on \textbf{DOWNLOAD RSTUDIO DESKTOP FOR WINDOWS}. The download process will start and it will take a few minutes depending upon your internet connection speed, the file is around 212.78 MB. After downloading the installation process is straightforward. Open the downloaded file and click on \textbf{Next}. In the next step, the installation location directory keeps it by default and click \textbf{Next} and then click \textbf{Install} tab. The installation process will start and take some time to complete and finally, click on \textbf{Finish}.


\subsection{Overview of RStudio}

The R programming language is highly advanced and sophisticated. It has many applications and uses, particularly in the area of data analysis.  On the other hand, RStudio is a helpful tool that is an integrated development environment that makes R simple to handle and operate (code compilation to debugging) as well as import data of various types.  From the following Figure~\ref{CH1_0} the top left area is called the command console while the upper right area indicates the Environment, History, Connections, Tutorial, and Presentations, and the bottom right area shows the Files, Plots, Packages, Help, and Viewer.
\begin{figure}[H]
\begin{center}
\includegraphics[width=17cm,height=14.0cm]{CH1_0}~
\caption{RStudio interface.}\label{CH1_0}
\end{center}
\end{figure}

\subsection{Working directory}
Simply execute the getwd() function to find out the current working directory and as follows:
<<ch1>>=
getwd()
@
\noindent The working directory can be chosen and set by hitting Ctrl+Shift+H or by clicking on Session > Set Working Directory > Choose Directory can be used to select and set the working directory as shown in Figure~\ref{CH1_3}.
\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=14.0cm]{CH1_3}~
\caption{Choosing working directory.}\label{CH1_3}
\end{center}
\end{figure}
\noindent On the other hand, simply execute the setwd() function by mentioning the desired path to choose your working directory as follows:
<<ch1s, eval=FALSE>>=
setwd('path')
@
\noindent The function list.files() allows you to display the files inside your selected working directory. Whereas the length function evaluates the total number of files in the working directory. For instance, in our case, the working directory contained 193 files.
<<ch1l, eval=FALSE>>=
list.files()
@
<<ch1le>>=
length(list.files())
@

\subsection{Importing data}
R allows to support of a variety of data file formats, including Excel, SPSS, SAS, and Stata. These files can be imported as shown in Figs~\ref{CH1_4}-\ref{CH1_6}.
\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=14.0cm]{CH1_4}~
\caption{Importing data, step-1.}\label{CH1_4}
\end{center}
\end{figure}
\noindent We can import several file formats by selecting Import Dataset from the top right panel's drop-down menu. In our example, data from an Excel file containing eight years' worth of sales data (eight observations and two variables) was imported. According to Figure \ref{CH1_5}, the data is saved on the desktop with the file name data1. Following a browse of the data file (see Figure \ref{CH1_4}), the imported data is shown in Figure~\ref{CH1_6}.
\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=14.0cm]{CH1_5}~
\caption{Importing data, step-2.}\label{CH1_5}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=14.0cm]{CH1_6}~
\caption{Importing data, step-3.}\label{CH1_6}
\end{center}
\end{figure}

\subsection{Open new files}
Rstudio offers a variety of new files for us to work with; we may open and use new files like R script, RMarkdown, HTML files, C++ files, etc. We can view these new files by selecting File (upper left), then New File (see Figure~\ref{CH1_7}). We can also open the most recent Files. Similarly, we can rename, save, and save as the files.
\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=14.0cm]{CH1_7}~
\caption{Opening the new files.}\label{CH1_7}
\end{center}
\end{figure}

\subsection{Installing and loading R packages}\label{ins}
In the Rstudio setting, we can install and load R packages in different ways for example if we wish to install a package named formatR~\cite{1Yihui2023}. So we may run the install.packages ("formatR") command and then load the respective package with library(formatR) for further use. On the other side, we can install the package by simply clicking on the Packages (see Figure~\ref{CH1_1}), then click on Install and a new wizard is opened and write the desired package name that you want to install in the blank tab (see Figure~\ref{CH1_2}), likewise formatR in our case. Finally, click on the Install tab and it will take a few minutes to make sure your internet connection is on. Once the installation is completed click on the respective package for loading and further use as shown in Figure~\ref{CH1_2}.

<<ch1lib, eval=FALSE>>=
install.packages("formatR")
# after installation the package can be load for further use.
library(formatR)
@

\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=14.0cm]{CH1_1}~
\caption{Package installation and lodding, step-1.}\label{CH1_1}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=14.0cm]{CH1_2}~
\caption{Package installation and lodding, step-2.}\label{CH1_2}
\end{center}
\end{figure}
\subsection{Package version}
\noindent Similarly, we can assess the package version using packageVersion("formatR") command as follows:
<<ch1p>>=
packageVersion("formatR")
@
\subsection{Package citation}
\noindent We may want to cite the R packages in our study work because they are widely used in many practical domains. To this end, citation("formatR") is a very helpful command that offers the package citation. The citation () function also provides the citation for Latex users as follows:
<<ch1c>>=
citation("formatR")
@
\noindent The package help is useful when working with a specific package. Quick information on a specific package's documentation and functions is provided by the help() function and as follows:
<<ch1h>>=
help(package="formatR")
@
\noindent The library() function allows to obtain a list of all installed packages.
<<ch1ll, eval=FALSE>>=
library()
@
\noindent The .libPaths() function provides the R package library locations and as follows:
<<ch1lp, eval=TRUE>>=
.libPaths()
@
\subsection{Remove package}
Executing the following command will automatically remove or uninstall any installed packages. For instance, using the following command can effortlessly eliminate the carData package~\cite{John2022}.
<<ch1lb, eval=FALSE>>=
remove.packages("carData")
@

\section{R Markdown: A basic illustration}

It can be accomplished to develop high-quality reports using R Markdown. There are numerous static and dynamic output formats supported by R Markdown documents, which are entirely reproducible. To this end, we provide a simple and basic illustration to R Markdown for instance, opening R Markdown file, important settings regarding output formats, adding table of content and section numbers, importing figures and and cross referencing. The RMarkdown file can be opened by clicking on R Markdown as shown in Figure~\ref{CH1_7}. A new window will appear and important to choose your working options. For instance, in the top left corner, we have four option and as follows:
\begin{itemize}
\item Document
\item Presentation
\item Shiny
\item From Template
\end{itemize}

\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=14.0cm]{CH1_8}~
\caption{A basic illustration of R markdown, step-1.}\label{CH1_8}
\end{center}
\end{figure}

\noindent We can select either of one to start working with R Markdown (here we select Document). On the other side, upper right tab represents the title of your document, author name and date.

\begin{itemize}
\item Title (write here the title of your document)
\item Author (provides here the authors details with names)
\item Date (Current date when you rendering your document)
\end{itemize}

\noindent The R Markdown allows users to select the output format either one of the following:

\begin{itemize}
\item HTML (provides the HTML output format)
\item PDF (provides the PDF output format)
\item WORD (provides the WORD output format)
\end{itemize}

\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=14.0cm]{CH1_9}~
\caption{A basic illustration of R markdown, step-2.}\label{CH1_9}
\end{center}
\end{figure}
\noindent The YAML (yet another markup language) is enclosed with --- and based on title, author information and date. Moreover, the YAML header can also be changed according to user requirements. For instance, when writing report, the table of content is not included by default (if we need table of content just add toc: true in YAML header). By pressing Ctrl+Alt+I, the new code chunk can be added. The code chunk is a crucial component of R Markdown because it contains curly brackets that are based on many parameters, such as eval, echo, fig.cap, and others and play a significant role in the text. The results are compiled or rendered by hitting knit tab or Ctrl+Shift+K, it will take few seconds to produce a documents either in pdf, html or in word format. For more detail readers are referred to \cite{1Xie2016, 1Xie2018, 1Xie2020}

\subsection{Adding table of contents with section numbers}

\noindent We can add in our documents, a table of contents (toc) and sections with numbers by adding the YAML header attributes toc=true and number\_sections=true (pay close attention to the modifications in Figure~\ref{CH1_12}). The syntax is shown on the left side of the accompanying Figure~\ref{CH1_12}, while the HTML output is shown on the right.

\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=14.0cm]{CH1_12}~
\caption{Adding table of contents with section number.}\label{CH1_12}
\end{center}
\end{figure}

\subsection{Importing and citing figure}

\noindent Figure import is allowed into documents using R Markdown. To import the figures, follow the instructions as shown in Figure~\ref{CH1_13}. The YAML header can be slightly changed as bookdown$::$html\_document2:, for adding and citing figures. The code chunk in curly brakets consists of label (that is for citing figure in the text), the fig.cop="Importing figure." is used for figure caption.The figure can be cited using \@ref(fig:label).  The syntax is shown on the left side of the accompanying Figure~\ref{CH1_13}, while the HTML output is shown on the right. It is important to note that when importing a figure into R Markdown, the respective figure should be kept in the same folder as the R Markdown.RMD file has been saved.
\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=14.0cm]{CH1_13}~
\caption{Importing and citing figure.}\label{CH1_13}
\end{center}
\end{figure}

\subsection{Adding citations}
R Markdown allows to add citations in th text. To this end, first of all create a new text file and save it as ref.bib as shown in Figure~\ref{CH1_14}. After that open the ref.bib file and paste the BibTex reference and save the file (see Figure~\ref{CH1_15a}). Finally, in R Markdown file cite the respective reference in the text and follow the syntax as shown in Figure~\ref{CH1_15b}.
\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=14.0cm]{CH1_14}~
\caption{Adding citations, step-1.}\label{CH1_14}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=14.0cm]{CH1_15a}~
\caption{Adding citations, step-2.}\label{CH1_15a}
\end{center}
\end{figure}
\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=14.0cm]{CH1_15b}~
\caption{Addind citations, step-3.}\label{CH1_15b}
\end{center}
\end{figure}

\begin{thebibliography}{99}
\bibitem{John2022}
John Fox, Sanford Weisberg and Brad Price (2022). carData:
  Companion to Applied Regression Data Sets. R package
  version 3.0-5. https://CRAN.R-project.org/package=carData

\bibitem{1Yihui2023}
 Yihui Xie (2023). formatR: Format R Code
  Automatically. R package version 1.14.
  https://CRAN.R-project.org/package=formatR

\bibitem{1Xie2016}
Xie, Y. (2016). Bookdown: Authoring books and technical documents with R markdown. CRC Press.

\bibitem{1Xie2018}
  Xie, Y., Allaire, J. J., \& Grolemund, G. (2018). R markdown: The definitive guide. CRC Press.

\bibitem{1Xie2020}
  Xie, Y., Dervieux, C., \& Riederer, E. (2020). R markdown cookbook. CRC Press.

\end{thebibliography}































\chapter{R Fundamentals}
\section{Introduction}

This chapter elaborates on a few fundamental functions that are important to have more insight of R including vector and its types, basic mathematical operations, data frame and its structure along with its internal mathematical operations and handling, descriptive summary of data frame, sorting data frame, rounding numbers using round() function, trunc() function, ceiling() function and floor() function,  the basic of matrix, its structure and several internal operations such as summary of a matrix, cbind() and rbind() functions, apply() function, tcrossprod(), crossprod() and \%*\% functions, assigning name to column and row of a matrix, R loops, repeat loop, while loop, if else statement, replication function, handling missing values, removing warning message and importing data cvs file. The Chapter depends on the following R packages such as data.table~\cite{2Matt2023} and misty~\cite{2Takuya2023}. R is a popular open-source programming language for statistical computing and data analysis. It is compatible with many popular operating systems, including Windows, Linux, and macOS. The main objectives behind the development of R is statistical analysis and large-scale data management, both of which are extremely convenient to handle and process. Its numerous built-in features make it easy to do complex statistical analysis either descriptive statistics, fitting of the probability distributions and inferential analysis. R offers a wide range of packages that make complex analysis simple to execute and intuitive.

\subsection{Vector}
In R programming language, everything is treated as an object. Vector is based on the collection or formulation, or sequence of objects or items or observations, or elements of the same kind, and each element of vector is called its component and sperated by a comma. The vector formulation is carried out using the \textcolor{blue}{c()} function. For instance, we have $x$ <- c$(1, 2, 3)$ as a double data vector, it contains a similar numeric three components and combines into the vector using the \textcolor{blue}{c()} function and is separated by a comma. A data vector can have one or more similar sequences of items. We have basically, two types of vector that is atomic vector and list vector. Further, the atomic vector has six types namely logical, integer, double, character, complex, and raw vector. Numeric vectors are comprised of integer and double vectors. Lists or recursive vectors can include additional lists. There are two main features of data vectors and are as follows:
\begin{itemize}
\item Length of vector that can be evaluated using length() function.
\item the other is a type of vector that can be specified using typeof() function in R.
\end{itemize}
On the other hand, atomic vectors include homogeneous components whereas the items in the list are heterogeneous. Many statistical R packages are based directly or indirectly on data vectors, hence vectors play an important role in working effectively with R.  As a result, fundamental knowledge of data vectors and their kinds is required to better outputs and analysis. The typeof() function allows the specification of the given data vector type
\subsection{Types of Atomic Vectors}
\begin{itemize}
\item Logical: Logical data types have three possible values: True, False and and NA. These values are determined by which criterion is met. The logical vector is a vector that contains Boolean values.
\item Integer: In R, integer vectors are also known as numeric vectors. This contains both negative and positive whole values. Numbers in R are double by default, therefore to make an integer, add L after the number.
\item Double or numeric: In R, double vectors are also referred to as numeric vectors. Numbers are double by default in R.
\item Character: Character vectors are the most complex sort of atomic vector since each element is a string, and a string can hold any amount of data. In R, you can construct a character data type value in two ways: by using the as.character() function or by inputting a string between double quotes ("") or single quotes ('').
\item Complex: It contains the numbers and with an imaginary component.
\item Raw: The raw data type stores raw bytes, making it a particularly unique data type. For example, the charToRaw and intToBits functions can be used to convert a character object or an integer numeric value to a raw object.
\end{itemize}

<<ch2>>=
# You can find out whether a data object is atomic or not
is.atomic(50)
is.atomic("USA")
@
\noindent As logical data vector consists of three components namely True, False and and NA and it can be created as follows:
<<ch2L>>=
# Creating logical vector
L <- c(FALSE, FALSE, TRUE, NA)
L
typeof(L)
@
\noindent The integer vectors or numeric vectors can be created is as follows:
<<ch2i>>=
# Creating integer vector
I <- c(3L, 400L, 10L, 2L)
I
typeof(I)
@

\noindent The character vector can be created is as follows:
<<ch2cr>>=
# Creating character vector
c <- c("Hello", 'Hi')
c
typeof(c)
@
\noindent The complex vector can be created is as follows:
<<ch2c>>=
# Creating Complex vector
co <- c(4+3i, 7i, 7-4i, -1+2i)
co
typeof(co)
@
\noindent The double vector can be created is as follows:
<<ch2d>>=
# Creating double vector
d <- c(4, 5, 6, 7)
d
typeof(d)
@

\noindent The raw vector can be created is as follows:
<<ch2r>>=
# Creating double vector
r=raw(6)
r
typeof(r)
@

\subsection{Vector creation using : and seq() function}
A data vector can be created by using one of the following function
\begin{itemize}
\item c(a:b), where a is the beginning value and b is the ending value. For example, c(1:3) is a data vector of three components with a sequence beginning with 1 and ending with 3, that is, 1, 2, 3.
\item The seq($a$, $b$, by=$z$) function creates a data vector that starts at a and wraps up at b, with the difference between one component and another being $z$.
\item The seq($a$, $b$, length.out = $n$) function generates a data vector that begins at a and ends at b, with a length of n.
\end{itemize}
\noindent The above functions are used to create the data vector and as follows:
<<ch2x2>>=
# Vector creation using : function
x2 <- c(1:5)
x2
# Vector creation using seq() function
x3 <- seq(1, 9, by = 2)
x3
# Vector creation using seq() function with length.out argument
x4 <- seq(1, 12, length.out = 6)
x4
@

\subsection{Vector length}
The function length() is used to determine the length of a vector. The following Box shows the example of finding the length of data vector.
<<ch2x1>>=
# Finding the length of vector
x1 <- c(10, 20, 30, 40, 50)
n = length(x1)
n
# Finding the length of Character vector
x <- c("A", "B", "C")
n = length(x)
n
@
\subsection{List Vector}
A list is another approach to create a vector; it contains heterogeneous elements for example, a list can carry strings, numbers, vectors, and logical values all together.  Even a matrix or a function can also be utilized as list elements. The list vector is created simply by list() function and as follows:

<<ch2l>>=
list_vector <- list(c(4, 7, 20), "A", "B")
list_vector
@

\noindent R allows us to execute simple mathematical operations such as addition, subtraction, multiplication, and division. To put it another way, it can be used as a calculator. Here is an example of addition, subtraction, multiplication, and division.

\noindent \textbf{Addition:}
The operator "+" is used to combine or add up the given values or vector. Some basic examples of addition is as follows:
<<Addition>>=
# addition
2 + 2
x = 2
y = x + 1
y
z = x + y
z
@
\noindent \textbf{Subtraction:}
In R, the subtraction operation is performed by using the operator "-". The following Box contains the basic examples of subtraction operation and as follow:
<<Subtraction>>=
# subtraction
2 - 2
x = 2
y = x - 1
y
z = x - y
z
@
\noindent \textbf{Multiplication:} The operator "*" is used to multiply the given values or vector. The following Box contains the basic examples of multiplication operation and as follow:
<<Multiplication>>=
# multiplication
2 * 2
w = 5
p = w * 3
p
x <- c(1:5)
y = 2 * x
y
@
\noindent \textbf{Division:} The operator â/â is used to divide the given values or vector. The following Box contains the basic examples of division operation and as follow:
<<Division>>=
# division
2/2
2/0
w = 5
p = w/3
p
x <- c(1:5)
y = 2/x
y
@
\noindent \textbf{sqrt() function:}
The sqrt() function evaluates the square root of a given data vector and is as follows:
<<ch2sq>>=
sqrt(49)
sqrt(49)
# using data vector
x <- c(100, 4, 16)
sqrt(x)
@
\noindent \textbf{abs() function:}
The abs() method returns the absolute value. Negative values are converted to positive values, whereas positive values remain positive.
<<ch2abs>>=
abs(-2)
x = 49
abs(x)
# using data vector
x <- c(-100, 4, -16)
abs(x)
@

\section{Data frame}
A data frame is a general R data structure that is used to store tabular data. It is a  two-dimensional structure of data. Each component comprises a column, while its contents constitute rows. The length of each column should be the same. Here, we illustrate a simple example to formulate a data frame. Suppose that we have two vectors of male and female weights in pounds. Each vector is based on four components. The data frame can be constructed by data.frame() function and as follows
<<ch2male>>=
# male_W represents the vector of male weights
male_W = c(78, 89, 56, 90)
# female_W represents the vector of female weights
female_W = c(56, 65, 50, 70)
# Creating a data frame of male and female weights
GW <- data.frame(male_W, female_W)
GW
@
\subsection{Structure of data frame}
The function str() is quite useful to find out the structure of the data frame. The function provides the total number of observations in the data frame, the total number of variables used in the data frame, and also shows the variable name with assigned values
<<ch2str>>=
# structure of data frame (GW)
str(GW)
@


\subsection{The internal operation of the data frame}
The data frame's internal operations can be accomplished by utilizing square brackets $\left[\right]$. The $\left[1\right]$  and $\left[1,\right]$ represent the entire first column and entire first row, respectively, whereas the $\left[2\right]$ and $\left[2,\right]$ represent the entire second column and row, respectively. The colon operator, on the other hand, can be used to describe the range of columns and rows in the data frame, for example, $\left[1:2\right]$ and $\left[1:2,\right]$ show the first and second entire columns and rows, respectively in the data frame.\\

\noindent \textbf{Column Operation:} The following output summary shows the column operation within the data frame.
<<ch2gw>>=
# taking out a whole first column that is male weights
GW[1]
# taking out a whole second column that is female weights
GW[2]
# taking out whole both column that is male and female weights
GW[1:2]
@
\noindent \textbf{Row operation:} The following output summary shows the row operation within the data frame.
<<ch2we>>=
# taking out the first row of male and female weights
GW[1, ]
# taking out the second row of male and female weights
GW[2, ]
# taking out the first two rows of male and female weights
GW[1:2, ]
@
\subsection{Smmary of data frame}
Despite the fact that the data frame has several columns, the summary () function delivers a descriptive summary of each column. Each column's minimum, first quartile, median, mean, third quartile, and maximum values are listed in the summary. The following Box provides the summary of the GW data frame, it consists of two columns, the first one is male weights (male\_W) and the second is female weights (female\_W).
<<ch2sum>>=
summary(GW)
@
\subsection{Summary of specific column}
The data frame contains massive amounts of data and columns; however, we are not always interested in the entire data frame. In this case, we can analyze the various statistical operations on a specific column by appending a dollar sign to the variable of interest or the specified column. The box below summaries the GW data frame to the only specific male weights (male\_W) column. Similarly, we yields for female weights using summary(GW\$male\_W)
<<ch2sumgw>>=
# summary of individual column
summary(GW$male_W)
@



\subsection{Mean and SD of indivisual column of data frame}
Here, we illustrate the mean and standard deviation of the specific column and can be computed by using mean() and sd() functions and as follows:
<<ch2mean>>=
# mean of individual column
mean(GW$male_W)
mean(GW$female_W)
# standard deviation of individual column
sd(GW$male_W)
sd(GW$female_W)
@

\subsection{Transpose of data frame}
In many applied analysis, we often need a transpose of data frame. To this end, we use data.table~\cite{2Matt2023} R package to obtain the transpose of data frame with transpose() function and as follows:
<<ch2data>>=
data.table::transpose(GW)
@
\subsection{Sorting data frame }
Sorting data is crucial for many different applied analyses, including rank set sampling and order statistics, among others. In R, we can sort the data either by accessing it or by descending order of the data frame. The following is an easy illustration of how to array data.
<<ch2female>>=
# male_W represents the vector of male weights
male_W = c(78, 89, 56, 90)
# female_W represents the vector of female weights
female_W = c(56, 65, 50, 70)
# Creating a data frame of male and female weights
GW <- data.frame(male_W, female_W)
# The data is sorted in decreasing order
print(GW[order(GW$male_W, GW$female_W, decreasing = TRUE), ])
cat(crayon::red("The data is sorted in decreasing order"))
# The data is sorted in increasing order
print(GW[order(GW$male_W, GW$female_W, decreasing = FALSE), ])
cat(crayon::red("The data is sorted in increasing order"))
@

\subsection{Sorting data vector}
To sort atomic data vector, call the sort() function and supply the vector as an argument to this function. The vector is returned in increasing order by default. However, the vector can be sorted in either ascending or descending order using sort(x, decreasing = FALSE) or sort(x, decreasing = TRUE), respectively. The following Box shows the simple illustration of both orders
<<ch2sor>>=
 # sorting double vector
x <- c(20, 10, 25, 2, 8, 5, 7, 3)
# sorting double vector in descending order
d_order <- sort(x, decreasing = TRUE)
cat("Original Vector :", x, "\n")
cat("Sorted in decending order :", d_order)
# sorting double vector in ascending order
a_order <- sort(x, decreasing = FALSE)
cat("Original Vector :", x, "\n")
cat("Sorted in ascending order :", a_order)
@
\noindent Similarly, the Character vector can also be sorted using the above process and as follows:
<<ch2c22h>>=
# sorting Character vector
y <- c("A", "C", "B", "D", "A")
# sorting Character vector in descending order
d_order <- sort(y, decreasing = TRUE)
cat("Original Vector :", y, "\n")
cat("Sorted in decending order :", d_order)
# sorting Character vector in ascending order
a_order <- sort(y, decreasing = FALSE)
cat("Original Vector :", y, "\n")
cat("Sorted in ascending order :", a_order)
@
\section{Rounding numbers}
A data vector is rounded using a variety of functions. For instance, we can use the round(), signif(), trunc(), ceiling(), and floor() functions to round the values. These functions roles are separate from one another and each serves a particular purpose.
\subsection{Rounding numbers using signif() function}
The signif () function evaluates the rounded values to a fixed number of significant digits. For instance, the round() function returns 1.67 when rounding 1.666 to two decimals. On the other hand, applying the signif () function to two digits produces a result of 1.70.
<<ch2z1>>=
z <- c(1:5)
z1 <- z/3
z1
# rounding numbers to a specific number of significant digits
signif(z1, digits = 2)
@
\subsection{Rounding numbers using round() function}
The function allows for the rounding of a single value or a data vector to a predetermined number of decimal places. In the Box that follows, the round() function is illustrated. When attempting to understand the distinctions between round () function and signif () function, we frequently conflate the two words. The primary difference is that signif(2.333, digits=2) provides 2.30 whereas round(2.333, digits=2) yields 2.33.
 <<ch2round>>=
# values are rounded to the number of decimal places you choose
round(z1, digits = 2)
@
\subsection{Rounding numbers using trunc() function}
In R programming, the trunc() function is very straightforward. It only removes the decimal points from the provided data vector without doing anything else. For instance, the output of the function trunc(x) for the data vector x  <-  c(1.22, 1.555, 2.566) is as follows: The function simply removes the decimal points from each component of the data vector x, 1, 1, and 2.
<<ch2trun>>=
# values are rounded and cutoff the decimal point
trunc(z1, digits = 2)
@

\subsection{Rounding numbers using ceiling() function}
The ceiling () function refers to a numeric input that is rounded up to the next higher value.
<<ch2ce>>=
ceiling(z1)
@

\subsection{Rounding numbers using floor() function}
The floor function reduces a numeric argument to the next lower value.
<<ch2floor>>=
floor(z1)
@

\section{Matrix}
A two-dimensional representation of data having columns and rows is called a matrix. A row is a horizontal representation of data, whereas a column is a vertical representation. The order of the matrix is the number of rows and number of column in a matrix. The syntax of a matrix is as follows: matrix(vector, nrow, ncol, byrow), where vector (represents the data vector), nrow (represents the number of rows in a matrix), and ncol(represents the number of columns in the matrix). By default, the matrix is filled column-wise, but by byrow option, the matrix row-wise and column-wise operation can be controlled by just replacing byrow=TRUE or byrow=FALSE. The following Box shows a simple example to understand the basics of creating a matrix in R.

<<ch2mat>>=
x <- c(1:9)
matrix(x, nrow = 3, ncol = 3, byrow = FALSE)
matrix(x, nrow = 3, ncol = 3, byrow = TRUE)
@

\subsection{Summary of a matrix}
The summary () function can also be used to compute a matrix's-basic statistics (minimum value, first quartile, median, mean, third quartile, and maximum values). It can be used byrow=True or byrow=FALSE allowing control of the summary function's row and column direction. The summary () function is demonstrated simply in the following Box:
<<ch2z>>=
x <- c(1:9)
z = matrix(x, nrow = 3, ncol = 3, byrow = FALSE)
# original matrix
z
# the summary of a matrix column-wise
summary(z)

v <- matrix(x, nrow = 3, ncol = 3, byrow = TRUE)
# the summary of a matrix row-wise
summary(v)
@
\subsection{Inside matrix operation}
As a matrix is based on rows and columns. In R, using the vector index operator [], we can manipulate a single value or a whole column and row. Here are a few keys of the matrix inside the operation. Suppose $z$ be a given matrix
\begin{itemize}
\item z[,1] means first whole column of a given matrix
\item z[1,] means first whole row of a given matrix
\item z[1,2] means the first value of first row and second column of a given matrix
\item z[,c(1,2)] means the whole first and second column
\item z[c(1,2),] means the whole first and second row
\end{itemize}
<<ch2zzz>>=
x <- c(1:9)
z = matrix(x, nrow = 3, ncol = 3, byrow = FALSE)
# First column of a given z matrix
z[, 1]
# First and second column of given z matrix
z[, c(1, 2)]
# summary of individual column
summary(z[, 1])
# First row of a given z matrix
z[1, ]
# First and second row of given z matrix
z[c(1, 2), ]
@
\subsection{Replacing specific value in matrix}
To do this, one can utilize the address of the row and column in a matrix where a certain value is to be inserted. For instance, $z$ matrix of order 3$\times$3 can be positioned as follows:
<<ch2matz>>=
x <- c(1:9)
z = matrix(x, nrow = 3, ncol = 3, byrow = FALSE)
z
@
\noindent The address of the $z$ matrix z[1,2] = 500 can be used to swap out 4 for 500.
<<ch2chaning>>=
# changing the first row's and second column's values to 500
z[1,2] = 500
z
@

\subsection{Specific row or colum elimination}
If  $z$ be a given matrix, then a specific row or column can be eliminated from the given matrix. A simple example is as follows:

\begin{itemize}
\item z[-c(1),] means removing first whole row of a given matrix
\item z[-c(1,2),] means removing first and second whole row of a given matrix
\item z[,-c(1)]  means removing first whole column of a given matrix
\item z[,-c(1,2)]  means removing first and second whole column of a given matrix
\end{itemize}
<<ch2zmat>>=
x <- c(1:9)
z = matrix(x, nrow = 3, ncol = 3, byrow = FALSE)
z
# elimination of first fow from the given matrix
z[-c(1), ]
# elimination of first column from the given matrix
z[, -c(1)]
@
\subsection{dim(), length() and  \%in\% functions}
In matrix operation, the following three functions can be used for specific purposes and as follows:
\begin{itemize}
\item The dim() function can be used to determine the matrix's number of rows and columns.
\item The length() function can be used to evaluate the total number of item or elements in the matrix.
\item The \%in\% function is used to determine whether a particular element is present in the given matrix. If the element is present in the specified matrix, the function returns TRUE; otherwise, it returns FALSE.
\end{itemize}
<<ch2dim>>=
x <- c(1:9)
z = matrix(x, nrow = 3, ncol = 3, byrow = FALSE)
z
"8" %in% z
dim(z)
length(z)
@
\subsection{cbind() and rbind() functions}
The cbind() and rbind() functions are used to join two or more matrices, respectively. As an illustration, we have two 3x3 order matrices, z and z1. The z and z1 are combined using the method rbind() to bring the total number of rows up to 6 (three rows of matrix z and three rows of matrix z1). The z and z1 are also combined using the method cbind(), bringing the total number of columns to 6 (three columns of matrix z and three columns of matrix z1).
<<ch2dinn>>=
x <- c(1:9)
z = matrix(x, nrow = 3, ncol = 3)
y <- c(10:18)
z1 = matrix(y, nrow = 3, ncol = 3)
Combine <- rbind(z, z1)
Combine
Combine <- cbind(z, z1)
Combine
@
\subsection{Transpose of a matrix}
In R, the t() function can be used to obtain a matrix's transpose. The transposed matrix is given by
<<ch2trans>>=
x <- c(1:9)
z = matrix(x, nrow = 3, ncol = 3, byrow = FALSE)
z
# transpose of a matrix
t(z)
@
\subsection{apply() function}
Let $z$ be the matrix of order $3\times3$ (3 rows and 3 column). The apply() function can be used to sum up individual row and column. The function is as follows apply($z$,1,sum), where "$z$" is given matrix under operation, "1" indicating the row-wise operation of "$z$" matrix and "sum" denotes the sum up the values of each row as shown in Eq.~\ref{apply}. For example the first row of the given $z$ matrix that is [1+4+7=12]. Similarly, the sum of second and third row are 15 and 18, respectively (see Eq.~\ref{apply}). The output summary in the following Box yields the sum of each row as 12, 15, and 18 after carrying apply function.
\begin{equation}\label{apply}
z=\left[\begin{array}{ccc}
1 & 4 & 7\\
2 & 5 & 8\\
3 & 6 & 9
\end{array}\right]=\left[\begin{array}{c}
1+4+7=12\\
2+5+8=15\\
3+6+9=18
\end{array}\right].
\end{equation}
<<apply>>=
x <- c(1:9)
z = matrix(x, nrow = 3, ncol = 3, byrow = FALSE)
z
# using apply function
apply(z,1,sum)
@
The addition operation to each column can be carried out using apply(z,1,sum), where "$z$" is given matrix under operation, "2" indicating the column-wise operation of "$z$" matrix and "sum" denotes the sum up the values of each column as shown in Eq.~\ref{apply1}.
\begin{equation}\label{apply1}
z=\left[\begin{array}{ccc}
1 & 4 & 7\\
2 & 5 & 8\\
3 & 6 & 9\\
\end{array}\right]
\end{equation}
<<ch2apply>>=
x <- c(1:9)
z = matrix(x, nrow = 3, ncol = 3, byrow = FALSE)
z
apply(z, 2, sum)
@
\subsection{tcrossprod(), crossprod() and \%*\% functions}
Here, we show the uses of tcrossprod(), crossprod() and \%*\% functions. The tcrossprod() function is simply the product of given matrix with its transpose. The transpose of a matrix can also be obtained using t() function. The following Eq.~\eqref{tcrossprod}, shows the product of given matrix $A$ with its transpose $A^{t}$. By comparing the resulting output is shown in Eq.~\eqref{tcrossprod} and tcrossprod(A) in the following box yield same results.
\begin{equation}\label{tcrossprod}
AA^{t}=\begin{bmatrix}1 & 3\\
2 & 4
\end{bmatrix}\begin{bmatrix}1 & 2\\
3 & 4
\end{bmatrix}=\begin{bmatrix}1\times1+3\times3 & \,\,\,\,\,1\times2+3\times4\\
2\times1+4\times3 & \,\,\,\,\,2\times2+4\times4
\end{bmatrix}=\begin{bmatrix}10 & 14\\
14 & 20
\end{bmatrix}.
\end{equation}
<<ch2tccr>>=
A <- matrix(c(1:4), nrow = 2)
A
tcrossprod(A)
@
\noindent The product of a two matrix can also be obtained using \%*\% function and as follows:
<<ch2sprod>>=
A <- matrix(c(1:4), nrow = 2)
A
tcrossprod(A)
# The tcrossprod can also be obtained using \%*\% function
A %*% t(A)
@
\noindent The other function that is crossprod(), before directly computing using R. Here, we illustrate manually computation procedure. Suppose, $A=\left[\begin{array}{cc}
1 & 3\\
2 & 4
\end{array}\right]$, the crossprod can be computed as follows:
\begin{equation}
\begin{bmatrix}1 & 2\end{bmatrix}\begin{bmatrix}1 & 3\\
2 & 4
\end{bmatrix}=\begin{bmatrix}1\times1+2\times2\\
1\times3+2\times4
\end{bmatrix}=\begin{bmatrix}5\\
11
\end{bmatrix},
\end{equation}
and
\begin{equation}
\begin{bmatrix}3 & 4\end{bmatrix}\begin{bmatrix}1 & 3\\
2 & 4
\end{bmatrix}=\begin{bmatrix}3\times1+4\times2\\
3\times3+4\times4
\end{bmatrix}=\begin{bmatrix}11\\
25
\end{bmatrix}.
\end{equation}

\begin{equation}
A=\begin{bmatrix}5 & \,\,\,\,11\\
11 & \,\,\,\,25
\end{bmatrix}.
\end{equation}
<<ch2cross>>=
A <- matrix(c(1:4), nrow = 2)
A
crossprod(A)
@

\subsection{Assigning name to column and row of a matrix}
The two functions, colnames() and rownames(), are used to assign a specific name to the column and row of the given matrix.
<<ch2hhh>>=
a <- c(1:9)
h <- matrix(a, 3, 3)
h
# giving name to each column as C1, C2 and C3
colnames(h) <- c("C1", "C2", "C3")
h
# giving name to each row as R1, R2 and R3
rownames(h) <- c("R1", "R2", "R3")
h
@

\section{R loop}
Time savings, fewer mistakes, and easier to read code are all benefits of loops. If we wish to repeat a series of actions multiple times, we use a loop. R will carry out the instructions within a loop till a predetermined number of times or until a predetermined condition is satisfied. In R, there are three primary forms of loops: for, repeat, and while loops.

\subsection{For loop}
One of the key components of R's control flow structure is the for loop. Applying the same set of operations to every element of a specified data structure allows you to repeatedly go over a group of objects, like a vector, list, matrix, or dataframe.
<<ch2example>>=
# a simple example of for loop
x <- c(1:5)
for (i in x) {
    print(i^2)
}
@
<<ch2loop>>=
# for loop based on the list vector
x <- list("a", "b", "c")
x
for (i in x) {
    print(i)
}
@
<<ch2for>>=
x <- c(2, 4, 6, 8, 10)
y <- numeric(5)
for (i in 1:5) {
    y[i] <- (x[i])^2
}
y
# here, we can use the data come through for loop
summary(y)
@

\subsection{For loops with next and break statement}

The break statement allows to stop an iteration at particular component. For example, in the following box, when $i=8$, the for loop just stopped.
<<ch2rep>>=
# Stop the loop using break function
x <- c(2, 4, 6, 8, 10)
for (i in x) {
    if (i == 8) {
        break
    }
    print(i)
}
@
\noindent The next statement allows to skip an iteration without stopping the loop. For example, in the following box, when $i=8$, the for loop just spiking the 8 from data vector and then keeping going on.
<<ch2skip>>=
# Skip an iteration without stopping the loop
x <- c(2, 4, 6, 8, 10)
for (i in x) {
    if (i == 8) {
        next
    }
    print(i)
}
@
\subsection{If else statement}
We can write a decision-making program in R by using the if else statement. The following is the simple illustration to understand the if else statement:
<<ch2num>>=
number <- c(1, 4, 5, 7, 9)

for (i in number) {
    if (i == 4) {
        print(paste("The number is", i, "Even!"))
    } else {
        print(paste("The number is", i, "Odd"))
    }
}
@
<<ch2if>>=
x <- 6
y <- 8
if (x > y) {
  print("x is greater than y")
} else {
  print("x is less than y")
}
@
\subsection{For loops over matrix elements}
R facilitates matrix loop operations and is provided by
<<ch2row>>=
matrix <- matrix(c(1:4), nrow = 2, ncol = 2)
matrix
for (row in 1:nrow(matrix)) {
    for (col in 1:ncol(matrix)) {
        print(matrix[row, col])
    }
}
@
<<ch2collo>>=
mat <- matrix(c(1:4), nrow = 2, ncol = 2)
for (i in 1:dim(mat)[1]) {
    for (j in 1:dim(mat)[2]) {
        mat[i, j] = i + j
    }
}
mat
@

\subsection{While loops}
When it's unclear how many times a certain block of code should be repeated, while loops are employed. We can use the while loop to run a series of statements if a certain condition is met. The most basic representation of a while loop is as follows: while$(x<5)$ indicates that the loop will continue to run until the specified condition $x<5$ is satisfied.
<<ch2less>>=
x <- 0
while (x < 5) {
    print(x)
    x <- x + 1
}
@

\noindent Even if the while condition is TRUE, we can end the loop with the break statement:
<<ch2while>>=
i <- 0
while (i <= 5) {
    if (i == 4) {
        break
    }
    print(i)
    i = i + 1
}
@

\noindent The following next statement allows us to skip iteration without breaking the loop:

<<ch2whileif>>=
i <- 0
while (i < 5) {
    if (i == 3) {
        i = i + 1
        next
    }
    print(i)
    i = i + 1
}
@
\subsection{Repeat loops}
In R, a repeat loop is used to repeatedly iterate over a block of code. Additionally, it keeps running the same code until a break statement is encountered.
<<ch2peat>>=
x <- 0
repeat {
    print(x)
    x = x + 1
    if (x >= 4) {
        break
    }
}
@
\subsection{rep function}
The rep() function in R can be used to repeat items of vectors or lists a predetermined number of times. For better understanding, the following are some useful examples of rep() function:
<<ch222>>=
# replicate 0 to ten times
rep(0, times = 10)
@
<<ch211>>=
# data vector
x <- c(100, 200, 500)
# replicate data vector x twice
rep(x, times = 2)
@
<<ch2vex>>=
# data vector
x <- c(100, 200, 500)
# replicate each component of data vector x three times
rep(x, each = 3)
@

<<ch2100>>=
# data vector
x <- c(100, 200, 500)
# each value in the vector is repeated a specific number of times
rep(x, times = c(2, 4, 6))
@

<<ch2vdata>>=
# data vector
x <- c(100, 200)
# replicate data vector x three times, twice for each component
rep(x, each = 2, times = 3)
@

<<ch2cxe>>=
x <- list("a", "b")
rep(x, times = 2)
@
\subsection{replicate function}
The replication () is another useful function to replicate items n times after specifying  $n$.
<<ch2cast>>=
replicate(n = 6, rexp(5, 0.5))
@


\section{Handling missing values}
R makes it possible to handle missing values, which are frequently encountered in data. For example, we can use the is.na() function to check for missing values in the data; if a missing value is found, the function returns TRUE. While the which(is.na()) function returns the count position of a "NA" value, as shown in the following output, the any(is.na()) function also returns TRUE if the data contain a "NA" value
<<ch2na>>=
x <- c(1, 2, 3, 4, NA)
is.na(x)
# checking of all data vector for any NA
any(is.na(x))
# position of NA
which(is.na(x))
@
\noindent With the use of the following function, the "NA" values in the provided data vector can be
eliminated, which also reduces the size of the final data vector. The following data vector,
which includes two "NA" entries, contains seven values for a straightforward illustration.
After removal, the vector that remains has five values, which are provided by

<<ch2removing>>=
x <- c(1, 2, 3, 4, NA, 10, NA)
# removing NA from the data vector
remove <- x[!is.na(x)]
remove
@
\noindent The other way to remove "NA" values is na.exclude () function which also evaluates the
position of "NA" values and given by
<<ch2tion>>=
x <- c(1, 2, 3, 4, NA, 10, NA)
# removing NA from the data vector
remove <- na.exclude(x)
remove
@
\noindent The na.omit() function is an other method of eliminating "NA" values. It likewise assesses
the location of "NA" values and is provided by
<<ch2bblom>>=
x <- c(1, 2, 3, 4, NA, 10, NA)
# removing NA from the data vector
remove <- na.omit(x)
remove
@
\subsection{Replacing NA values in a data vector}
Frequently, we would want to replace "NA" values rather than eliminate them. The
following codes assist in doing so by making "NA" values equal to zero in the example
below.
<<ch2naw1>>=
x <- c(1, 2, 3, 4, NA, 10, NA)
replace <- x
replace[is.na(replace)] <- 0
replace
@
\subsection{Replacing NA values in a data frame}
The other way of replacing missing values in the given data frame using misty R package and as follows:
<<ch2mmko>>=
x <- c(10, 35, NA, 72, 100)
# replace NA with 28
misty::na.as(x, na = 28)
@
\subsection{Replacing NA values in a matrix}
The other way of replacing missing values in the given matrix using misty R package and as follows:
<<ch2nhuo>>=
data <- c(2, NA, 1, 3)
x <- matrix(data, 2, 2)
# replace NA with 28
misty::na.as(x, na = 28)
@

\subsection{Frequancy distribution with NA values}
Despite the availability of "NA" values, the misty~\cite{2Takuya2023} library's freq() function makes it
possible to obtain the frequency distribution and given by
<<ch2xdxcf>>=
data <- data.frame(x1 = c(1, 1, 2, 2, 3,
    1, 4, 3, 4, NA), x2 = c(2, NA, 3, 6,
    8, 7, 8, 3, 2, NA))
misty::freq(data$x2, val.col = TRUE)
@
\noindent If we have more than one variable with "NA", the frequency distribution using freq()
function can also be yielded as follows:
<<ch2dungh>>=
data <- data.frame(x1 = c(1, 1, 2, 3,
    1, 4, NA), x2 = c(2, NA, 6,
    2, 3, 2, NA))
misty::freq(data[, c("x1", "x2")], print = "all",
    val.col = TRUE)
@


\subsection{Descriptive summary with NA values}
Despite the availability of "NA" values, the misty [2] library's descript() function makes
it possible to obtain the descriptive summary and given by
<<ch2gibtn>>=
data <- data.frame(x1 = c(1, 2, 3, 4, NA),
    x2 = c(2, NA, 23, 6, 8), x3 = c(45, NA,
        23, 40, 30))
misty::descript(data$x1)
@
\noindent More than one variable descriptive summary with NA values can also be obtained and
follows:
<<ch2frame>>=
data <- data.frame(x1 = c(1, 2, 3, 4, NA),
    x2 = c(2, NA, 23, 6, 8), x3 = c(45, NA,
        23, 40, 30))
misty::descript(data[, c("x1", "x2", "x3")])
@

\subsection{Removing warnings}
The suppressWarnings() function allows to remove warnings form output and given by
<<ch2super>>=
log(-1)
# removing warnings
suppressWarnings({
    log(-1)
})
@
\subsection{Data importing}
The data can be importing into R scrip using read.csv () function of excel cvs file. After executing the following command, one can import data file and given by
<<ch2cdfrw, eval=FALSE>>=
read.csv(file.choose(), header = T)
@





\begin{thebibliography}{99}

\bibitem{2Matt2023}
Matt Dowle and Arun Srinivasan (2023). data.table:
  Extension of `data.frame`. R package version 1.14.8.
  https://CRAN.R-project.org/package=data.table

\bibitem{2Takuya2023}
Takuya Yanagida (2023). misty: Miscellaneous Functions 'T. Yanagida'. R package
version 0.5.2. https://CRAN.R-project.org/package=misty



\end{thebibliography}



\chapter{Positioning and Variability Measures}
\section{Introduction}
This chapter is based on some important positioning and variability measures. The positioning or location measures provide information on the center of the data, whereas the dispersion or variability measures show how the values deviate from the center. We provide the manual as well as R scripts in computing of multiple measures, such as descriptive statistic, mean, median, mode, geometric mean and harmonic mean, quartiles, standard deviation, variance, ordinary moments, actual moments, skewness, kurtosis, the combine mean and variance. Moreover, the Chapter depends on the following R packages namely modeest~\cite{Paul2019}, psych~\cite{William2023}, moments~\cite{Lukasz2022}, fishmethods~\cite{Gary2023} and gds~\cite{Partha2021}.


\subsection{Min, Max and Length of data set}
For a given data vector $x$, we can find the minimum value, maximum value and the length of data set using min(), max() and length() functions in R, respectively. These functions can be used and as follows:

<<a>>=
x <- c(9, 8, 10, 7, 6, 8, 3, 7, 4, 9, 3, 7, 8)
min(x)
max(x)
length(x)
@

\subsection{Descriptive statistic}
The descriptive summary of the given data set can be obtained using summary() function. It induces the minimum observation (min.), the lower quartile (Ist Qu.), the median, the mean, the upper quartile (3rd Qu.) and the maximum observation of the given vector of data set. The $x$ represent the data vector. Following is a summary of the data:
<<des , eval = T>>=
summary(x)
@
\noindent A box plot can also be used to obtain a descriptive summary of the data, as illustrated in Figure.~\ref{fig:box}.
<<box,fig.cap="A box plot with descriptive summary.",fig.align='center'>>=
boxplot(x, col = "green", main = "", axes = TRUE, staplewex = 1)
text(y = boxplot.stats(x)$stats, labels = boxplot.stats(x)$stats,
x = 1.3, col = "red")
@

\subsection{Frequency distribution}
Frequency distributions are tables in which each value of a variable is related with its respective frequencies. It is denoted by $F$ or $f$. For a given data vector, the frequency distribution can be obtained using table() function and given by
<<FD , eval = T>>=
x<-c(2, 2, 2, 2, 2, 2, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 5)
table(x)
@
\subsection{Cumulative frequency distribution}
The cumulative frequency distribution is important in finding median, quartiles, deciles and percentiles and it can be obtained using cumsum() function.
<<CFD , eval = T>>=
cumsum(table(x))
@


\subsection{Mode}
A value which occur maximum times in a given set of data is called mode. Some time the data set has no mode or some time it has more than one model values. The modeest~\cite{Paul2019} R package is used to evaluate the modal for a given data vector. The mfv function is specifically used to compute the modal value. Three data sets are used to evaluate the mode and as follows:
<<Mode>>=
# Different two data sets
x <- c(1, 3, 6, 3, 45, 67, 3, 65)
modeest::mfv(x)
x <- c(10, 10, 10, 2, 5, 4, 7, 8, 9, 8, 8)
modeest::mfv(x)
@
\noindent A data set that has only one mode is referred to as unimodal, two modes is referred to as bimodal, and three modes is referred to as trimodal. On the other hand, a multimodal data set has four or more modes.
\subsection{Percentile}
A value which divides the arrange set of data into 100 equal part is called percentile. The median, second quartile, fifth decile and 50th percentile are equal. From the following vector of data, the 75th percentile is as follows:
<<ch33>>=
x <- c(3, 4, 2, 6, 3, 7, 6)
# the median is given by
median(x)
# the 50th percentile is given by
quantile(x, c(0.5))
# the 75th percentile is given by
quantile(x, c(0.75))
@

\subsection{Hormonic mean}
Harmonic mean (H.M) is the reciprocal of the arithmetic mean of the reciprocals of the observations. It is given by
\begin{equation}
\text{H.M}=\frac{n}{\frac{1}{x_{1}}+\frac{1}{x_{2}}+\cdots+\frac{1}{x_{n}}}=\frac{n}{\sum_{i=1}^{n}\left(\frac{1}{x_{i}}\right)},
\end{equation}

\noindent where $n$ is the total number of values in the given data set. Suppose, we have a data set of five values that is 2, 4, 6, 8, 10. The H.M can be computed as follows:
\begin{equation}
\text{H.M}=\frac{5}{\frac{1}{2}+\frac{1}{4}+\frac{1}{6}+\frac{1}{8}+\frac{1}{10}}=\frac{5}{1.141667}=4.379562.
\end{equation}
The H.M can be computed using harmonic.mean () function from psych~\cite{William2023} library.
<<ch333, message=FALSE, warning=FALSE>>=
x1 <- c(2, 4, 6, 8, 10)
psych::harmonic.mean(x1)
@
\noindent If any value in a given data series is zero, it is impossible to find the H.M of that series because there is no reciprocal for zero.

\subsection{Geomatric mean}
The $n$th root of the product of the values determines the Geometric mean (G.M) of a series with $n$ observations and given by
\begin{equation}
\text{G.M}=\sqrt[n]{x_{1}\times x_{2}\times\cdots\times x_{n}}=\left(x_{1}\times x_{2}\times\cdots\times x_{n}\right)^{1/n},
\end{equation}
where $n$ is the total number of values in the given data set. Taking log on both sides, we get

\begin{equation}
\log\text{G.M}=\frac{1}{n}\left(\log x_{1}+\log x_{2}+\cdots+\log x_{n}\right)=\frac{1}{n}\left(\sum_{i=1}^{n}\log x_{i}\right).
\end{equation}
Hence, the G.M is as follows:

\begin{equation}\label{gm}
\text{G.M}=\text{Antilog}\left[\frac{1}{n}\left(\sum_{i=1}^{n}\log x_{i}\right)\right].
\end{equation}

\begin{table}[H]
  \centering
  \caption{G.M computation.}
    \begin{tabular}{ccccccc}
    \hline
    x     & 2     & 4     & 6     & 8     & 10    &  \\
    \hline
    log(x)  & 0.69315 & 1.38629 & 1.79176 & 2.07944 & 2.30259 & 8.25323 \\
    \hline
    \end{tabular}%
  \label{tab:gm}%
\end{table}%

\noindent By using Eq.~\eqref{gm}, we get the G.M
\begin{equation}
\text{G.M}=\text{Antilog}\left[\frac{1}{5}\left(8.25323\right)\right]=5.210342.
\end{equation}

\noindent The G.M can be computed using geometric.mean () function from psych~\cite{William2023} library.
<<Geomatric>>=
x <- c(2, 4, 6, 8, 10)
psych::geometric.mean(x)
@
\noindent Negative values cannot be included in the calculation of a geometric mean.


\subsection{Weighted mean}
The simple arithmetic mean gives equal weights to all values. But in some situations, the some values has more weights than the others. Therefore, in such situations, the weighted mean is implemented. In weighted arithmetic mean, the weight ($w$) is assigned corresponding to each value of $x$. It is computed from the following formula and as follows:
\begin{equation}
\bar{x}_{w}=\frac{w_{1}x_{1}+w_{2}x_{2}+\cdots+w_{n}x_{n}}{w_{1}+w_{2}+\cdots+w_{n}}=\frac{\sum_{i=1}^{n}w_{i}x_{i}}{\sum_{i=1}^{n}w_{i}},
\end{equation}
where $w$ denotes the weights applied to $x$ and $x$ denotes the data values to be averaged.


\begin{table}[H]
  \centering
  \caption{Weighted mean computation.}
    \begin{tabular}{ccccccc}
    \hline
    x     & 10    & 20    & 30    & 40    & 50    &  \\
    \hline
    w     & 2     & 4     & 6     & 8     & 10    & 30 \\
    \hline
    wx    & 20    & 80    & 180   & 320   & 500   & 1100 \\
    \hline
    \end{tabular}%
  \label{tab:wa}%
\end{table}%

\begin{equation}
\bar{x}_{w}=\frac{\sum_{i=1}^{n}w_{i}x_{i}}{\sum_{i=1}^{n}w_{i}}=\frac{1100}{30}=36.666.
\end{equation}
The weighted mean can be computed using weighted.mean () function and is as follows:
<<weightsss>>=
weights <- c(2,  4,  6,  8, 10)
x <- c(10, 20, 30, 40, 50)
weighted.mean(x, weights)
@

\subsection{Measures of dispersion}
Here, we demonstrate a few key dispersion metrics: the first four ordinary moments, the first four central or mean moments, the Pearson's skewness and kurtosis measures. The first four ordinary moments are as follows:
\begin{equation}
\mu_{1}^{\prime}=\frac{1}{n}\sum_{i=1}^{n}x_{i},\,\,\mu_{2}^{\prime}=\frac{1}{n}\sum_{i=1}^{n}x_{i}^{2},\,\,\mu_{3}^{\prime}=\frac{1}{n}\sum_{i=1}^{n}x_{i}^{3},\,\,\mu_{4}^{\prime}=\frac{1}{n}\sum_{i=1}^{n}x_{i}^{4}.
\end{equation}
Here, we use relatively simple data to demonstrate the computation of ordinary moments shown in Table~\ref{tab:mom}
\begin{equation}
\mu_{1}^{\prime}=\frac{15}{5}=3,\,\,\mu_{2}^{\prime}=\frac{55}{5}=11,\,\,\mu_{3}^{\prime}=\frac{225}{5}=45,\,\,\mu_{4}^{\prime}=\frac{979}{5}=195.8.
\end{equation}
The ordinary moments are as follows, employing the moments~\cite{Lukasz2022} R package library's all.moments() function:
<<ordin11>>=
x <- c(1:5)
# first four ordinary moments
moments::all.moments(x, order.max = 4)
@

\noindent First four mean or actual or central moments is given by
\begin{equation}
\mu_{1}=\frac{1}{n}\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right),\,\,\mu_{2}=\frac{1}{n}\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2},
\end{equation}
\begin{equation*}
\mu_{3}=\frac{1}{n}\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{3},\,\,\mu_{4}=\frac{1}{n}\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{4}.
\end{equation*}
\noindent where all odd order moments are vanished only even order moments exists. On a small data set we compute the first four ordinary and mean moments and given by in following Table~\ref{tab:mom}. Based on the Table~\ref{tab:mom}, the first four mean moments are given by
\begin{equation}
\mu_{1}=\frac{0}{5}=0,\,\,\mu_{2}=\frac{10}{5}=2,\,\,\mu_{3}=\frac{0}{5}=0,\,\,\mu_{4}=\frac{34}{5}=6.8.
\end{equation}

<<Moments>>=
# first four mean moments
moments::all.moments(x, central = TRUE, order.max = 4)
@

\begin{table}[H]
  \centering
  \caption{First four ordinary and mean moment}
    \begin{tabular}{cccccccc}
    \hline
    \multicolumn{4}{c}{Ordinary moments} & \multicolumn{4}{c}{Central moments} \\
    \hline
    $x$     & $x^2$  & $x^3$  & $x^4$  & $(x-3)$   & $(x-3)^2$ & $(x-3)^3$ & $(x-3)^4$\\
    \hline
    1     & 1     & 1     & 1     & -2    & 4     & 8     & 16 \\
    2     & 4     & 8     & 16    & -1    & 1     & 1     & 1 \\
    3     & 9     & 27    & 81    & 0     & 0     & 0     & 0 \\
    4     & 16    & 64    & 256   & 1     & 1     & -1    & 1 \\
    5     & 25    & 125   & 625   & 2     & 4     & -8    & 16 \\
    \hline
    15    & 55    & 225   & 979   & 0     & 10    & 0     & 34 \\
    \hline
    3     & 11    & 45    & 195.8 & 0     & 2     & 0     & 6.8 \\
    \hline
    \end{tabular}%
  \label{tab:mom}%
\end{table}%



\subsection{Karl Pearson's and Bowley's coefficient of skewness}
A symmetrical distribution is one in which the mode, median, and mean are all equal. Conversely, if not, the distribution is referred to as asymmetric or skewed. Skewness is an important statistical concept since it sheds light on the distributional form and characteristics of a dataset. It measures the degree to which the data deviate from a distribution that is symmetrical. Karl Pearson's coefficient of skewness is given by
\begin{equation}
\text{S}_{\text{kp }}=\frac{3(\bar{x}-\widetilde{x})}{\text{\ensuremath{\sigma}}},\qquad-1\le\text{S}_{\text{kp }}\le1,
\end{equation}
if $\text{S}_{\text{kp }}=0$, denotes the distribution is symmetrical about mean. If $\text{S}_{\text{kp}}>0$, the distribution is skew to the right and if $\text{S}_{\text{kp}}<0$, the distribution is skew to the left.


<<Personch3>>=
x <- c(15, 20, 14, 7, 6, 8, 3, 7, 2, 9, 2, 7, 8)
mean <- mean(x)
median <- median(x)
sd <- sd(x)
sk <- 3 * (mean - median)/sd
sk
@
\noindent Bowley's coefficient of skewness is given by

\begin{equation}
\text{S}_{\text{b }}=\frac{Q_{3}+Q_{1}-2Q_{2}}{\text{\ensuremath{Q_{3}-Q_{1}}}},\qquad-1\le\text{S}_{\text{b }}\le1,
\end{equation}
where $Q_3$ and $Q_1$ are the upper and lower quartiles, respectively. Whereas $Q_2$ is a median of given data set. The quartiles can be computed from the following formulas

\begin{equation}
Q_{1}=\frac{(n+1)}{4}\text{th value, \ensuremath{Q_{2}}=\ensuremath{\frac{2(n+1)}{4}\text{th value \text{and \ensuremath{Q_{3}}=\ensuremath{\frac{3(n+1)}{4}\text{th value, }}}}}}
\end{equation}
where $n$ is the total number of values in the given data set. If $\text{S}_{\text{b }}=0$, denotes the distribution is symmetrical about mean. If $\text{S}_{\text{b}}>0$, the distribution is skew to the right and if $\text{S}_{\text{b}}<0$, the distribution is skew to the left.
<<dchhh>>=
Q1 <- quantile(x, 0.25)
Q2 <- quantile(x, 0.5)
Q3 <- quantile(x, 0.75)
sb <- (Q3 - 2 * Q2 + Q1)/(Q3 - Q1)
sb
@

\subsection{Combine mean and variance}
If we have data sets with various series, we can use the following formula to compute the combined arithmetic mean:

\begin{equation}\label{cm}
\bar{x}_{c}=\frac{n_{1}\bar{x}_{1}+n_{2}\bar{x}_{2}+\cdots+n_{n}\bar{x}_{n}}{n_{1}+n_{2}+\cdots+n_{n}},
\end{equation}
\noindent where the first series' mean, the second series' mean, and so on, the $n$th series' mean, are denoted by the $\bar{x}_1$, $\bar{x}_2$ and so on $\bar{x}_n$. In contrast, the number items in the first series, second series, and so on  in $n_n$ series are represented by the $n_1$, $n_2$, and so on $n_n$. As an illustration, we have three series, where the first, second, and third series mean values are 5, 8, and 2, respectively. There are 10, 10, and 50 items in the first, second, and third series, in that order. The combine mean is as follows:

\begin{equation}
\bar{x}_{c}=\frac{n_{1}\bar{x}_{1}+n_{2}\bar{x}_{2}+n_{3}\bar{x}_{3}}{n_{1}+n_{2}+n_{3}}=\frac{10\times5+10\times8+50\times2}{10+10+50}=3.2857.
\end{equation}
\noindent The combine variance can be computed as
\begin{equation}
S_{c}^{2}=\frac{n_{1}\left[S_{1}^{2}+\left(\bar{x}_{1}-\bar{x}_{c}\right)^{2}\right]+n_{2}\left[S_{2}^{2}+\left(\bar{x}_{2}-\bar{x}_{c}\right)^{2}\right]+\cdots+n_{n}\left[S_{n}^{2}+\left(\bar{x}_{n}-\bar{x}_{c}\right)^{2}\right]}{n_{1}+n_{2}+\cdots+n_{n}},
\end{equation}
where $S_{1}^{2}$, $S_{2}^{2}$, and so on $S_{n}^{2}$ represent the variance of the first series, the second series, and so on, the $n$th series. The first series' mean, the second series' mean, and so on, the $n$th series' mean, are denoted by the $\bar{x}_1$, $\bar{x}_2$ and so on $\bar{x}_n$. On the other hand, the $n_1$, $n_2$, and so on $n_n$ denote the number items in the first series, second series, and so on in the $n_n$ series and $\bar{x}_{c}$ is a combine mean shown in Eq.~\eqref{cm}. As an example, we have three series, with variances of 2, 3, and 0.4 and mean values of 5, 8, and 2, respectively, for the first, second, and third series. The first, second, and third series each contain 10, 10, and 50 items, respectively.

\begin{equation*}
S_{c}^{2}=\frac{10\left[2+\left(5-3.2857\right)^{2}\right]+10\left[3+\left(8-3.2857\right)^{2}\right]+50\left[0.4+\left(2-3.2857\right)^{2}\right]}{10+10+50}=5.7809.
\end{equation*}

\noindent In R, we can compute the combine mean and variance using combinevar () function from fishmethods~\cite{Gary2023} library:
<<combine mean and variance, warning=FALSE>>=
means <- c(5, 8, 2)
variances <- c(2, 3, 0.4)
n <- c(10, 10, 50)
fishmethods::combinevar(means, variances, n)
@

\subsection{Location and varibility measures for gruop data}\label{ld}
Using a simple set of data from Table~\ref{tab:cb}, we calculate the following several measures of locations and variability for group data: mean, variance standard deviation, coefficient of variation, median, mode, first, second, and third quartiles, inter quartile-range, skewness, and kurtosis.

\begin{table}[H]
  \centering
  \caption{Group data set.}
    \begin{tabular}{ccccc}
    \hline
    LCB   & 2     & 4     & 6     & 8 \\
    \hline
    UCB   & 4     & 6     & 8     & 10 \\
    \hline
    f     & 5     & 10    & 30    & 20 \\
    \hline
    \end{tabular}%
  \label{tab:cb}%
\end{table}%
\noindent The computation of mean, variance, standard deviation, and coefficient of variation is shown in Table~\ref{tab:mv}.
\begin{table}[H]
  \centering
  \caption{Computation of mean, variance, standard deviation and coefficient of variation.}
    \begin{tabular}{ccccccc}
    \hline
    LCB   & UCB   & $f$     & $x$     & $fx$    & $(x-\bar{x})^{2}$ & $f(x-\bar{x})^{2}$ \\
    \hline
    2     & 4     & 5     & 3     & 15    & 16    & 80 \\
    4     & 6     & 10    & 5     & 50    & 4     & 40 \\
    6     & 8     & 30    & 7     & 210   & 0     & 0 \\
    8     & 10    & 20    & 9     & 180   & 4     & 80 \\
    \hline
          &       & 65    &       & 455   &       & 200 \\
          \hline
    \end{tabular}%
  \label{tab:mv}%
\end{table}%

\noindent Table~\ref{tab:mv} is used to determine the mean of data set given in Table~\ref{tab:cb}
\begin{equation}
\bar{x}=\frac{\sum fx}{\sum f}=\frac{455}{65}=7.
\end{equation}
\noindent The variance is given by
\begin{equation}
S^{2}=\frac{\sum f(x-\bar{x})^{2}}{\sum f}=\frac{200}{65}=3.0769.
\end{equation}
\noindent The standard deviation is given by
\begin{equation}
\text{sd}=\sqrt{\frac{\sum f(x-\bar{x})^{2}}{\sum f}}=\sqrt{3.0769}=1.7541.
\end{equation}
\noindent The coefficient of variation is given by
\begin{equation}
\text{C.V}=\frac{\text{sd}}{\bar{x}}\times100=\frac{1.7541}{7}\times100=25.0588.
\end{equation}

\subsection{Median, mode and quartiles for group data}

Table~\ref{tab:cb} for group data is used to compute the median, mode, lower quartile, upper quartile, and interquartile range. The median is defined as
\begin{equation}
\text{Median}=l+\frac{h}{f}(\frac{n}{2}-c),
\end{equation}
\noindent where $l$ is a lower class boundary, $h$ is a common interval, $f$ is a frequency, $n/2$ is a median class and $c$ is a preceding class.


\begin{table}[H]
  \centering
  \caption{Computation of median, mode and quartiles for group data.}
    \begin{tabular}{cccr}
    \hline
    LCB   & UCB   & f     & \multicolumn{1}{c}{cf} \\
     \hline
    2     & 4     & 5     & \multicolumn{1}{c}{5} \\
    4     & 6     & 10    & \multicolumn{1}{c}{15} \\
    \textbf{6} & \textbf{8} & \textbf{30} & \multicolumn{1}{c}{\textbf{45}} \\

    8     & 10    & 20    & \multicolumn{1}{c}{65} \\
     \hline
          &       & 65    &  \\
          \hline
    \end{tabular}%
  \label{tab:med1}%
\end{table}%

\noindent The first step in computing the median is to identify the median class (n/2=65/2=32.5). By determining the median class in the cumulative frequency (cf) column (see Table~\ref{tab:med1}), we can obtain the $l$, $f$, and $c$ and given by
\begin{equation}
\text{Median}=l+\frac{h}{f}(\frac{n}{2}-c)=6+\frac{2}{30}(32.5-15)=7.16667.
\end{equation}
\noindent The most frequent value for frequency distribution is given by
\begin{equation}
\text{Mode}=l+\frac{f_{m}-f_{1}}{\left(f_{m}-f_{1}\right)+\left(f_{m}-f_{2}\right)}\times h,
\end{equation}
\noindent where $f_m$ is the frequency of the modal class, $f_1$ is the frequency of the class preceding the modal class, $f_2$ is the frequency of the class succeeding the modal class, $l$ is a lower class boundary and $h$ is a common interval.

\begin{equation}
\text{Mode}=l+\frac{f_{m}-f_{1}}{\left(f_{m}-f_{1}\right)+\left(f_{m}-f_{2}\right)}\times h=6+\frac{30-10}{\left(30-10\right)+\left(30-20\right)}\times2=7.33.
\end{equation}

\noindent The first step in determining the quartile is to identify the quartile class (n/4=65/4=16.25). By locating this class in the cumulative frequency (cf) distribution column, we can obtain the $l$, $f$, and $c$. The lower quartile can be computed and given by
\begin{equation}
Q_{1}=l+\frac{h}{f}(\frac{n}{4}-c),\qquad Q_{2}=l+\frac{h}{f}(\frac{2n}{4}-c)\qquad Q_{3}=l+\frac{h}{f}(\frac{3n}{4}-c).
\end{equation}
Using Table~\ref{tab:med1}, we get
\begin{equation}
Q_{1}=l+\frac{h}{f}(\frac{n}{4}-c)=6+\frac{2}{30}(16.25-15)=6.0833.
\end{equation}
\noindent The upper quartile can be computed and given by
\begin{equation}
Q_{3}=l+\frac{h}{f}(\frac{3n}{4}-c)=8+\frac{2}{20}(48.75-45)=8.3750.
\end{equation}

\noindent The inter quartile range is a difference of upper and quartile and given by
\begin{equation}
IQR=Q_{3}-Q_{1}=8.3750-6.0833=2.29167.
\end{equation}
\noindent In R, we can compute several locations and variability measures for group data given in \ref{ld}, using gds () function from gds~\cite{Partha2021} library:
<<gdsss>>=
gds::gds(c(2, 4, 6, 8), c(4, 6, 8, 10), c(5, 10, 30, 20))
@


\begin{thebibliography}{99}
\bibitem{Paul2019}
Paul Poncet (2019). modeest: Mode Estimation. R package
  version 2.4.0.
  https://CRAN.R-project.org/package=modeest.


\bibitem{William2023}
William Revelle (2023). psych: Procedures for
Psychological, Psychometric, and Personality Research.
Northwestern University, Evanston, Illinois. R package
version 2.3.6, <URL:
https://CRAN.R-project.org/package=psych>.


\bibitem{Lukasz2022}
Lukasz Komsta and Frederick Novomestky (2022). moments:
  Moments, Cumulants, Skewness, Kurtosis and Related
  Tests. R package version 0.14.1.
  https://CRAN.R-project.org/package=moments

\bibitem{Gary2023}
Gary A. Nelson (2023). fishmethods: Fishery Science
  Methods and Models. R package version 1.12-1.
  https://CRAN.R-project.org/package=fishmethods


\bibitem{Partha2021}
Partha Sarathi Bishnu (2021). gds: Descriptive
  Statistics of Grouped Data. R package version 0.1.1.
  https://CRAN.R-project.org/package=gds


\end{thebibliography}


\chapter{Essentials to Model Fitting}
\section{Introduction}

This Chapter is based on the following distribution theory related topics: fitting of the probability distributions (taking a simple example of exponential distribution), graphical illustration of PDF and CDF, total test time plot, unknown parameter estimation using AdequacyModel, estimation using maxLik function, variance co-variance matrix, ordinary moments, mean moments, median and quartile deviation, distributional properties using dprop R package, fitting of generalized (G) families of distributions (Bell-G family), density and hazard rate function shapes of Bell Weibull
distribution, ordinary moments of the BellW distribution, Lorenz and Bonferroni curve of the BellW distribution, applications to real data, Kaplan-Maier plot, probability-probability plot, fitting of discrete Probability distributions, binomial distribution, Poisson distribution, parameter estimation of discrete distribution, zero-inflated model and zero-one-inflated models.

\subsection{R packages dependencies}
This Chapter depends on the following R packages such as DataSetsUni, stats, AdequacyModel, maxLik, dprop, survival and countDM.



\subsection{A simple illustration of exponential distribution}
Different dimensions of fitting of probability distributions including PDF, CDF, QF, random number generation, graphical illustration of PDF, SF, HRF, bonferoni and Lorenz curve, probability-probability plot, Kaplan Meier plot, estimation and practical implementation are studied. Here, we take an exponential distribution as a case study. The Probability density function of the exponential distribution is as follows:
\begin{equation}
f(x)=\lambda e^{-\lambda x};\qquad x\ge0,
\end{equation}
where $\lambda>0,$ is the only rate parameter of the exponential distribution. Here, we specify the PDF of exponential distribution taking $x=0.5$ and $\lambda=2$ using the following codes:

<<>>=
# specifying PDF of the exponential distribution
PDF_Exp <- function(x, lambda) {
    f <- lambda * exp(-lambda * x)
    return(f)
}
PDF_Exp(0.5, 2)
@

\noindent We can also specify the PDF using the direct function dexp(x, rate)

<<>>=
dexp(0.5,2)
@

\noindent The cumulative distribution function (CDF) of the exponential distribution is as follows:
\begin{equation}
f(x)=1-e^{-\lambda x};\qquad x\ge0,
\end{equation}
where $\lambda>0,$ is the only rate parameter of the exponential distribution. Here, we specify the CDF of exponential distribution taking $x=0.5$ and $\lambda=2$ using the following codes:

<<>>=
# specifying CDF of the exponential distribution
CDF_Exp <- function(x, lambda) {
    F <- 1-exp(-lambda * x)
    return(F)
}
CDF_Exp(0.5, 2)
@

\noindent We can also specify the CDF using the direct function pexp(x, rate) function given by:

<<>>=
pexp(0.5, 2)
@

\noindent The quantile function of the exponential distribution is given by
\begin{equation}
x_{q}=-\frac{1}{\lambda}\log(1-q);\qquad0\le q\le1.
\end{equation}
Using $q=0.7$ and $\lambda=0.5$, we express the QF of the exponential distribution using the following codes:
<<>>=
# specifying QF of exponential distribution
QF_Exp <- function(q, lambda) {
    QF <- -1/lambda * log(1 - q)
    return(QF)
}
QF_Exp(0.7, 0.5)
@

\noindent Additionally, we can use the direct function qexp(q, rate) to specify the QF and follows:
<<>>=
qexp(0.7, 0.5)
@
\noindent To generate random numbers from the exponential distribution, the following R codes based on QF can be used after specifying $n$ and $\lambda$ and given by:
<<>>=
# generating random numbers using the exponential distribution
rn_Exp <- function(n, lambda) {
    q <- runif(n, min = 0, max = 1)

    QF <- -1/lambda * log(1 - q)
    return(QF)
}
rn_Exp(5,0.5)
@

\noindent The R rexp() function can be used to generate random numbers from the exponential distribution. To create n random numbers, the following rexp(n, rate) function is used
<<>>=
rexp(5, 0.5)
@

\noindent The survival function (SF) gives the probability that a subject will survive past time $t$.The SF of the exponential distribution is as follows:
\begin{equation}
f(x)=e^{-\lambda x},
\end{equation}
\noindent where $\lambda>0,$ is the only rate parameter of the exponential distribution. Here, we specify the SF of exponential distribution taking $x=0.5$ and $\lambda=2$ using the following R codes:

<<>>=
# specifying CDF of exponential distribution
SF_Exp <- function(x, lambda) {
    SF <- exp(-lambda * x)
    return(SF)
}
SF_Exp(0.5, 2)
@
\noindent The hazard rate function (HRF) is the ratio of PDF to SF, for exponential distribution, it is given by:
\begin{equation}
h(x)=\frac{f(x)}{1-F(x)}=\frac{\lambda e^{-\lambda x}}{e^{-\lambda x}}=\lambda.
\end{equation}
\noindent Here, we specify the HRF of exponential distribution taking $x=2$ and $\lambda=2$ using the following codes:
<<>>=
HRF_Exp <- function(x, lambda) {
    f <- lambda * exp(-lambda * x)
    sf <- exp(-lambda * x)
    hrf <- f/sf
    return(hrf)
}
HRF_Exp(2, 2)
@

\subsection{PDF and CDF shapes of exponential distribution}
The Figure~\ref{fig:exp_pdf} provides the graphical illustration of PDF at several parametric values of
the exponential distribution. Here, we employ three different values of parameter $\lambda$ that is
0.5, 1 and 1.5. The PDF and CDF shapes starts modifying after changing the parametric
values and given by
<<exp_pdf, fig.cap="Graphical illustration of PDF and CDF at some parametric values.",fig.height=6, fig.width=11,fig.align='center'>>=
par(mfrow = c(1, 2))
# graphical illustration of PDF when lambda = 0.5, 1 and
# 1.5
curve(PDF_Exp(x, 0.5), main = "", ylab = "f(x)", xlim = c(0,
    5), ylim = c(0, 1.4), lty = 1, col = 1, lwd = 3)
# when lambda = 1.0
curve(PDF_Exp(x, 1), add = T, lty = 2, col = 2, lwd = 3)
# when lambda = 1.5
curve(PDF_Exp(x, 1.5), add = T, lty = 3, col = 3, lwd = 3)
legend("topright", c(expression(lambda == 0.5), expression(lambda ==
    1), expression(lambda == 1.5)), lty = c(1:3), col = c(1:3),
    lwd = c(2, 2, 2), cex = 1.4)
# graphical illustration of CDF when lambda = 0.5, 1 and
# 1.5
curve(CDF_Exp(x, 0.5), main = "", ylab = "F(x)", xlim = c(0,
    5), ylim = c(0, 1), lty = 1, col = 1, lwd = 3)
# when lambda = 1.0
curve(CDF_Exp(x, 1), add = T, lty = 2, col = 2, lwd = 3)
# when lambda = 1.5
curve(CDF_Exp(x, 1.5), add = T, lty = 3, col = 3, lwd = 3)
legend("bottomright", c(expression(lambda == 0.5), expression(lambda ==
    1), expression(lambda == 1.5)), lty = c(1:3), col = c(1:3),
    lwd = c(2, 2, 2), cex = 1.4)
@


\subsection{Total test time plot}
The total test time plot is useful to underline the hazard rate function of the real data.
The hazard rate function can be increasing, decreasing, increasing-decreasing ete. The
total time plot can be obtained using TTT () function from AdequacyModel library. The
increasing hazard rate function (left) and decreasing-increasing (right) are depicted in
Figure~\ref{fig:tttt}.
<<tttt,fig.cap="TTT plot of two different data sets.",fig.height=6, fig.width=11,fig.align='center'>>=
par(mfrow = c(1, 2))
x <- DataSetsUni::data_airpollution
AdequacyModel::TTT(x, col = 2)
x <- DataSetsUni::data_airborne
AdequacyModel::TTT(x, col = 4)
@


\subsection{Estimation}
Here, we outline the three approaches for estimating the exponential distribution's un-
known parameter. We use an exponential distribution in a straightforward example to
help with comprehension purposes. The goodness.fit () function from AdequacyModel
library allows to estimate the unknown parameter of given distribution after defining its PDF and CDF as given by:

<<warning=FALSE, fig.cap="Estimated PDF.">>=
# x is a data vector
x <- DataSetsUni::data_airpollution
# defining the PDF of the exponential distribution
PDF_Exp <- function(par, x) {
    lambda = par[1]
    F = 1 - exp(-lambda * x)
    f = lambda * exp(-lambda * x)
    return(f)
}
# defining the CDF of the exponential distribution
CDF_Exp <- function(par, x) {
    lambda = par[1]
    F = 1 - exp(-lambda * x)
    f = lambda * exp(-lambda * x)
    return(F)
}
res_Exp = AdequacyModel::goodness.fit(pdf = PDF_Exp, cdf = CDF_Exp,
    starts = c(0.023), data = x, method = "B", domain = c(0,
        Inf), mle = NULL)
res_Exp$mle
@
\noindent The graphical presentation of actual and estimated values can be viewed in Figure~\ref{fig:ppp},
whereas the estimated CDF showing a good fit under exponential distribution and given
by:

<<ppp,fig.cap="Graphical presentation of PDF and CDF.",fig.height=6, fig.width=11,fig.align='center'>>=
par(mfrow = c(1, 2))
hist(x, prob = T, main = "", xlab = "x")
curve(PDF_Exp(res_Exp$mle, x), add = T, lty = 1, col = 2, lwd = 6)
legend("topright", legend = c("Exponential"), col = 2, lty = 1,
    lwd = 4, cex = 1.2)
# plot of estimated CDF
plot(ecdf(x), main = "", ylab = "cdf")
curve(CDF_Exp(res_Exp$mle, x), add = T, lty = 1, col = 2, lwd = 5)
legend("bottomright", legend = c("Exponential"), col = 2, lty = 1,
    lwd = 4, cex = 1.2)
@
\noindent With the AdequacyModel package, the goodness.fit () function permits to evaluate the several important goodness-of-fit measures (see section 4.2.5) namely the estimated parameters along with standard errors of the estimates, Akaike information criterion (AIC),
corrected Akaike information criterion (CAIC), Bayesian information criterion (BIC), Hannan-Quinn information criterion (HQIC), Cram\'er-Von Mises test (W), Anderson-Darling(A) and Kolmogorov-Smirnov (KS) test.
\subsection{maxLik R package based estimation}
The maxLik package allows to estimate parameters by specifying the initial values. The
summary function provides the estimates, standard error, t-statistic with p-values. It also
gives the information criteria namely AIC and BIC. Following is the simple illustration of the exponential distribution
<<warning=FALSE>>=
x <- DataSetsUni::data_airpollution
pdf_exp <- function(param) {
    alpha <- param[1]
    ll = sum(dexp(x, alpha, log = TRUE))
    return(ll)
}
mle <- maxLik::maxLik(pdf_exp, start = c(0.023))
summary(mle)
AIC(mle)
@
\noindent The third way of estimating parameter that is maximizing the log likelihood function and as follows:
<<>>=
x <- DataSetsUni::data_airpollution
mle_exp <- function(x) {
    n <- length(x)
    sx <- sum(x)
    lambda <- 1/(sx/n)
    loglik <- n * log(lambda) - lambda * sx
    list(loglik = loglik, lambda = lambda)
}
mle_exp(x)
@


\subsection{Variance co-variance matrix}
The vcov () function from maxLik library allows to compute Variance co-variance matrix.
The variances in the matrix are contained in its diagonal elements, whereas the covariances
are found in its off-diagonal elements
<<warning=FALSE>>=
x <- DataSetsUni::data_airpollution
pdf_wei <- function(param) {
    alpha <- param[1]
    beta <- param[2]

    ll = sum(dweibull(x, alpha, beta, log = TRUE))
    return(ll)
}
mle <- maxLik::maxLik(pdf_wei, start = c(1.7, 0.7))
vcov(mle)
@


\subsection{Moments}
The first four ordinary moments of the exponential distribution is given by
<<>>=
pdf_E <- function(lambda, x) {
    g = lambda * exp(-lambda * x)
    return(g)
}
E_mom <- function(lambda, r) {
    f <- function(lambda, r, x) {
        (x^(r)) * (pdf_E(lambda, x))
    }
    y = stats::integrate(f, lower = 0, upper = Inf,
        subdivisions = 1e+06, lambda = lambda, r = r)$value
    return(y)
}
m1 <- E_mom(5, 1)
m2 <- E_mom(5, 2)
m3 <- E_mom(5, 3)
m4 <- E_mom(5, 4)
m1; m2; m3; m4
@
\noindent Using the relationship between mean moments and ordinary moments, the first four mean
moments of the exponential distribution is given by


<<>>=
mu1 <- m1
mu2 <- m2 - (m1)^2
mu3 <- m3 - 3 * (m1 * m2) + 2 * (m1)^3
mu4 <- m4 - 4 * m3 * m1 + 6 * m2 * (m1)^2 - 3 * (m1)^4
mu1; mu2; mu3; mu4
@
\noindent The coefficient of skewness and kurtosis measures based on moments are important to describe the spread and height of distribution and given by
\begin{equation}
\beta_{1}=\frac{\mu_{3}^{2}}{\mu_{2}^{3}}\qquad\gamma_{1}=\sqrt{\beta_{1}}\,\,\text{and\,\,\ensuremath{\beta_{2}}=\ensuremath{\frac{\mu_{4}}{\mu_{2}^{2}}\qquad\gamma_{2}}=\ensuremath{\beta_{2}}-3}.
\end{equation}
<<>>=
beta_1 <- (mu3^2)/(mu2^3)
gamma_1 <- sqrt(beta_1)
# coefficient of skewness
gamma_1
beta_2 <- mu4/(mu2)^2
gamma_2 <- (beta_2) - 3
# coefficient of kurtosis
gamma_2
@

\subsection{Median and quartile deviation}
The median and quartile deviation of the exponential distribution is given by
<<>>=
# median
lambda <- 2
med_exp <- stats::qexp(0.5, lambda)
med_exp
# quartile deviation
qd_exp <- (stats::qexp(0.75, lambda) - stats::qexp(0.25, lambda))/2
qd_exp
@

\subsection{Distributional properties using dprop R package}
The dprop [6] package allows to quantify some important properties of several popular and
widely used probability distributions, such as the first four ordinary and central moments,
the mean and variance, the coefficient of variation, the median, the quartile deviation,
and Pearson's coefficient of skewness and kurtosis at some parametric values
<<>>=
# distributional properties of Weibull distribution
dprop::d_wei(2, 2)
@

\section{Generalized families of distributions}
The generalization of the classical distributions is a well-established technique that has been valued as highly as many practical statistical issues. As a fact, the classical distributions are restricted to goodness of fit tests and tail features. Due to their fewer features, classical models generally do not offer an ideal fitting. On the other hand, the capacity to effectively address the challenging real-world problem has strengthened the appeal of the generalization of distributions. A useful study regarding the generalization of distributions under different modes and procedures, here we mention few but not limited, the readers are referred to \cite{dTahir2016}, \cite{dMaurya2021} and \cite{dTahir2015}. Despite the fact that there are a great deal of flexible families of distributions and extended models available in the literature, most of them still lack computational support. The following R packages are frequently used in relation to distribution theory: VGAM, extraDistr, emdbook, MASS, countDM, gamlss.dist, fitdistrplus. The Newdistns R package deals with several extended families of distributions and evaluates the PDF, CDF, QF, random number generation and maximum likelihood estimation of the unknown parameters. Another important package uses hundred parametric families of distribution to compute the two commonly used risk measures namely the value at risk and expected shortfall that is VaRES. The PDF, CDF, and different distributional features of the extended families sometimes entail special functions and binomial expansions, and therefore frequently don't offer closed-form solutions. As a fact, the users don't fully enjoy these flexible families of distributions. Here, we illustrate a simple way to fit these extended families of distributions using R.
\subsection{Fitting of the Bell-G family of distributions}

\noindent Recently, {\it Alsadat et al.}~\cite{dAlsadat2023} introduced a flexible family of distributions called the Bell-G family of distributions, which leverages the Weibull distribution as a special model and is applied to actuarial as well as COVID-19 data sets. Here, we'll demonstrate how the generalized families can be used and how to put up their R programmes in a simple and basic way. The user can work on and create R programmes for any other generalized family of distributions, once they have gone through the following R programmes. The CDF of the Bell-G family of distribution is as follows:
\begin{equation}\label{cdf_bell}
F(x)=\frac{1-\exp\left[-e^{\lambda}\left(1-e^{-\lambda G(x)}\right)\right]}{1-\exp\left(1-e^{\lambda}\right)},
\end{equation}

\noindent where $x>0$ and $\lambda>0$. The $G\left(x\right)=1-\exp(-\alpha x^{\beta})$ is a baseline CDF that a Weibull distribution. \noindent The following codes permits to compute the CDF of the Bell-Weibull distribution

<<>>=
cdf_BellW <- function(alpha, beta, lambda, x) {
    # G is the baseline CDF that is Weibull model
    G = (1 - exp(-alpha * x^beta))
    # F is the CDF of the Bell-G family
    F = (1 - exp(-exp(lambda) * (1 - exp(-lambda * G))))/(1 -
        (exp(-exp(lambda) + 1)))
    return(F)
}
cdf_BellW(0.5, 2, 1.5, 2)
@


\noindent The PDF of the Bell-G family of distribution is as follows:

\begin{equation}
f(x)=\frac{\lambda g(x)\exp\left[\lambda\left(1-G(x)\right)\right]\exp\left[-e^{\lambda}\left(1-e^{-\lambda G(x)}\right)\right]}{1-\exp\left(1-e^{\lambda}\right)},
\end{equation}

\noindent where $x>0$ and $\lambda>0$. The $G\left(x\right)=1-\exp(-\alpha x^{\beta})$ and $g(x)=\alpha\beta x^{(\beta-1)}\exp(-\alpha x^{\beta}$ are a baseline CDF and PDF, respectively.
\noindent The following codes allows to compute the PDF of the Bell-Weibull distribution
<<>>=
pdf_BellW <- function(alpha, beta, lambda, x) {
    # G is the baseline CDF that is Weibull model
    G = (1 - exp(-alpha * x^beta))
    # g is the baseline PDF that is Weibull model
    g = alpha * beta * x^(beta - 1) * exp(-alpha * x^beta)
    # f is the PDF of the Bell-G family
    f = lambda * g * exp(lambda * (1 - G)) * exp(-exp(lambda) *
        (1 - exp(-lambda * G)))/(1 - (exp(-exp(lambda) + 1)))
    return(f)
}
pdf_BellW(1.2, 0.3, 2, 1.5)
@
\noindent By using Eq.~\eqref{cdf_bell} and solve for $x$. The QF of the Bell-G family of distributions is as follows:
\begin{equation}\label{qf1}
Q_{-}BellG=G^{-1}\left(\frac{1}{\lambda}\log\left[1-\frac{\log\left(1-q\left[1-\exp\left(1-e^{\lambda}\right)\right]\right)}{-e^{\lambda}}\right]\right),
\end{equation}
\noindent where $G^{-1}$ is a QF of a baseline Weibull distribution and as follows:
\begin{equation}\label{qf2}
q_{-}Weibull=\left[-\frac{1}{\alpha}\log\left(1-q_{-}BellG\right)\right]^{1/\beta},
\end{equation}

\noindent using Eq.~\eqref{qf1}, the $q_{-}BellG$ is given by
\begin{equation}\label{qf3}
q_{-}BellG=\frac{1}{\lambda}\log\left[1-\frac{\log\left(1-q\left[1-\exp\left(1-e^{\lambda}\right)\right]\right)}{-e^{\lambda}}\right].
\end{equation}
\noindent The QF of the Bell-W distribution can be obtained by replacing Eq.~\eqref{qf3} in Eq.~\eqref{qf2}. The following codes allows to compute the QF of the Bell-Weibull distribution
<<>>=
q_BellW <- function(alpha, beta, lambda, q) {
    # the QF of the BellG family
    q_BellG = -1/lambda * log(1 - ((log(1 - q * (1 - (exp(-exp(lambda) +
        1)))))/(-exp(lambda))))
    # the QF of the baseline Weibull distribution
    q_weibull = (-1/alpha * (log(1 - q_BellG)))^1/beta
    return(q_weibull)
}

q_BellW(1.2, 2, 2, 0.2)
@

\noindent The following codes can be used to obtain the random numbers based on the QF of the BellW distribution is as follows:
<<>>=
r_BellW <- function(alpha, beta, lambda, n) {
    q <- runif(n, min = 0, max = 1)
    q_BellW(alpha, beta, lambda, q)
}
r_BellW(1.2, 2, 2, 5)
@

\noindent The survival function can be computed $s(x)=1-F(x)$. It is given by for the BellW distribution:
\begin{equation}
S\left(x\right)=1-F(x)=\frac{\exp\left(-e^{\lambda}\left[1-e^{-\lambda G(x)}\right]\right)-\exp\left(1-e^{\lambda}\right)}{1-\exp\left(1-e^{\lambda}\right)},
\end{equation}

\noindent where $x>0$ and $\lambda>0$. The $G\left(x\right)=1-\exp(-\alpha x^{\beta})$ and $\alpha>0$ and $\beta>0.$
In R setting, it can be yielded as given by
<<>>=
sf_BellW <- function(alpha, beta, lambda, x) {
    # G is the baseline CDF that is Weibull model
    G = (1 - exp(-alpha * x^beta))
    # F is the CDF of the Bell-G family
    F = (1 - exp(-exp(lambda) * (1 - exp(-lambda * G))))/(1 -
        (exp(-exp(lambda) + 1)))
    # the survival function can be obtained by 1-CDF
    sf = 1-F
    return(sf)
}
sf_BellW(0.5, 2, 1.5, 2)
@

\noindent The hazard rate function can be computed as $h(x)=f(x)/s(x)$. It is given by for the BellW distribution:
\begin{equation}
h\left(x\right)=\frac{f(x)}{1-F(x)}=\frac{\lambda\,g(x)\exp\left(\lambda[1-G(x)]\right)\exp\left[-e^{\lambda}\left(1-e^{-\lambda G(x)}\right)\right]}{\exp\left(-e^{\lambda}\left[1-e^{-\lambda G(x)}\right]\right)-\exp\left(1-e^{\lambda}\right)},
\end{equation}
\noindent where $x>0$ and $\lambda>0$. The $G\left(x\right)=1-\exp(-\alpha x^{\beta})$ and $g(x)=\alpha\beta x^{(\beta-1)}\exp(-\alpha x^{\beta}$ are a baseline CDF and PDF, respectively. The following codes can be used to specify the HRF:
<<>>=
h_BellW <- function(alpha, beta, lambda, x) {
    # G is the baseline CDF that is Weibull model
    G = (1 - exp(-alpha * x^beta))
    # g is the baseline PDF that is Weibull model
    g = alpha * beta * x^(beta - 1) * exp(-alpha * x^beta)
    # F is the CDF of BellG family
    F = (1 - exp(-exp(lambda) * (1 - exp(-lambda * G))))/(1 -
        exp(1 - exp(lambda)))
    # f is the PDF of BellG family
    f = lambda * g * exp(lambda * (1 - G)) * exp(-exp(lambda) *
        (1 - exp(-lambda * G)))/(1 - exp(1 - exp(lambda)))
    s = 1 - F
    # the HRF is the ratio of PDF to survival function
    hrf = f/s
    return(hrf)
}
h_BellW(0.5, 2, 1.5, 2)
@

\subsection{Density and hazard rate function shapes of Bell Weibull distribution}
The most common practice regarding new distributions is to underline the flexibility in PDF and HRF through shape. The following R codes helps in ploting the shapes and as follows:
<<pdf_hrf plot, fig.cap="PDF and HRF plot.",fig.pos="t", fig.height=6, fig.width=11,fig.align='center'>>=
par(mfrow = c(1, 2))
curve(pdf_BellW(1, 1.3, 1.1, x), main = "alpha=1", ylab = "f(x)",
    xlim = c(0, 1.5), ylim = c(0, 2.5), lty = 1, col = "red",
    lwd = 3)
legend("topright", c(expression(beta == 1.3 ~ ~lambda ==
    1.1)), lty = 1, col = "red", lwd = 2, cex = 1)
# hazard rate plot
curve(h_BellW(0.1, 5, 1.5, x), main = "", ylab = "h(x)", xlim = c(0,
    2), ylim = c(0, 10), lty = 1, col = "red", lwd = 3)
legend("topright", c(expression(alpha == 0.1 ~ ~beta == 5 ~ ~lambda ==
    1.5)), lty = 1, col = "red", lwd = 2, cex = 1)
@

\subsection{Ordinary moments of the BellW distribution}
There are several uses of moments in understanding the skewness and kurtosis measures. We can also specify the mean and variance of the distribution through moments. The following codes allows to find out the $r$th ordinary moments. The moments can be obtained by specifing the paramaetrs values as well as $r$. For first ordinary moment or the mean of distribution can be computed by replacing $r$=1 and as follows:
<<>>=
BellW_mom <- function(alpha, beta, lambda, r) {
    f <- function(alpha, beta, lambda, r, x) {
        (x^r) * (pdf_BellW(alpha, beta, lambda, x))
    }
    y = integrate(f, lower = 0, upper = Inf, subdivisions = 1e+06,
        alpha = alpha, beta = beta, lambda = lambda, r = r)
    return(y)
}
# first ordinary moment
BellW_mom(1.2, 0.8, 0.5, 1)
@

\subsection{Lorenz and Bonferroni curve of the BellW distribution}

The Lorenz and Bonferroni curve are the commonly used income inequality measures in distribution theory. It is a graph used in economics to show how wealth or income is distributed. The curves are investigate income and poverty, but they have also been considered to be useful in other disciplines such as reliability, demography, insurance, and medicine. For better understanding the readers are referred to \cite{dPundir2005}. The following codes are provide the graphical illustration of both curves and as follows:
<<>>=
# quantile function of Bell Weibull distribution
quantile = function(alpha, beta, lambda, q) {
 (-1/alpha * (log(1 - (-1/lambda * log(1 - q * ((1 -
  exp(-lambda))))))))^1/beta
}
Ia = function(alpha, beta, lambda, a) {
 n = length(a)
 y = 0
 for (i in 1:n) {
  y[i] = integrate(function(alpha, beta, lambda, x) {
   x * pdf_BellW(alpha, beta, lambda, x)
  }, lower = 0, upper = a[i], subdivisions = 1e+05,
   alpha = alpha, beta = beta, lambda = lambda)$value
 }
 return(y)
}
lornz <- function(alpha, beta, lambda, p) {
 q = quantile(alpha, beta, lambda, p)
 mu = BellW_mom(alpha, beta, lambda, 1)$value
 y <- (Ia(alpha, beta, lambda, q))/(mu)
 return(y)
}
@

\noindent The following R code is provide the graphical illustration of Bonferroni curve
<<>>=
Ia = function(alpha, beta, lambda, a) {
  n = length(a)
  y = 0
  for (i in 1:n) {
    y[i] = integrate(function(alpha, beta, lambda, x) {
      x * pdf_BellW(alpha, beta, lambda, x)
    }, lower = 0, upper = a[i], subdivisions = 1e+05, alpha = alpha,
      beta = beta, lambda = lambda)$value
  }
  return(y)
}
bonf <- function(alpha, beta, lambda, p) {
  q = quantile(alpha, beta, lambda, p)
  mu = BellW_mom(alpha, beta, lambda, 1)$value
  y <- (Ia(alpha, beta, lambda, q))/(p * mu)
  return(y)
}
@

<<bon plot, fig.cap="Lorenz and Bonferroni curve.",fig.pos="t", fig.height=6, fig.width=11,fig.align='center'>>=
par(mfrow = c(1, 2))
curve(lornz(0.2, 1, 0.3, x), ylab = expression(paste(L(pi))),
 xlab = expression(paste(pi)), xlim = c(0, 1), ylim = c(0,
  1), lty = 1, col = 1, lwd = 3)
curve(lornz(0.4, 1, 0.9, x), add = T, lty = 2, col = 2,
 lwd = 3)
legend("topleft", c(expression(alpha == 0.2 ~ ~beta == 1 ~
 ~lambda == 0.3), expression(alpha == 0.4 ~ ~beta ==
 1 ~ ~lambda == 0.9)), lty = c(1:2), col = c(1:2), lwd = c(2,
 2))
curve(bonf(0.2, 1, 0.3, x), ylab = expression(paste(B(pi))),
  xlab = expression(paste(pi)), xlim = c(0, 1), ylim = c(0,
    1), lty = 1, col = 1, lwd = 3)
curve(bonf(0.4, 1, 0.9, x), add = T, lty = 2, col = 2, lwd = 3)
legend("topleft", c(expression(alpha == 0.2 ~ ~beta == 1 ~
 ~lambda == 0.3), expression(alpha == 0.4 ~ ~beta ==
 1 ~ ~lambda == 0.9)), lty = c(1:2), col = c(1:2), lwd = c(2,
 2))
@


\subsection{Applications to real data}
The application section is a crucial component for highlighting the model's adaptability, flexibility, and versatility while taking into consideration the empirical data. The commonly employed R package to estimate model parameters along with standard errors of the estimates with several goodness-of-fit tests including Akaike information criterion (AIC), corrected Akaike information criterion (CAIC), Bayesian information criterion (BIC), Hannan-Quinn information criterion (HQIC), Cram\'er-Von Mises test (W), Anderson-Darling(A) and  Kolmogorov-Smirnov (KS) test. The least information criterion and high P-value of KS test tend to favour the best model when comparing two or more models.

<<warning=FALSE>>=
x<-DataSetsUni::data_vehicleinsur
pdf_BellW <- function(par, x) {
    alpha = par[1]
    beta = par[2]
    lambda = par[3]
    G = (1 - exp(-alpha * x^beta))
    g = alpha * beta * x^(beta - 1) * exp(-alpha * x^beta)
    F = (1 - exp(-exp(lambda) * (1 - exp(-lambda * G))))/(1 -
        (exp(-exp(lambda) + 1)))
    f = lambda * g * exp(lambda * (1 - G)) * exp(-exp(lambda) *
        (1 - exp(-lambda * G)))/(1 - (exp(-exp(lambda) +
        1)))
    return(f)
}
cdf_BellW <- function(par, x) {
    alpha = par[1]
    beta = par[2]
    lambda = par[3]
    G = (1 - exp(-alpha * x^beta))
    g = alpha * beta * x^(beta - 1) * exp(-alpha * x^beta)
    F = (1 - exp(-exp(lambda) * (1 - exp(-lambda * G))))/(1 -
        (exp(-exp(lambda) + 1)))
    f = lambda * g * exp(lambda * (1 - G)) * exp(-exp(lambda) *
        (1 - exp(-lambda * G)))/(1 - (exp(-exp(lambda) +
        1)))
    return(F)
}
res_BellW = AdequacyModel::goodness.fit(pdf = pdf_BellW, cdf = cdf_BellW,
    starts = c(0.0701301002, 0.58007, 1.9), data = x, method = "B",
    domain = c(0, Inf), mle = NULL)
@
\noindent The output summary of estimated parameters along with standrad errors and goodness-of-fit tests is as follows:
<<warning=FALSE>>=
res_BellW
@
\noindent The estimated PDF and CDF of real data based on the BellW distribution is as follows:
<<application plot, fig.cap="Estimated PDF and CDF plot.",fig.pos="t", fig.height=6, fig.width=11,fig.align='center'>>=
par(mfrow = c(1, 2))
# histograme and PDF plot
hist(x, prob = T, main = "", ylim = c(0, 0.04))
curve(pdf_BellW(res_BellW$mle, x), add = T, lty = 1, col = 2,
    lwd = 3)
legend("topright", legend = c("BellW"), col = 2, lty = 1, lwd = 4,
    cex = 1.2)
# CDF plot
plot(ecdf(x), main = "")
curve(cdf_BellW(res_BellW$mle, x), add = T, lty = 1, col = 2,
    lwd = 4)
legend("bottomright", legend = c("BellW"), col = 2, lty = 1,
    lwd = 4, cex = 1.2)
@
\subsection{Kaplan-Maier plot}
\noindent The Kaplan-Meier (KM) curve, a graphical representation of the SF, indicates the probability of an event occurring at a specific time interval. The curve should resemble the genuine survival function for the population under study if the sample size is sufficient.
<<>>=
library(survival)
x <- DataSetsUni::data_vehicleinsur
delta = rep(1, length(x))
x <- sort(x)
km = survfit(Surv(x, delta) ~ 1)
alpha = 0.01427632
beta = 1.08430563
lambda = 1.45786859
sf <- function(x) {
    (1 - ((1 - exp(-exp(lambda) * (1 - exp(-lambda * (1 - exp(-alpha *
        x^beta))))))/(1 - (exp(-exp(lambda) + 1)))))
}
@

\subsection{Probability-probability plot}
\noindent A P-P plot, also known as a probability-probability plot, is a probability plot used in statistics to determine how closely two data sets agree or how well a data set fits a certain model.
<<>>=
x<-DataSetsUni::data_vehicleinsur
cdf_BellW = function(x, alpha, beta, lambda) {
    G = (1 - exp(-alpha * x^beta))
    g = alpha * beta * x^(beta - 1) * exp(-alpha * x^beta)
    F = (1 - exp(-exp(lambda) * (1 - exp(-lambda * G))))/(1 -
        (exp(-exp(lambda) + 1)))
}
x = sort(x)
n = length(x)
Fn = seq(1, n)/n
@

<<km plot, fig.cap="P-P and Kaplan-Maier plot.",fig.pos="t", fig.height=6, fig.width=11,fig.align='center'>>=
par(mfrow = c(1, 2))
# P-P plot
plot(Fn, cdf_BellW(x, 0.014, 1.084, 1.458), xlab = "x", ylab = "P-P Plot",
    pch = 21, col = "red", bg = "red")
abline(0, 1, lwd = 3)
legend("topleft", c("BellW"), col = "red", lty = c(1), bty = "n",
    cex = 1.2, lwd = 3)
# Kaplan-Maier plot
plot(km, conf.int = FALSE, ylab = "Survival Probability", xlab = "x",
    xlim = c(0, 200.8), lty = 1, col = "1", pch = 19, lwd = 3)
lines(seq(0, 200, length.out = 100), sf(seq(0, 200, length.out = 100)),
    col = "red", lty = 5, lwd = 4)
legend("topright", c("K-M", "BellW"), col = c("black", "red"),
    lty = c(1, 5), bty = "n", cex = 1.2, lwd = 3)
@


\section{Discrete Probability Distributions}

If a discrete random variable's possible values are represented in the probability distribution along with the accompanying probabilities, the distribution is said to be discrete. Such a distribution will be used to describe data that have a limited (finite) number of outcomes that can be counted. As an illustration, consider the number of children in a household, the roll of a fair die, which has six possible outcomes (1, 2, 3, 4, 5, and 6), and the toss of a coin, which has two possible outcomes (heads or tails). The discrete distributions that are encountered most commonly are the negative binomial distribution~\cite{dWeisstein2003}, the binomial distribution~\cite{dWeisstein2002}, the Poisson distribution~\cite{dClarke1946}, the geometric distribution~\cite{dChattamvelli2022}, the hypergeometric distribution~\cite{dSkibinsky1970}, the Bell distribution~\cite{dCastellares2018}, and the Bell-Touchard distribution~\cite{dCastellares2020}.

\subsection{Binomial distribution}
A random variable is said to follow a binomial distribution if the experiment is repeated fix number of times with two possible outcomes either success or failure, the successive trails are independent and the probability of success remains same from one trail to another trail. It is defined by the following PMF:

\begin{equation}
P(X=x)=\binom{n}{x}p^{x}q^{n-x},
\end{equation}
where $x\in(0, 1, 2,\cdots, n)$, $p\in(0,1)$ and $q=1-p$.
\begin{itemize}
\item dbinom computes the probability mass function
\item pbinom computes the cumulative distribution function
\item qbinom computes the quantile function
\item rbinom computes the random values
\end{itemize}

\noindent R in-built syntax to compute the PMF  of binomial distribution~\cite{dRCoreTeam2021} is as follows:
<<eval=FALSE>>=
# syntax
dbinom(x, size, prob)
@
\noindent For example, a coin is tossed twice or two coins are tossed together.  (i) What is the chance of getting both heads? that is 0.25. (ii) What is the chance of getting one head? that is 0.50. When tossing a coin twice, we have the total possible outcomes and as follows: (HT, TH, HH, TT).

<<>>=
# what is the chance of getting both heads?
dbinom(x = 2, size = 2, prob = 0.5)
# what is the chance of getting one head?
dbinom(x = 1, size = 2, prob = 0.5)
@
\noindent A fair coin is tossed 8 times and X represents the number of heads. Find the followings
(i) plot the probability distribution for X (ii) at least four heads (iii) at most 2 heads


<<PMFbinomial plot, fig.cap="Graphical illustration of PDF.",fig.pos="t", fig.height=6, fig.width=11,fig.align='center'>>=
par(mfrow = c(1, 2))
x = c(0:5)
prob <- dbinom(x, size = 8, prob = 0.75)
data.frame(x, prob)
plot(x, prob, type = "h", col = "red", lwd = 4)
prob1 <- dbinom(x, size = 2, prob = 0.15)
data.frame(x, prob1)
plot(x, prob1, type = "h", col = "red", lwd = 4)
@

\noindent The probability of getting at least 4 heads that is $P(X\ge4)=1-P(X<4)$. R in-built syntax to compute the CDF  of binomial distribution is as follows:
<<eval=FALSE>>=
# syntax
pbinom(q, size, prob, lower.tail = FALSE)
@
<<>>=
# at least four heads
1 - (dbinom(x = 0, size = 8, prob = 0.5) + dbinom(x = 1, size = 8,
    prob = 0.5) + dbinom(x = 2, size = 8, prob = 0.5) + dbinom(x = 3,
    size = 8, prob = 0.5))
# we can alternatively can be computed the at least four heads
pbinom(q = 3, size = 8, prob = 0.5, lower.tail = FALSE)
@

\noindent The probability of getting at most 2 heads that is $P(X\le2)=P(X=0)+P(X=1)+P(X=2)$
<<>>=
dbinom(x = 0, size = 8, prob = 0.5) + dbinom(x = 1, size = 8,
    prob = 0.5) + dbinom(x = 2, size = 8, prob = 0.5)
@

\noindent Suppose that X follows a binomial distribution with $n$ (size)=6 and $p$=0.25. Find out (i) P(X=4) and (ii) P(X$\ge$1)

<<>>=
# the chance of getting P(X=4)
dbinom(x = 4, size = 6, prob = 0.25)
# the chance of getting at least one success
1 - dbinom(x = 0, size = 6, prob = 0.25)
# we can also evaluates the chance of getting at least one success
pbinom(q = 0, size = 6, 0.25, lower.tail = FALSE)
@


<<CDFbinomial plot, fig.cap="Graphical illustration of CDF.",fig.pos="t", fig.height=6, fig.width=11,fig.align='center'>>=
par(mfrow = c(1, 2))
x = c(0:5)
prob <- pbinom(x, size = 8, prob = 0.85)
data.frame(x, prob)
plot(x, prob, type = "l", col = "red", lwd = 4)
prob1 <- pbinom(x, size = 4, prob = 0.35)
data.frame(x, prob1)
plot(x, prob1, type = "l", col = "red", lwd = 4)
@



\noindent The random variate based on the binomial distribution can be computed using the following syntax:
<<eval=FALSE>>=
# syntax
pbinom(n, size, prob)
@
\noindent If we wish to draw a random sample of 10 from binomial distribution with size=20 and probability of success is 0.5 that is given by:
<<>>=
rbinom(10, 20, 0.5)
@

\subsection{Poisson distribution}
\noindent The Poisson distribution is a discrete distribution that calculates the likelihood that a certain number of events will occur within a particular span of time. For instance, the arrival of emergency call every one hour. In the 24 hours prior, how many people visited the certain internet page. It is a limiting case of binomial distribution. It is widely used to model the count data in many practical domains. It is defined by the following PMF:
\begin{equation}
P(X=x)=\frac{\lambda^{x}e^{-\lambda}}{x!},
\end{equation}
where $x\ge0$ and $\lambda>0,$ is the only rate parameter of the Poisson distribution. It is important to note that the Poisson distribution has mean and variance equal to $\lambda$.
\begin{itemize}
\item dpois computes the probability mass function
\item ppois computes the cumulative distribution function
\item qpois computes the quantile function
\item rpois computes the random values
\end{itemize}
The following syntax can be used to specify the PMF of the Poisson distribution~\cite{dRCoreTeam2021}:
<<eval=FALSE>>=
# syntax
dpois(x, lambda, log = FALSE)
@


<<PMF plot, fig.cap="Graphical illustration of PMF of Poisson distribution.",fig.pos="t", fig.height=6, fig.width=11,fig.align='center'>>=
par(mfrow = c(1, 2))
prob = dpois(0:8, 2, log = FALSE)
plot(prob, type = "h", lwd = 4, col = "red", xlab = "X")
prob1 = dpois(0:8, 0.8, log = FALSE)
plot(prob1, type = "h", lwd = 4, col = "red", xlab = "X")
@

\noindent The function ppois computes the CDF of the Poisson distribution:
<<eval=FALSE>>=
# syntax
ppois(q, lambda, lower.tail = TRUE, log.p = FALSE)
@

<<>>=
round(ppois(0:8, lambda = 2.5), 4)
@
\noindent The function qpois~\cite{dRCoreTeam2021} computes the QF of the Poisson distribution:
<<eval=FALSE>>=
# syntax
qpois(p, lambda, lower.tail = TRUE, log.p = FALSE)
@
<<>>=
qpois(0.8, 2)
@
\noindent The function rpois~\cite{dRCoreTeam2021} computes the random sample based on the Poisson distribution:
<<eval=FALSE>>=
# syntax
rpois(n, lambda)
@
\noindent If we wish to draw a random sample of 8 from the Poisson distribution with rate parameter $\lambda=1.5$, that is given by:
<<>>=
rpois(8, 1.5)
@

\subsection{Estimation}
Here, we show the how the maximum likelihood estimator is obtained by taking Possion distribution as for illustration. First of all take likelihood function as shown in Eq.~\eqref{mle1} and then log-likelihood function is obtained just taking log of the PMF (see Eq.~\eqref{mle2}). The maximum likelihood estimator for $\lambda$ is yielded by differentiating Eq.~\eqref{mle3} with respect to $\lambda$ and solve for $\lambda$. The likelihood function of a Poisson distribution is as follows:
\begin{equation}\label{mle1}
L(\lambda,x_{i})=\prod_{i=1}^{n}\frac{e^{-\lambda}\lambda^{x_{i}}}{x_{i}!}.
\end{equation}
By taking the log of the above function, then the log-likelihood function is given by

\begin{equation}\label{mle2}
l(\lambda,x_{i})=\ln\left(\prod_{i=1}^{n}\frac{e^{-\lambda}\lambda^{x_{i}}}{x_{i}!}\right)=\sum_{i=1}^{n}\ln\left(\frac{e^{-\lambda}\lambda^{x_{i}}}{x_{i}!}\right),
\end{equation}

applying log, we get

\begin{equation}
l(\lambda,x_{i})=\sum_{i=1}^{n}\left[-\lambda+x_{i}\ln(\lambda)-\ln(x!_{i})\right],
\end{equation}

after applying summation, yields

\begin{equation}\label{mle3}
l(\lambda,x_{i})=-n\lambda+\ln(\lambda)\sum_{i=1}^{n}x_{i}-\sum_{i=1}^{n}\ln(x!_{i}).
\end{equation}
The maximum likelihood estimator for $\lambda$ that is the sample mean is as follows:
\begin{equation}
\hat{\lambda_{n}}=\frac{1}{n}\sum_{i=1}^{n}x_{i}.
\end{equation}
\noindent The above mathematical illustration is now translated into R codes and maxLik R package~\cite{dArne2011} is used to estimate the parameter $\lambda$ by taking initial parameter value as 0.2 to run the function. The summary function provides the maximum likelihood estimator along with its respective standard error, the log-likelihood and t-test value and as follows:
<<>>=
x <- countDM::data_sbirth
n <- length(x)
loglik <- function(p) {
    lambda <- p[1]
    loglik <- sum(x) * log(p[1]) - n * p[1] - sum(lgamma(x + 1))
    return(loglik)
}
res <- maxLik::maxLik(loglik, start = c(0.2))


summary(res)
@
\noindent The AIC of the model is obtained by using AIC() function as follows:
<<>>=
AIC(res)
@
\noindent Another way to obtain the maximum likelihood estimator using the PMF and is as follows:
<<>>=
x <- countDM::data_sbirth
pdf_pois <- function(param) {
    lambda <- param[1]
    ll = sum(dpois(x, lambda, log = TRUE))
    return(ll)
}
mle <- maxLik::maxLik(pdf_pois, start = c(0.023))
summary(mle)
@


\subsection{Zero-Inflated model}

We frequently come across the situations where the data contains a huge number of zeros, such as when we get information regarding crime and show that a significant portion of people never commit any crimes at all or zero crime. Therefore, inflated models are crucial and usually utilized in such circumstances. Researchers frequently use the Poisson, negative binomial, geometric, and binomial distributions to model count data with a high fraction of zeros. The VGAM R package~\cite{dThomas2015} permits to model the zero-inflated geomatric distribution (zigeom), the zero-inflated negative binomial (zinegbin) distribution, the zero-inflated binomial distribution (zibinom) and the zero-inflated Poisson distribution (zipois). The Rfast2 and countDM R packages~\cite{dMuhammad2023} provide the maximum likelihood estimation of these models. Here, we illustrate the R package maxLik~\cite{dArne2011} to maximize the log-likelihood function of the zero inflated Poisson model and as follows:

<<warning=FALSE>>=
x <- countDM::data_sbirth
no <- sum(x == 0)
    n <- length(x)
    n1 <- n - no
    x1 <- x[x > 0]
    loglik <- function(p) {
        alpha <- p[1]
        lambda <- p[2]
        loglik <- no * log(p[1] + (1 - p[1]) * exp(-p[2])) +
            n1 * log(1 - p[1]) + sum(dpois(x1, p[2], log = TRUE))
    }
    result <- maxLik::maxLik(loglik, start = c(0.2, 1.5))
   summary(result)
@
\noindent We can also use the countDM R package~\cite{dMuhammad2023} to evaluate the maximum likelihood estimate after specifying the initial parametric values and data vector x and as follows:

<<warning=FALSE>>=
x <- countDM::data_sbirth
countDM::mle_zip (x, 0.2, 1.5)
@

\noindent We can also maximize the PMF to evaluate the maximum likelihood estimate and as follows:
<<>>=
x <- countDM::data_sbirth
pdf_zip <- function(param) {
    lambda <- param[1]
    pi <- param[2]

    ll = sum(VGAM::dzipois(x, lambda, pi, log = TRUE))
    return(ll)
}
MLE <- maxLik::maxLik(pdf_zip, start = c(0.023, 0.2))
summary(MLE)
@
\noindent Moreover, the countDM R package~\cite{dMuhammad2023} allows to fit zero-one-inflated models (often the observed data contain the excessive zero's and one's) including the Poisson and the Bell distribution and given by

<<warning=FALSE>>=
x <- countDM::data_sbirth
# zero-one-inflated Poisson model
countDM::mle_zoip (x, 0.2,0.1, 0.5)
# zero-one-inflated Bell model
countDM::mle_zoibell (x, 0.1,0.2,0.2)
@

\begin{thebibliography}{99}

\bibitem{dTahir2016}
Tahir, M. H., \& Cordeiro, G. M. (2016). Compounding of distributions: a survey and new generalized classes. Journal of Statistical Distributions and Applications, 3, 1-35.

\bibitem{dMaurya2021}
Maurya, S. K., \& Nadarajah, S. (2021). Poisson generated family of distributions: a review. Sankhya B, 83, 484-540.

\bibitem{dTahir2015}
Tahir, M. H., \& Nadarajah, S. (2015). Parameter induction in continuous univariate distributions: Well-established G families. Anais da Academia Brasileira de Ciencias, 87, 539-568.




\bibitem{dAlsadat2023}
Alsadat, N., Imran, M., Tahir, M. H., Jamal, F., Ahmad, H., \& Elgarhy, M. (2023). Compounded Bell-G class of statistical models with applications to COVID-19 and actuarial data. Open Physics, 21(1), 20220242.

\bibitem{dPundir2005}
Pundir, S., Arora, S., \& Jain, K. (2005). Bonferroni curve and the related statistical inference. Statistics \& probability letters, 75(2), 140-150.

\bibitem{dWeisstein2003}
Weisstein, E. W. (2003). Negative Binomial Distribution. https://mathworld. wolfram. com/.

\bibitem{dWeisstein2002}
Weisstein, E. W. (2002). Binomial distribution. https://mathworld. wolfram. com/.
\bibitem{dClarke1946}
Clarke, R. D. (1946). An application of the Poisson distribution. Journal of the Institute of Actuaries, 72(3), 481-481.
\bibitem{dChattamvelli2022}
Chattamvelli, R., \& Shanmugam, R. (2022). Discrete distributions in engineering and the applied sciences. Springer Nature.
\bibitem{dSkibinsky1970}
Skibinsky, M. (1970). A characterization of hypergeometric distributions. Journal of the American Statistical Association, 65(330), 926-929.

\bibitem{dCastellares2018}
Castellares, F., Ferrari, S. L., \& Lemonte, A. J. (2018). On the Bell distribution and its associated regression model for count data. Applied Mathematical Modelling, 56, 172-185.

\bibitem{dCastellares2020}
Castellares, F., Lemonte, A. J., \& Moreno-Arenas, G. (2020). On the two-parameter Bell-Touchard discrete distribution. Communications in Statistics-Theory and Methods, 49(19), 4834-4852.

\bibitem{dRCoreTeam2021}
R Core Team (2021). R: A language and environment for statistical
  computing. R Foundation for Statistical Computing, Vienna, Austria.
  URL https://www.R-project.org/.

\bibitem{dArne2011}
Arne Henningsen and Ott Toomet (2011). maxLik: A package for maximum
  likelihood estimation in R. Computational Statistics 26(3), 443-458.
  DOI 10.1007/s00180-010-0217-1.

\bibitem{dThomas2015}
Thomas W. Yee (2015). Vector Generalized Linear and Additive Models:
  With an Implementation in R. New York, USA: Springer.

\bibitem{dMuhammad2023}
Muhammad Imran, M.H. Tahir and Saima Shakoor (2023). countDM:
  Estimation of Count Data Models. R package version 0.1.0.
  https://CRAN.R-project.org/package=countDM

\end{thebibliography}




\chapter{Time Series Analysis}

\section{Introduction}
This Chapter focuses on the following time series related topics: time series analysis, time series analysis related R packages, time series data related R packages, time series data and its frequency, autocorrelation and partial autocorrelation, time series display, simple moving average with forecast and accuracy measures, graphical illustration, simple or single exponential smoothing, Holt's model, Hotlt-Winter's model with multiplicative seasonal component, Hotlt-Winter's model with additive seasonal component with application, Box-Jenkins approch, Augmented Dickey-Fuller test, Ljung-Box test, auto.arima() function, graphical representation of residuals, Seasonal ARIMA model, ETS model, ANN model and models comparison to COVID-19 related deaths. This Chapter based on the following R packages:stats~\cite{RCoreTeam2021}, forecast~\cite{Hyndman2023}, ggplot2~\cite{HWickham2016}, fpp2~\cite{Rob2023}, gridExtra~\cite{Baptiste2017}, tseries~\cite{Adrian2023}, hivdata~\cite{Muhammad2023}.


\subsection{Time series analysis}
Time series analysis is an important to formulate effective future policies using the historical data. The collection of data by successive time periods is called a time series, for instance, the weekly recorded temperature, monthly reported HIV cases, annual production of wheat, etc. A set of statistical procedures used to analyze the time series data is called time series analysis. Time series analysis often has two goals: first, it aims to comprehend or model the random mechanism that produces the observed series; second, it forecasts the series' future values based on its historical data. The time series is composed of four elements: seasonal fluctuation, cyclical variation, random or accidental or irregular variation, and secular or drift trend. Time series data defines the phenomenon being examined at over specific points in time, allowing for the analysis of fluctuations in variables over time and inspection of how they change over time. It is used in a wide range of disciplines, including economic forecasts~\cite{Ghysels2018, Stock2001}, budgetary analysis~\cite{Brady2017}, financial markets and the cryptocurrency market~\cite{Ampountolas2023}, stock markets~\cite{Horak2023},  medical and infectious diseases~\cite{Bobeica2023}-\cite{Bracher2022}, demographic forecasts~\cite{Lassila2014, Melchior2021},  production forecasts~\cite{Yu2022}, agriculture~\cite{Ahmar2023, Aliu2023}, and weather~\cite{Giunta2023, Sun2022}, etc.

\subsection{Time series analysis related R packages}
Here, we provide some useful and widely used R packages for time series and forecasting analysis, both univariate and multivariate time series such as {\sf fable}~\cite{Mitchell2023} (models of forecasting for tidy time series), {\sf forecast}~\cite{Hyndman2023} (linear models and time series forecasting functions), {\sf TSstudio}~\cite{Rami2023} (time series analysis and forecasting functions), {\sf timeSeries}~\cite{Diethelm2023} (time series objects in finance) and {\sf tseries}~\cite{Adrian2023} (time series analysis and computational finance)

\subsection{Time series data related R packages}
R enables the efficient handling and application of diverse data sets. Here, we offer a some popular and practical R packages related to time series data including {\sf tsibble}~\cite{wang2023} (diverse time series data sets), {\sf fpp2}~\cite{Rob2023} (the data sets are derived from the second edition of "Forecasting: Principles and Practice"), {\sf expsmooth}~\cite{Rob2015} (the data sets are derived from "Forecasting with Exponential Smoothing"), {\sf hivdata}~\cite{Muhammad2023} (HIV and ART reported monthly cases in Pakistan for male, female, children and transgender) and {\sf fma}~\cite{HyndmanRJ2023} (The data sets are derived from "Forecasting: Methods and Applications").

\subsection{Data and its frequency}
Time series data and frequency are crucial for analysis while dealing with R. Specifically, the ts() function is used to convert data into a time series data vector. It is possible to use the following syntax:
<<eval=FALSE>>=
stats::ts(x, start, frequency)
@
\noindent As an illustration, we have monthly departmental sales data spanning two years, from January 2016 to December 2017. The information is as follows: 988,	1014,	1040,	1117,	1158,	1195,	1204,	1245,	1278, 1244,	1275,	1311,	1446,	1482,	1505,	1588,	1632,	1678, 1733,	1789,	1831,	1875,	1922, 2014. Here, we transform the data into a time series display that goes from January 2016 to December 2017 using the ts() function.
<<>>=
# data vector
x <- c(988, 1014, 1040, 1117, 1158, 1195, 1204, 1245, 1278, 1244, 1275,
    1311, 1446, 1482, 1505, 1588, 1632, 1678, 1733, 1789, 1831, 1875,
    1922, 2014)
# creating time series object
data_sale <- stats::ts(x, start = 2016, frequency = 12)
data_sale
@
\noindent We use frequency values of 1 for annual data, 4 for quarterly data, 12 for monthly data, and 52 for weekly data.

<<>>=
x <- ts(c(10, 14, 8, 25, 16, 22, 14, 35, 15, 27, 18, 40, 28, 40, 25, 65),
    start = 2009, frequency = 4)
x
@

\subsection{Autocorrelation and partial autocorrelation}
Autocorrelation and partial autocorrelation are two crucial measures that are frequently employed to highlight the characteristics of time series. The measures help in assessing the relationship between the present and past series values and also determine which historical series values are most predictive of future values. The auto-regressive (AR) order indicated by $p$ can be evaluated with the use of the partial Autocorrelation function (PACF). Conversely, the moving average (MA) order, denoted by $q$, is usefully disclosed by the Autocorrelation function (ACF). Furthermore, the ACF assesses whether the series is stationary or not. Along with evaluating the trend and seasonality, it also evaluates the randomness and unpredictability of the time series.

\begin{itemize}
\item The PACF captures the degree of relationship between $y_t$ and $y_{t-k}$, when other time lags are eliminated or the influence between intervening observations is not taken into account. To determine the correlation, for instance, between $y_t$ and $y_{t-2}$, we must exclude the impact of $y_1$, the intervening data.
\item The ACF captures the correlation of the given time series with itself at various or different lags.
\end{itemize}
\noindent The following Figure~\ref{fig:ACF and PACF plots} shows the ACF (left) and PACF (right) plots of the following time series. Any thing between these blue doted lines is statistically insignificant. The first two lags in ACF plot are statistically significant and on the other side, the PACF has only first lag.
<<ACF and PACF plots, fig.cap="ACF and PACF plots.",fig.height=6, fig.width=11,fig.align='center', warning=FALSE, message=FALSE>>=
library(ggplot2)
x <- ts(c(1, 4, 3, 7, 2, 1, 15, 20, 5, 27, 18, 4, 40, 25, 65, 64, 40, 30,
    20), start = 2009, frequency = 1)
# a plot of ACF
v1 <- forecast::ggAcf(x) + ggtitle("")
# a plot of PACF
v2 <- forecast::ggPacf(x) + ggtitle("")
gridExtra::grid.arrange(v1, v2, nrow = 1)
@

\subsection{Time series display}
The tsdisplay() or ggtsdisplay() function allows to quick visual inspection of time series along-with ACF and PACF plots.
<<warning=FALSE, message=FALSE, fig.cap="Visual inspection of time series using ACF and PACF plots.">>=
library(ggplot2)
forecast::ggtsdisplay(fpp2::austourists)
@

\noindent The diff() function evaluates the difference of the series with ACF and PACF plots. The first order difference of the given time series can be obtained using the codes shown below.
<<fig.cap="Time series display after differencing.", eval=FALSE>>=
x <- fpp2::austourists
forecast::ggtsdisplay(diff(x), plot.type = "partial",
    theme = ggplot2::theme_bw())
@

\subsection{Simple moving average}
In time series analysis, the simple moving average model (SMA), is a common approach for modeling univariate time series.That is simply, the average over a given time period and placed in the middle. Then, skip the first period observation and add the next period value and so on. The computation of 3-year SMA is illustrated in the following Table~\ref{ma1}.

\begin{table}[H]
  \centering
  \caption{A simple illustration of 3-year SMA.}
    \begin{tabular}{ccc}
    \hline
    $x_t$     & 3-year SMA    & Computation \\
    \hline
    150   &       &  \\
    175   & 171.67 & (150+175+190)/3=171.67 \\
    190   & 188.33 & (175+190+200)/3=188.33 \\
    200   & 195.00 & (190+200+195)/3=195.00 \\
    195   & 203.33 & (200+195+215)/3=203.33 \\
    215   & 202.67 & (195+215+198)/3=202.67 \\
    198   &       &  \\
    \hline
    \end{tabular}%
  \label{ma1}%
\end{table}%
\noindent The SMA can be computed using ma(x, order) syntax, where $x$ represents the data vector and the argument order denotes the order of SMA (the two period or three period etc). The three-year SMA can be assessed using the following codes:
<<message=FALSE, warning=FALSE>>=
x <- ts(c(150, 175, 190, 200, 195, 215, 198), start = 1935, frequency = 1)
# 3-year SMA
forecast::ma(x,3)
@
\noindent The demonstration of the 2-year SMA (center) is shown in Table~\ref{ma2}.
\begin{table}[H]
  \centering
  \caption{A simple illustration of 2-year SMA (center)}
    \begin{tabular}{ccclr}
    \hline
    x     & 2-Year SMA & 2-Year SMA (centre) & \multicolumn{2}{c}{Computation} \\
    \hline
    150   &       &       &       &  \\
    175   & 162.5 & 172.50 & (150+175)/2=162.5 & \multicolumn{1}{l}{(162.5+182.5)=172.50} \\
    190   & 182.5 & 188.75 & (175+190)/2=182.5 & \multicolumn{1}{l}{(182.5+195.0)=188.75} \\
    200   & 195.0 & 196.25 & (190+200)/2=195.0 & \multicolumn{1}{l}{(195.0+197.5)=196.25} \\
    195   & 197.5 & 201.25 & (200+195)/2=197.5 & \multicolumn{1}{l}{(197.5+205.0)=201.25} \\
    215   & 205.0 & 205.75 & (195+215)/2=205.0 & \multicolumn{1}{l}{(205.0+206.5)=205.75} \\
    198   & 206.5 &       & (215+198)/2=206.5 &  \\
     \hline
    \end{tabular}%
  \label{ma2}%
\end{table}%
\noindent Using ma() function, the 2-year SMA can be computed after specifying the data vector
and order of moving average (in our case it is 2) and follows:
<<message=FALSE, warning=FALSE>>=
x <- ts(c(150, 175, 190, 200, 195, 215, 198), start = 1935, frequency = 1)
# 2-year SMA
forecast::ma(x,2)
@
\noindent Here, the 2-year and 7-year SMAs are calculated using the monthly reported male HIV cases in Pakistan. The data set is obtained using hivdata R package.
<<Moving average plots, fig.cap="A plot of actual and predicted based on the 2-year and 7-year SMA.",fig.height=4, fig.width=11,fig.align='center', warning=FALSE, message=FALSE>>=
library(ggplot2)
# Monthly reported HIV cases in Pakistan data
x <- hivdata::malehiv
# 2-year SMA
mov2 <- forecast::ma(x, 2)
m2 <- forecast::autoplot(x, series = "Data") + autolayer(mov2,
 series = "2-SMA") + xlab("Year") + ylab("HIV Cases") +
 ggtitle("HIV Cases") + scale_colour_manual(values = c(Data = "grey7",
 `2-SMA` = "red"), breaks = c("Data", "2-SMA"))
# 7-year SMA
mov7 <- forecast::ma(x, 7)
m7 <- forecast::autoplot(x, series = "Data") + autolayer(mov7,
 series = "7-SMA") + xlab("Year") + ylab("HIV Cases") +
 ggtitle("HIV Cases") + scale_colour_manual(values = c(Data = "grey7",
 `7-SMA` = "red"), breaks = c("Data", "7-SMA"))
gridExtra::grid.arrange(m2, m7, ncol = 2)
@
\noindent A forecast for the next two periods along with 80\% and 95\% lower and upper confidence interval are obtained as follows:
<<warning=FALSE>>=
fit1 <- forecast::forecast(mov2, h=2)
fit1
fit2 <- forecast::forecast(mov7, h=2)
fit2
@
\noindent The results of the accuracy measures based on the 2-year and 7-year SMA indicate that the 7-year SMA has the lowest accuracy measures; for example, the 7-year SMA's RMSE is lower than the 2-year SMA's, which are 116.4172 and 294.7793.
<<warning=FALSE>>=
round(forecast::accuracy(fit1), 4)
round(forecast::accuracy(fit2), 4)
@

\subsection{Simple or single exponential smoothing}
Exponential smoothing forecasting models are often utilized for short-term forecast. Data without a seasonal pattern or trend are smoothed using the Simple Exponential Smoothing (SES) method. It is also known as Brown's model~\cite{Brown1956}. These models use exponentially decreasing weights over time. For instance, more weight is given to the most recent values and less weight is given to the more past values. The SES has only one parameter known as the smoothing factor or smoothing coefficient and denoted by $\alpha$. Usually, the value of $\alpha$ is specified to be between 0 and 1. Large values of smoothing coefficient suggest that the model primarily focuses on the most recent past observations, while smaller value of smoothing coefficient indicate that less weight is given to the past value when making predictions. When $\alpha$ closer to 1 signify fast learning, whereas forecasts are influenced solely by the latest data, while if $\alpha$ closer to 0 indicate slow learning, when forecasts are heavily influenced by previous observations.

\begin{equation}
F_{t+1}=\alpha A_{t}+(1-\alpha)F_{t},
\end{equation}
where $A_t$ and $F_t$ are actual and foretasted value at time 't', respectively and $F_{t+1}$ represent the one-period-ahead forecast observation. Table~\ref{Brown} presents the forecast using Brown's model. The first forecast value is considered and same as the first actual value. We use smoothing parameter 0.4 and one period-ahead forecast is computed for the year 1941 that that is 199.
\begin{table}[htbp]
  \centering
  \caption{Forecast using Brown's model}
    \begin{tabular}{cccc}
    \hline
    Year  & $A_t$ & $F_t$ & Computation\\
    \hline
    1935  & 150   & 150   &       150      \\
    1936  & 175   & 150   &       0.4 $\times$ 150 + (1 - 0.4) $\times$ 150 = 150\\
    1937  & 190   & 160   &       0.4 $\times$ 175 + (1 - 0.4) $\times$ 150 = 160\\
    1938  & 200   & 172   &       0.4 $\times$ 190 + (1 - 0.4) $\times$ 160 = 172\\
    1939  & 195   & 183   &       0.4 $\times$ 200 + (1 - 0.4) $\times$ 172 = 183\\
    1940  & 215   & 188   &       0.4 $\times$ 195 + (1 - 0.4) $\times$ 183 = 188\\
    1941  &       & 199   &       0.4 $\times$ 215 + (1 - 0.4) $\times$ 188 = 199\\
    \hline
    \end{tabular}%
  \label{Brown}%
\end{table}%
The following R codes can be used to evaluate the Brown's model one period-ahead point forecast along with 80\% and 95\% confidence interval and as follows:
<<message=FALSE>>=
x <- ts(c(150, 175, 190, 200, 195, 215), start = 1935, frequency = 1)
fit <- forecast::ses(x, alpha = 0.4)
forecast::forecast(fit, h = 1)
@
\noindent Here, we presents the Brown's method application to annual average sunspot area time
series data. The data set is obtained using fpp2 R package [23] (the data sets are derived
from the second edition of "Forecasting: Principles and Practice"). The graphical illus-
tration of time series, ACF and PACF of annual average sunspot area data are shown in
Figure~\ref{fig:brown oil plot}.
<<brown oil plot, fig.cap="Graphical illustration of time series, ACF and PACF of annual average sunspot area data.",fig.pos="t", fig.height=4, fig.width=9,fig.align='center', warning=FALSE>>=
library(ggplot2)
x2 <- fpp2::sunspotarea
v1 <- forecast::autoplot(x2)
v2 <- forecast::ggAcf(x2) + ggtitle("")
v3 <- forecast::ggPacf(x2) + ggtitle("")
gridExtra::grid.arrange(v1, v2, v3, nrow = 1)
@

\noindent Here, we use time series data related to the daily averages of the sunspot areas for the full
sun over a year, expressed in millionths of a hemisphere. Magnetic areas that resemble dark patches on the sun's surface are known as sunspots. From May of 1874 to June of
1976, the Royal Greenwich Observatory recorded sunspot observations every day. The
Holtwinters() function from forecast library allows to fit Brown model taking parameter
beta=FALSE and gamma=FALSE and given by

<<message=FALSE>>=
x2<-fpp2::sunspotarea
Brown <- HoltWinters(x2, beta=FALSE, gamma = FALSE)
Brown
@
\noindent The h = 6 period ahead forecast along with 80\% and 95\% lower and upper confidence
interval of annual average sunspot area time series based on the Browm model data is
given by:

<<>>=
fit <- forecast::forecast(Brown, h = 6)
fit
@

\noindent The accuracy measures of the fitted Brown model can be simply yielded using accuracy()
function from forecast library and given by:
<<>>=
round(forecast::accuracy(fit), digit = 4)
@
\noindent Figure 5.5 displays the estimated and actual values graphically (left panel), while the
right panel displays the actual and six-month forecast along with 80\% and 95\% lower and
upper confidence interval.




<<brownn method plot, fig.cap="Graphical illustration of actual and fitted and forecasted of annual average sunspot area data.",fig.pos="t", fig.height=4, fig.width=10,fig.align='center', warning=FALSE, message=FALSE, echo=FALSE>>=
library(ggplot2)
x2<-fpp2::sunspotarea
BRO <- HoltWinters(x2, beta=FALSE, gamma = FALSE)
fit<-forecast::forecast(BRO,h=6)
v1<-autoplot(x2, series="Actual") +
  autolayer(fitted(fit), series="Fitted") +

  xlab("Year") +
  ylab("x") +
  ggtitle("Annual average sunspot area") +
  guides(colour=guide_legend(title="Brown's forecast"))

v2<-autoplot(x2) +
  autolayer(fit, series="Forecast") +

  xlab("Year") +
  ylab("x") +
  ggtitle("Brown's Method") +
  guides(colour=guide_legend(title=""))
gridExtra::grid.arrange(v1, v2, ncol=2)
@



\subsection{Holt's Model}
The trend component is included in Holt's model, often known as double exponential smoothing, which is an extension of the SES approach. The following is the Holt's model:
\begin{equation}
F_{t+1}=L_{t}+T_{t},
\end{equation}

where
\begin{equation}\label{level}
L_{t}=\alpha y_{t}+(1-\alpha)(L_{t-1}+T_{t-1})\qquad0\le\alpha\le1
\end{equation}
and
\begin{equation}\label{trend}
T_{t}=\beta(L_{t}-L_{t-1})+(1-\beta)T_{t-1}\qquad0\le\beta\le1,
\end{equation}
represent the level and trend component respectively. The initial value for performing the double exponential smoothing that is for $L_1$=$y_1$. On the other side, the $T_1$ can be initially set up either of the following three combinations $T_{1}=y_{2}-y_{1}$, $T_{1}=\frac{1}{3}\left[\left(y_{2}-y_{1}\right)+\left(y_{3}-y_{2}\right)+\left(y_{4}-y_{3}\right)\right]$ or $T_{1}=\frac{y_{n}-y_{1}}{n-1}$. Table ~\ref{holtt} shows the implementation of the Holt's method and one period ahead forecast is obtained. The level smoothing parameter $\alpha=0.4$ and the trend smoothing parameter $\beta=0.1$ is used. The initial value of level component ($L_t$) and trend component ($T_t$) respectively, can be considered and as follows: ($L_{1}=y_1$) that is ($L_{1}=650$) and ($T_{1}=y_{2}-y_1$) that is 678-650=28. The level component ($L_2$) and trend component ($T_2$) can be computed using Eq.~\eqref{level} and Eq.~\eqref{trend} respectively and as follows: 0.4 $\times$ 678 + 0.6$\times$(650 + 28) = 678 and 0.1 $\times$ (678 - 650) + 0.9 $\times$ 28 = 28. Similarly, the level component ($L_3$) and trend component ($T_3$) respectively are given by: 0.4 $\times$ 720 + 0.6$\times$(678 + 28) = 711.60 and 0.1 $\times$ (720 - 678) + 0.9 $\times$ 28 = 28.56. The rest of all level and trend components can be computed using the above illustration.

\begin{table}[htbp]
  \centering
  \caption{Forecast using Holt's method.}
    \begin{tabular}{ccccc}
    \hline
    year  & yt    & Lt    & Tt    & Forecast \\
    \hline
    2000  & 650.00 & 650.00 & 28.00 &  \\
    2001  & 678.00 & 678.00 & 28.00 & 678.00 \\
    2002  & 720.00 & 711.60 & 28.56 & 706.00 \\
    2003  & 785.00 & 758.10 & 30.35 & 740.16 \\
    2004  & 859.00 & 816.67 & 33.18 & 788.45 \\
    2005  & 920.00 & 877.91 & 35.98 & 849.85 \\
    2006  & 850.00 & 888.33 & 33.43 & 913.89 \\
    2007  & 758.00 & 856.26 & 26.88 & 921.76 \\
    2008  & 892.00 & 886.68 & 27.23 & 883.13 \\
    2009  & 920.00 & 916.35 & 27.47 & 913.91 \\
    2010  & 789.00 & 881.89 & 21.28 & 943.82 \\
    2011  & 844.00 & 879.50 & 18.91 & 903.17 \\
    2012  &       &       &       & 898.42 \\
    \hline
    \end{tabular}%
  \label{holtt}%
\end{table}%

<<>>=
x <- ts(c(650, 678, 720, 785, 859, 920, 850, 758, 892, 920, 789,
    844), start = 2000, frequency = 1)
fit <- HoltWinters(x, alpha = 0.4, beta = 0.1, gamma = FALSE)
forecast::forecast(fit, h = 1)
@

\noindent Here, we present the application of Holt's model to Saudi Arabia's oil production annually,
measured in millions of tonnes, from 1965 to 2013. The data is obtained using fpp2 R
package [23] (the data sets are derived from the second edition of "Forecasting: Principles
and Practice"). The graphical illustration of time series, ACF and PACF of Saudi Arabia's
oil production annually data are shown in Figure~\ref{fig:stss}

<<stss, fig.cap="Graphical illustration of time series, ACF and PACF of annual oil production in Saudi Arabia.",fig.pos="t", fig.height=4, fig.width=9,fig.align='center', warning=FALSE>>=
library(ggplot2)
x2 <- fpp2::oil
v1 <- forecast::autoplot(x2)
v2 <- forecast::ggAcf(x2) + ggtitle("")
v3 <- forecast::ggPacf(x2) + ggtitle("")
gridExtra::grid.arrange(v1, v2, v3, nrow = 1)
@
\noindent With the gamma FALSE parameter supplied by the forecast library's Holtwinters() function, Holt's model can be fitted.
<<message=FALSE>>=
x2 <- fpp2::oil
HM <- HoltWinters(x2, gamma = FALSE)
HM
@
\noindent Based on the Holt's model data, the h = 6 period ahead forecast of Saudi Arabia's annual
oil production time series data, together with the 80\% and 95\% lower and upper confidence intervals, are provided by:

<<>>=
fit <- forecast::forecast(HM, h = 6)
fit
@

\noindent The accuracy measures of the fitted Holt's model are easily obtained using the forecast library's accuracy() function and are given by
<<>>=
round(forecast::accuracy(fit), digit = 4)
@
\noindent The actual and estimated values are visually displayed in the left panel of Figure~\ref{fig:Holt method plot}, while
the actual and six-month forecast, along with the lower and higher confidence intervals of
80\% and 95\%, are shown in the right panel
<<Holt method plot, fig.cap="Graphical illustration of actual and fitted and forecasted of annual oil production of Saudi Arabia.",fig.pos="t", fig.height=4, fig.width=9,fig.align='center', warning=FALSE, message=FALSE, echo=FALSE>>=
library(ggplot2)
x2<-fpp2::oil
HM <- HoltWinters(x2, gamma = FALSE)
fit<-forecast::forecast(HM,h=6)
v1<-autoplot(x2, series="Actual") +
  autolayer(fitted(fit), series="Fitted") +

  xlab("Year") +
  ylab("x") +
  ggtitle("Annual oil production") +
  guides(colour=guide_legend(title="Holt's forecast"))

v2<-autoplot(x2) +
  autolayer(fit, series="Forecast") +

  xlab("Year") +
  ylab("x") +
  ggtitle("Holt's method forecasts") +
  guides(colour=guide_legend(title=""))
gridExtra::grid.arrange(v1, v2, ncol=2)
@


\subsection{Hotlt-Winter's model with multiplicative seasonal component}

The triple exponential smoothing or Hotlt-Winter's Model is an extension of double exponential smoothing. The triple exponential smoothing takes into account the level, trend and seasonal components. The seasonal component can be multiplicative or additive. The following approach can be used for time series with a linear trend and a multiplicative seasonal pattern
$y_{t}=T_{t}\times S_{t}\times I_{t}.$

\noindent The h-period-ahead forecast is obtained by the following expression
\begin{equation}
F_{t+h}=(L_{t}+hT_{t})S_{t-m+h},
\end{equation}
\noindent where $L_t$, $T_t$ and $S_{t-m}$ respectively denote the level, trend and seasonal components and are given by
\begin{equation}
L_{t}=\alpha\left(\frac{y_{t}}{S_{t-m}}\right)+\left(1-\alpha\right)\left[L_{t-1}+T_{t-1}\right],
\end{equation}

\begin{equation}
T_{t}=\beta\left(L_{t}-L_{t-1}\right)+(1-\beta)T_{t-1},
\end{equation}

\begin{equation}
S_{t}=\gamma\left(\frac{y_{t}}{L_{t}}\right)+(1-\gamma)S_{t-m},
\end{equation}
\noindent where $m$ is the number of periods, such as months or quarters in a year. For monthly data it takes value 12 and 4 for quarterly data. We require $m$ values, or one full cycle of data, in order to initialize the seasonal component. The level, trend, and seasonality smoothing factors are denoted by $\alpha$, $\beta$, and $\gamma$, respectively and lies between 0 and 1. Here, we illustrate the Holt's-Winter forecasting method and evaluation of initial values of level, trend and seasonality. The quarterly data is given in the following Table~\ref{HW} and multiplicative seasonality Holt's-Winter model is used and one-period-ahead forecast is obtained. The initial values is obtained and as follows:

\begin{equation}
T_{m}=\frac{\left(y_{m+1}+y_{m+2}\cdots+y_{m+m}\right)-\left(y_{1}+y_{2}\cdots+y_{m}\right)}{m^{2}}
\end{equation}

\begin{equation}
L_{m}=\frac{\left(y_{1}+y_{2}+\cdots\cdots+y_{m}\right)}{m}\,\,\text{and}\,\,S_{i}=\frac{y_{i}}{L_{m}},\,\,i=1,2,\cdots,m
\end{equation}


$$S_{2010,\,1}=\frac{48}{\left(48+64+40+108\right)/4}=0.7385;\,\,S_{2010,\,2}=\frac{64}{\left(48+64+40+108\right)/4}=0.9846;$$
$$S_{2010,\,3}=\frac{40}{\left(48+64+40+108\right)/4}=0.6154;\,\,S_{2010,\,4}=\frac{108}{\left(48+64+40+108\right)/4}=1.6615.$$

\begin{table}[H]
  \centering
  \caption{Calculation of the Hotlt-Winter's method with a seasonal multiplicative component.}
    \begin{tabular}{rrrrrcc}
    \hline
    \multicolumn{1}{c}{Year} & \multicolumn{1}{c}{Quarter} & \multicolumn{1}{c}{\textit{yt}} & \multicolumn{1}{c}{\textit{Lt}} & \multicolumn{1}{c}{\textit{Tt}} & \textit{St} & forecast \\
    \hline
    \multicolumn{1}{c}{2010} & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{48} &       &       & 0.7385 &  \\
    \multicolumn{1}{c}{2010} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{64} &       &       & 0.9846 &  \\
    \multicolumn{1}{c}{2010} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{40} &       &       & 0.6154 &  \\
    \multicolumn{1}{c}{2010} & \multicolumn{1}{c}{4} & \multicolumn{1}{c}{108} & \multicolumn{1}{c}{65.000} & \multicolumn{1}{c}{7.500} & 1.6615 &  \\
    \multicolumn{1}{c}{2011} & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{72} & \multicolumn{1}{c}{77.500} & \multicolumn{1}{c}{8.000} & 0.8147 & 53.538 \\
    \multicolumn{1}{c}{2011} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{96} & \multicolumn{1}{c}{87.900} & \multicolumn{1}{c}{8.240} & 1.0276 & 84.185 \\
    \multicolumn{1}{c}{2011} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{64} & \multicolumn{1}{c}{97.712} & \multicolumn{1}{c}{8.397} & 0.6312 & 59.163 \\
    \multicolumn{1}{c}{2011} & \multicolumn{1}{c}{4} & \multicolumn{1}{c}{148} & \multicolumn{1}{c}{102.702} & \multicolumn{1}{c}{8.056} & 1.5733 & 176.305 \\
    \multicolumn{1}{c}{2012} & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{68} & \multicolumn{1}{c}{105.300} & \multicolumn{1}{c}{7.511} & 0.7471 & 90.234 \\
    \multicolumn{1}{c}{2012} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{116} & \multicolumn{1}{c}{112.825} & \multicolumn{1}{c}{7.512} & 1.0278 & 115.928 \\
    \multicolumn{1}{c}{2012} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{80} & \multicolumn{1}{c}{121.617} & \multicolumn{1}{c}{7.640} & 0.6419 & 75.960 \\
    \multicolumn{1}{c}{2012} & \multicolumn{1}{c}{4} & \multicolumn{1}{c}{168} & \multicolumn{1}{c}{124.762} & \multicolumn{1}{c}{7.191} & 1.4826 & 203.367 \\
    \multicolumn{1}{c}{2013} & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{120} & \multicolumn{1}{c}{137.685} & \multicolumn{1}{c}{7.764} & 0.7969 & 98.584 \\
    \multicolumn{1}{c}{2013} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{168} & \multicolumn{1}{c}{149.049} & \multicolumn{1}{c}{8.124} & 1.0676 & 149.497 \\
    \multicolumn{1}{c}{2013} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{108} & \multicolumn{1}{c}{159.391} & \multicolumn{1}{c}{8.346} & 0.6561 & 100.882 \\
    \multicolumn{1}{c}{2013} & \multicolumn{1}{c}{4} & \multicolumn{1}{c}{268} & \multicolumn{1}{c}{170.341} & \multicolumn{1}{c}{8.606} & 1.5189 & 248.692 \\
          &       &       &       &       & 0.797 & 142.602 \\
          \hline
    \end{tabular}%
  \label{HW}%
\end{table}%
\noindent For the fourth quarter of 2010, the starting level component value is as follows:
$$L_{2010,\,4}=\frac{y_{2010,\,4}}{S_{2010,\,4}}\frac{108}{1.6615}=65.$$
\noindent The initial trend component value of the fourth quater of 2010 is as follows:
$$T_{2010,\,4}=\frac{\left[\frac{\left(72+96+64+148\right)}{4}-\frac{\left(48+64+40+108\right)}{4}\right]}{4}=\frac{95-65}{4}=7.5$$
\noindent One-period-ahead forecast is given by
$$F_{2011,\,1}=(L_{2010,\,4}+T_{2010,\,4})S_{1}=(65+7.5)\times0.7385=53.538.$$





\subsection{Hotlt-Winter's model with additive seasonal component}
For time series analysis involving additive seasonal pattern and linear trend, the subsequent methodology can be employed $y_{t}=T_{t}+S_{t}+I_{t}.$ The definitions and computations for the following three components level, trend, and seasonality component can be found
by, correspondingly
\begin{equation}
L_{t}=\alpha\left(y_{t}-S_{t-m}\right)+\left(1-\alpha\right)\left[L_{t-1}+T_{t-1}\right],
\end{equation}

\begin{equation}
T_{t}=\beta\left(L_{t}-L_{t-1}\right)+(1-\beta)T_{t-1},
\end{equation}
and
\begin{equation}
S_{t}=\gamma\left(y_{t}-L_{t}\right)+(1-\gamma)S_{t-m}.
\end{equation}

\noindent The h-period-ahead forecast is obtained by the following expression

\begin{equation}
F_{t+h}=(L_{t}+hT_{t})+S_{t-m+h}.
\end{equation}

\begin{table}[H]
  \centering
  \caption{The computation of Hotlt-Winter's method with additive seasonal component.}
    \begin{tabular}{rrrrcc}
     \hline
    \multicolumn{1}{c}{Quarter} & \multicolumn{1}{c}{\textit{yt}} & \multicolumn{1}{c}{\textit{Lt}} & \multicolumn{1}{c}{\textit{Tt}} & \textit{St} & forecast \\
     \hline
    \multicolumn{1}{c}{1} & \multicolumn{1}{c}{48} &       &       & -17.00 &  \\
    \multicolumn{1}{c}{2} & \multicolumn{1}{c}{64} &       &       & -1.00 &  \\
    \multicolumn{1}{c}{3} & \multicolumn{1}{c}{40} &       &       & -25.00 &  \\
    \multicolumn{1}{c}{4} & \multicolumn{1}{c}{108} & \multicolumn{1}{c}{65.00} & \multicolumn{1}{c}{7.50} & 43.00 &  \\
    \multicolumn{1}{c}{1} & \multicolumn{1}{c}{72} & \multicolumn{1}{c}{72.67} & \multicolumn{1}{c}{7.53} & -13.73 & 55.50 \\
    \multicolumn{1}{c}{2} & \multicolumn{1}{c}{96} & \multicolumn{1}{c}{80.37} & \multicolumn{1}{c}{7.57} & 2.33  & 79.20 \\
    \multicolumn{1}{c}{3} & \multicolumn{1}{c}{64} & \multicolumn{1}{c}{87.94} & \multicolumn{1}{c}{7.57} & -24.79 & 62.93 \\
    \multicolumn{1}{c}{4} & \multicolumn{1}{c}{148} & \multicolumn{1}{c}{95.61} & \multicolumn{1}{c}{7.59} & 44.88 & 138.51 \\
    \multicolumn{1}{c}{1} & \multicolumn{1}{c}{68} & \multicolumn{1}{c}{102.98} & \multicolumn{1}{c}{7.54} & -17.98 & 89.46 \\
    \multicolumn{1}{c}{2} & \multicolumn{1}{c}{116} & \multicolumn{1}{c}{110.56} & \multicolumn{1}{c}{7.55} & 2.95  & 112.85 \\
    \multicolumn{1}{c}{3} & \multicolumn{1}{c}{80} & \multicolumn{1}{c}{117.97} & \multicolumn{1}{c}{7.52} & -27.43 & 93.32 \\
    \multicolumn{1}{c}{4} & \multicolumn{1}{c}{168} & \multicolumn{1}{c}{125.47} & \multicolumn{1}{c}{7.52} & 44.41 & 170.38 \\
    \multicolumn{1}{c}{1} & \multicolumn{1}{c}{120} & \multicolumn{1}{c}{133.04} & \multicolumn{1}{c}{7.53} & -16.99 & 115.01 \\
    \multicolumn{1}{c}{2} & \multicolumn{1}{c}{168} & \multicolumn{1}{c}{140.82} & \multicolumn{1}{c}{7.58} & 7.80  & 143.52 \\
    \multicolumn{1}{c}{3} & \multicolumn{1}{c}{108} & \multicolumn{1}{c}{148.27} & \multicolumn{1}{c}{7.55} & -29.99 & 120.97 \\
    \multicolumn{1}{c}{4} & \multicolumn{1}{c}{268} & \multicolumn{1}{c}{156.50} & \multicolumn{1}{c}{7.69} & 57.83 & 200.23 \\
          &       &       &       & -16.99 & 147.19 \\
          &       &       &       & 7.80  & 179.67 \\
          \hline
    \end{tabular}%
  \label{tab:addlabel}%
\end{table}%

<<hw plot, fig.cap="Graphical illustration of time series, ACF and PACF of monthly corticosteroid drug subsidy data in Australia.",fig.pos="t", fig.height=10, fig.width=10,fig.align='center'>>=
library(ggplot2)
x1 <- ts(c(48, 64, 40, 108, 72, 96, 64, 148, 68, 116,
    80, 168, 120, 168, 108, 268), start = 2010, frequency = 4)
HW <- HoltWinters(x1, alpha = 0.01, beta = 0.2, gamma = 0.2,
    seasonal = "additive")
fit <- forecast::forecast(HW, h = 2)
fit
@




\subsection{Holt-Winter's model application to corticosteroid drug data}
Here, we demonstrate the Holt-Winter model's application to Australian monthly corticosteroid medication subsidy data spanning the years 1991 to 2008. The fpp2 R package is
used to call the a10 function in order to acquire the data. Figure~\ref{fig:sstss} displays the graphical
representation of the actual time series, ACF, and PACF of the monthly corticosteroid
drug subsidy data in Australia. It is provided by

<<sstss, fig.cap="Graphical illustration of time series, ACF and PACF of monthly corticosteroid drug subsidy data in Australia.", fig.pos="t", fig.height=4, fig.width=9, fig.align='center', warning=FALSE>>=
library(ggplot2)
x2 <- fpp2::a10
v1 <- forecast::autoplot(x2)
v2 <- forecast::ggAcf(x2) + ggtitle("")
v3 <- forecast::ggPacf(x2) + ggtitle("")
gridExtra::grid.arrange(v1, v2, v3, nrow = 1)
@
\noindent Holt's model can be fitted using the forecast library's Holtwinters() function, which treats
the seasonal component as "multiplicative" and is provided by

<<>>=
x2 <- fpp2::a10
HW <- HoltWinters(x2, seasonal = "multiplicative")
HW
@
\noindent Based on the Holt's model with multiplicative seasonal trend, the h = 12 period ahead
forecast of the monthly corticosteroid drugs subsidy data in Australia, together with the
80\% and 95\% lower and higher confidence intervals, are provided by
<<>>=
fit_1 <- forecast::forecast(HW, h = 12)
fit_1
@

\noindent It is simple to get the accuracy measures of the fitted Holt's model using the forecast
library's accuracy() function; these are supplied by
<<>>=
round(forecast::accuracy(fit_1), digit = 4)
@
\noindent The actual and fitted values are visually displayed in the left panel of Figure~\ref{fig:AF plot}, while
the actual and 12-period forecast, together with the lower and higher confidence intervals
of 80\% and 95\%, are shown in the right panel.

<<AF plot, fig.cap="Graphical illustration of actual, fitted and forecasted of monthly corticosteroid drug subsidy data in Australia.",fig.pos="t", fig.height=4, fig.width=9,fig.align='center', warning=FALSE, echo=FALSE>>=
library(ggplot2)
x2<-fpp2::a10
         HW <- HoltWinters(x2,  seasonal="multiplicative")
         fit<-forecast::forecast(HW,h=12)
         v1<-autoplot(x2, series="Actual") +
           autolayer(fitted(fit), series="Fitted") +

           xlab("Year") +
           ylab("x") +
           ggtitle("Corticosteroid drug subsidy") +
           guides(colour=guide_legend(title="HW multiplicative"))

         v2<-autoplot(x2) +
           autolayer(fit, series="") +

           xlab("Year") +
           ylab("x") +
           ggtitle("HW multiplicative forecasts") +
           guides(colour=guide_legend(title="Forecast"))
         gridExtra::grid.arrange(v1, v2, ncol=2)
@




\subsection{Holt-Winter's model application to daily pageviews for the Hyndsight blog data}
Here, we demonstrate the Holt-Winter model's application to the daily page views of
the Hyndsight blog data collected between April 30, 2014, and April 29, 2015. The
fpp2 R package is used to acquire the data by invoking the hyndsight function. The
forecast library's Holtwinters() function makes it possible to fit Holt's model by treating
the seasonal component as "additive" and given by



<<>>=
x2 <- fpp2::hyndsight
HW1 <- HoltWinters(x2, seasonal = "additive")
HW1
@
\noindent Using the Holt's additive model, the h = 12 period ahead projection for daily pageviews
for the Hyndsight blog, together with the 80\% and 95\% lower and higher confidence
intervals, is given by
<<>>=
fit_1 <- forecast::forecast(HW1, h = 12)
fit_1
@

\noindent The left panel of Figure 5.10 shows the actual and estimated values graphically, while
the right panel shows the actual and 12-point forecast together with the lower and higher
confidence intervals of 80\% and 95\%.
<<hw multi plot, fig.cap="Graphical illustration of actual, fitted and forecasted of daily pageviews for the Hyndsight blog data.",fig.pos="t", fig.height=4, fig.width=9,fig.align='center', warning=FALSE, echo=FALSE>>=
library(ggplot2)
x2<-fpp2::hyndsight
         HW <- HoltWinters(x2,  seasonal="additive")
         fit<-forecast::forecast(HW,h=12)
         v1<-autoplot(x2, series="Actual") +
           autolayer(fitted(fit), series="Fitted") +

           xlab("Year") +
           ylab("x") +
           ggtitle("Daily page views for the Hyndsight") +
           guides(colour=guide_legend(title="HW additive"))

         v2<-autoplot(x2) +
           autolayer(fit, series="") +

           xlab("Year") +
           ylab("x") +
           ggtitle("HW additive forecasts") +
           guides(colour=guide_legend(title="Forecast"))
         gridExtra::grid.arrange(v1, v2, ncol=2)
@


\subsection{Box-Jenkins approch}
The Box-Jenkins approach, which was introduced by Box and Jenkins in the early 1970s, is a well-known technique for time series prediction. It is a systematic approach which incorporates the integrated autoregressive moving average (ARIMA) time series models. The ARIMA(p,d,q) model consists of three parameters including the auto regressive order (p), the differencing order (d) and the moving average order (q). Three primary steps are involved in building a Box-Jenkins time series model: model identification, model estimate, and model validation. ARIMA model assumes that the given time series is stationary which means the observations of a time series are independent of time and hence mean, variance, and covariance remain constant. On the other side, Non-stationary time series are those that exhibit trend or seasonality. By differencing a non-stationary time series, one can convert it into stationary. Generally, a non-stationary time series often becomes stationary after the first difference (d=1).

\subsection{Augmented Dickey-Fuller Test}
Here, we illustrate the Augmented Dickey-Fuller (ADF) test with null hypothesis: the series is non-stationary (there is a unit root) against alternative hypothesis: the series is stationary (there is not a unit root). The null hypothesis cannot be rejected if the $p$-value is greater than 5\%.

<<warning=FALSE, message=FALSE>>=
x <- hivdata::tghiv
tseries::adf.test(x)
@

\noindent The ADF test yields a test statistic value of -1.6375 and a $p$-value of 0.7229 with a number of lags selected of 4. The null hypothesis cannot be rejected since the $p$-value is greater than 5\%. For this reason, the data cannot be considered stationary.
<<actual plot, fig.cap="Graphical illustration of actual series.",fig.pos="t", fig.height=4, fig.width=9,fig.align='center', warning=FALSE>>=
v1 <- forecast::autoplot(x)
v2 <- forecast::ggAcf(x) + ggtitle("")
v3 <- forecast::ggPacf(x) + ggtitle("")
gridExtra::grid.arrange(v1, v2, v3, nrow = 1)
@
\noindent We can difference the given time series by using diff () function and is as follows:
<<warning=FALSE>>=
v <- diff(x)
@
\noindent As seen in the accompanying graphical depiction, the series became stationary after the first difference.
<<diff plot, fig.cap="Graphical illustration of series ($d=1$).",fig.pos="t", fig.height=4, fig.width=9,fig.align='center', warning=FALSE>>=
v4 <- forecast::autoplot(v)
v5 <- forecast::ggAcf(v) + ggtitle("")
v6 <- forecast::ggPacf(v) + ggtitle("")
gridExtra::grid.arrange(v4, v5, v6, nrow = 1)
@
\noindent Additionally, the ADF test verified that the series is shifted to stationary; since the $p$-value (0.01) is smaller than 0.05, the null hypothesis may be rejected.
<<message=FALSE, warning=FALSE>>=
tseries::adf.test(v)
@


\subsection{Ljung-Box test}
After selecting and estimating the parameters of a fitted ARIMA model, we next determine whether or not the model is suitable. To this end, the Ljung-Box test is used to determine the goodness-of-fit test (whether or not the autocorrelation exists in a time series). The Ljung-Box test's formulation of the hypothesis is as follows: In contrast to the alternative hypothesis, which states that the residuals are not independently distributed (serial correlation), the null hypothesis states that the residuals are distributed independently. If the Ljung-Box test's $p$-value is higher than 0.05, the null hypothesis cannot be rejected because it indicates that the residuals in our time series model are not correlated (independent).
<<>>=
Box.test(v, lag = 10, type = "Ljung")
@

\subsection{auto.arima() function}
The ARIMA model can be fitted by specifying the $p$, $d$ and $q$ order. To this end, {\sf forecast} package allows to manually fit ARIMA model using the following function Arima(x, order=c(p, d, q)). The argument $x$ denotes the respective time series and order=c(p, d, q) are the auto regressive order (p), the differencing order (d) and the moving average order (q). The another useful function that fit automatically optimum ARIMA model that is auto.arima () function and given by

<<>>=
x <- hivdata::tghiv
am <- forecast::auto.arima(x)
fit <- forecast::forecast(am, h = 4)
@
\noindent Figure~\ref{fig:ppplot} shows the graphical representation of residual and showing good fit under
ARIMA (0, 1, 0) model with drift. The checkresiduals() function permits to get residual
plots and given by
<<ppplot, fig.cap="Graphical representation of residual.",fig.pos="t", fig.height=6, fig.width=13,fig.align='center', warning=FALSE>>=
forecast::checkresiduals(am)
@

<<ACT and FITTED plots, fig.cap="Graphical representation of actual, predicted and forecasted of transgender HIV cases in Pakistan.",fig.height=6, fig.width=13,fig.align='center', echo=FALSE>>=
v1<-autoplot(x, series="Actual") +
  autolayer(fitted(fit), series="Fitted") +

  xlab("Year") +
  ylab("x") +
  ggtitle("HIV Cases") +
  guides(colour=guide_legend(title="ARIMA's forecast"))

v2<-autoplot(x) +
  autolayer(fit, series="Forecast") +

  xlab("Year") +
  ylab("x") +
  ggtitle("ARIMA's Model") +
  guides(colour=guide_legend(title=""))
gridExtra::grid.arrange(v1, v2, ncol=2)
@

\subsection{Seasonal ARIMA model}
The seasonal ARIMA (SARIMA) model works effectively if the time series has seasonality. It is an extension of ARIMA model that includes the seasonal component. The auto.arima () function allows to automatically fit a SARIMA model if the given time series has seasonal component. The SARIMA can be fitted manually after specifying the parameters Arima(x, order=c(p, d, q), seasonal=c(P, D, Q)). On the other side, the auto.arima () function permits to automatically fit the SARIMA model as as follows:
<<>>=
x <- fpp2::austourists
sam <- forecast::auto.arima(x)
fit <- forecast::forecast(sam, h = 4)
@
\noindent The diagnosis measures also support that the fit is good. Based on the Ljung-Box test's $p$-value is higher than 0.05, the null hypothesis cannot be rejected.

<<sACT and FITTED plots, fig.cap="Graphical representation of actual, predicted and forecasted.",fig.height=6, fig.width=13,fig.align='center', echo=FALSE>>=
v1<-autoplot(x, series="Actual") +
  autolayer(fitted(fit), series="Fitted") +

  xlab("Year") +
  ylab("x") +
  ggtitle("HIV Cases") +
  guides(colour=guide_legend(title="SARIMA forecast"))

v2<-autoplot(x) +
  autolayer(fit, series="Forecast") +

  xlab("Year") +
  ylab("x") +
  ggtitle("SARIMA Model") +
  guides(colour=guide_legend(title=""))
gridExtra::grid.arrange(v1, v2, ncol=2)
@





\subsection{ETS model}
The three components of the ETS (Error, Trend, Season) model are represented by the letters "E" for error, "T" for trend, and "S" for seasonal pattern. Error type ("A", "M", or "Z") is represented by the first character; trend type ("N", "A", "M", or "Z") is represented by the second character; and season type ("N", "A", "M", or "Z") is represented by the third character. The letters "A" stands for additive, "M" for multiplicative, "Z" for always being automatically chosen, and "N" for none. Readers are referred to reference~\cite{Hyndman2002}, for a brief overview of smoothing time series models.  Here, we use the COVID-19 confirmed reported cases in Pakistan, the data set is recently used by reference~\cite{Fayomi2022} and follows:4976, 6127, 5152, 5445, 5499, 5857, 5870, 5908, 5611, 4825,
    4487, 5292, 5480, 5113, 4696, 4414, 4213, 3377, 4113, 4198, 4298,
    4109, 3785, 3447, 3084, 2869, 3265, 2517, 1531, 2379, 3232, 2566,
    3256, 4207, 2370, 4007, 3084, 3060, 2253, 2724, 2726, 2482, 2455,
    2697, 2117, 1771, 1843, 2028, 1893, 1923, 1629, 1490, 1383, 1118,
    1303, 1303, 1194, 1239, 1019, 838, 1038, 1119, 1043, 991, 1050,
    907, 663, 930, 1097, 1052, 935, 901, 914, 735, 979, 1037, 1277,
    1400, 1228, 1347, 830, 1517, 1683, 1737, 1828, 1980, 1808, 1590,
    1980, 2545, 2327, 2783, 2607, 2452, 2145, 2579, 2158, 1425, 1841,
    2819, 3752, 3262, 4119, 4497, 4537, 4950, 5026, 4858, 3582, 4722,
    5661, 4745, 4720, 4455, 4040, 3884, 4850, 4934, 4619, 4786, 3711,
    3669. The state space smoothing model can be fitted using the ets() function, which is provided by

<<echo=FALSE>>=
y <- c(4976, 6127, 5152, 5445, 5499, 5857, 5870, 5908, 5611, 4825,
    4487, 5292, 5480, 5113, 4696, 4414, 4213, 3377, 4113, 4198, 4298,
    4109, 3785, 3447, 3084, 2869, 3265, 2517, 1531, 2379, 3232, 2566,
    3256, 4207, 2370, 4007, 3084, 3060, 2253, 2724, 2726, 2482, 2455,
    2697, 2117, 1771, 1843, 2028, 1893, 1923, 1629, 1490, 1383, 1118,
    1303, 1303, 1194, 1239, 1019, 838, 1038, 1119, 1043, 991, 1050,
    907, 663, 930, 1097, 1052, 935, 901, 914, 735, 979, 1037, 1277,
    1400, 1228, 1347, 830, 1517, 1683, 1737, 1828, 1980, 1808, 1590,
    1980, 2545, 2327, 2783, 2607, 2452, 2145, 2579, 2158, 1425, 1841,
    2819, 3752, 3262, 4119, 4497, 4537, 4950, 5026, 4858, 3582, 4722,
    5661, 4745, 4720, 4455, 4040, 3884, 4850, 4934, 4619, 4786, 3711,
    3669)
x = ts(y, frequency = 7)
@

<<>>=
fit <- forecast::ets(x)
fit
@
\noindent A 5-day ahead forecast using ETS (M, N, M) model and given by:
<<>>=
fit <- forecast::forecast(fit, h = 5)
@
\noindent Figure~\ref{fig:sETSACT} presents an actual and expected plot with a 5-day ahead forecast
<<sETSACT, fig.cap="Graphical representation of actual and predicted and actual and forecasted.",fig.height=6, fig.width=13,fig.align='center', echo=FALSE>>=
v1<-autoplot(x, series="Actual") +
  autolayer(fitted(fit), series="Fitted") +
  xlab("Daily") +
  ylab("x") +
  ggtitle("Confirmed Cases") +
  guides(colour=guide_legend(title="ETS forecast"))
v2<-autoplot(x) +
  autolayer(fit, series="Forecast") + xlab("Daily") +
  ylab("x") + ggtitle("ETS Model") +
  guides(colour=guide_legend(title=""))
gridExtra::grid.arrange(v1, v2, ncol=2)
@

\subsection{ANN Model}
Forecasting techniques such as artificial neural networks are predicated on elementary mathematical models of the human brain. They permit intricate nonlinear correlations between the predictors and the response variable. An artificial neural network's (ANN) fundamental structure consists of the input, hidden, and output layers. Neural network nonlinear autoregressive (NNAR) model which uses the lagged value of the time series as inputs and is a feed-forward neural network with a single hidden layer. The inputs consist of lags 1 to p and lags m to mp, where m is the frequency of the time series. The ANN model can be applied to both seasonal and non-seasonal data. For non-seasonal data, the fitted model is called NNAR (p, k), where k is the number of nodes in the single hidden layer and p is a lagged input. With the exception of the stationary restrictions, the NNAR (p, 0) model is comparable to the ARIMA (p, 0, 0) model. A NNAR (p, P, k) [m] is the fitted model for seasonal data; it is comparable to an ARIMA (p, 0, 0) (P, 0, 0) [m] but does not have stationary restrictions.
<<>>=
x <- hivdata::allpakhiv
nnet <- forecast::nnetar(x)
nnet
@
\noindent The three-day point forecast is obtained using ANN and provided by along with the lower
and upper confidence intervals of 80\% and 95\%
<<>>=
pred <- forecast::forecast(nnet, h = 3, PI = TRUE, npaths = 100)
pred
@
\noindent A plot of actual and predicted along with three days ahead forecast is presented in Figure~\ref{fig:vsETS}
<<vsETS, fig.cap="Graphical representation of actual and predicted and actual and forecasted.",fig.height=6, fig.width=13,fig.align='center', echo=FALSE, warning=FALSE>>=
library(ggplot2)
v1<-autoplot(x, series="Actual") +
  autolayer(fitted(pred), series="Fitted") +
  xlab("Year") +
  ylab("x") +
  ggtitle("HIV Cases") +
  guides(colour=guide_legend(title="ANN model"))
v2<-autoplot(x) +
  autolayer(pred, series="Forecast") + xlab("Year") +
  ylab("x") +
  ggtitle("ANN Forecast") +
  guides(colour=guide_legend(title=""))
gridExtra::grid.arrange(v1, v2, nrow=1)
@

\subsection{Models comparison to COVID-19 related deaths}
Here, we utilize COVID-19-related deaths~\cite{Fayomi2022} to illustrate the model comparisons. We
employ the ARIMA, ANN, and ETS time series models. The data set that follows is
provided by 112,	149,	73,	137,	148,	98,	144,	157,	118,	70,	142,	201,
151,	131,	146,	113,	79,	161,	119,	108,	140,	120,	118,	78,
113,	104,	126,	48,	83,	76,	74,	135,	104,	131,	102,	88,
74,	57,	92,	65,	75,	67,	73,	56,	43,	71,	80,	92,
83,	84,	76,	58,	53,	77,	76,	47,	57,	56,	13,	59,
46,	46,	39,	27,	37,	30,	27,	39,	38,	44,	36,	23,
20,	23,	27,	40,	24,	34,	29,	19,	25,	17,	24,	25,
35,	27,	15,	21,	24,	47,	31,	39,	17,	30,	37,	40,
40,	11,	32,	45,	32,	39,	44,	76,	86,	65,	62,	40,
67,	46,	60,	67,	95,	68,	53,	86,	81,	102,	79,	73,
67,	72. The ets(), auto.arima() and nnetar() function are used to fit ETS, ARIMA and ANN
model using COVID-19 related deaths~\cite{Fayomi2022} data and given by
<<echo=FALSE>>=
y <- c(112,	149,	73,	137,	148,	98,	144,	157,	118,	70,	142,	201,
151,	131,	146,	113,	79,	161,	119,	108,	140,	120,	118,	78,
113,	104,	126,	48,	83,	76,	74,	135,	104,	131,	102,	88,
74,	57,	92,	65,	75,	67,	73,	56,	43,	71,	80,	92,
83,	84,	76,	58,	53,	77,	76,	47,	57,	56,	13,	59,
46,	46,	39,	27,	37,	30,	27,	39,	38,	44,	36,	23,
20,	23,	27,	40,	24,	34,	29,	19,	25,	17,	24,	25,
35,	27,	15,	21,	24,	47,	31,	39,	17,	30,	37,	40,
40,	11,	32,	45,	32,	39,	44,	76,	86,	65,	62,	40,
67,	46,	60,	67,	95,	68,	53,	86,	81,	102,	79,	73,
67,	72)
x=ts(y,frequency=7)
@

<<>>=
ETS <- forecast::ets(x)
ARIMA <- forecast::auto.arima(x)
ANN <- forecast::nnetar(x)
@
\noindent The accuracy() function from the forecast library is used to compute a set of accuracy
measures, including mean error (ME), root mean square error (RMSE), mean absolute
error (MAE), mean percentage error (MPE), mean absolute percentage error (MAPE),
mean absolute scaled error (MASE), and autocorrelation of errors at lag 1 (ACF1).
<<>>=
round(forecast::accuracy(ARIMA), 3)
round(forecast::accuracy(ETS), 3)
round(forecast::accuracy(ANN), 3)
@
\noindent Due to least errors, ANN produces the best fit out of all the competing models. The five day point forecast is obtained and provided by along with the lower and upper confidence
intervals of 80\% and 95\%
<<>>=
pred1 <- forecast::forecast(ANN, h = 2, PI = TRUE, npaths = 100)
pred1
@
\noindent A plot of actual and predicted along with two days ahead forecast is presented in Figure~\ref{fig:xxv2}
<<xxv2, fig.cap="Graphical representation of actual vs predicted and actual vs forecasted.",fig.height=5, fig.width=13,fig.align='center',echo=FALSE,warning=FALSE>>=
library(ggplot2)
v1<-autoplot(x, series="Actual") +
  autolayer(fitted(pred1), series="Fitted") +
  xlab("Year") +
  ylab("x") +
  ggtitle("COVID-19 deaths") +
  guides(colour=guide_legend(title="ANN model"))
v2<-autoplot(x) +
  autolayer(pred1, series="Forecast") + xlab("Year") +
  ylab("x") +
  ggtitle("ANN Forecast") +
  guides(colour=guide_legend(title=""))
gridExtra::grid.arrange(v1, v2, nrow=1)
@


\begin{thebibliography}{99}
\bibitem{RCoreTeam2021}
R Core Team (2021). R: A language and environment for
statistical computing. R Foundation for Statistical Computing,
Vienna, Austria. URL https://www.R-project.org/.

\bibitem{Hyndman2023}

Hyndman R, Athanasopoulos G, Bergmeir C, Caceres G, Chhay L, O'Hara-Wild M,
Petropoulos F, Razbash S, Wang E, Yasmeen F (2023). forecast: Forecasting
functions for time series and linear models. R package version 8.21.1, <URL:
  https://pkg.robjhyndman.com/forecast/>.

  \bibitem{HWickham2016}
  H. Wickham. ggplot2: Elegant Graphics for Data Analysis.
  Springer-Verlag New York, 2016.

  \bibitem{Rob2023}
Rob Hyndman (2023). fpp2: Data for "Forecasting: Principles and Practice"
(2nd Edition). R package version 2.5.
https://CRAN.R-project.org/package=fpp2.

   \bibitem{Baptiste2017}
  Baptiste Auguie (2017). gridExtra: Miscellaneous Functions for
  "Grid" Graphics. R package version 2.3.
  https://CRAN.R-project.org/package=gridExtra

  \bibitem{Adrian2023}
  Adrian Trapletti and Kurt Hornik (2023). tseries: Time Series Analysis and
Computational Finance. R package version 0.10-54.
https://CRAN.R-project.org/package=tseries.

\bibitem{Muhammad2023}

Muhammad Imran and Saima Shakoor (2023). hivdata: Six-Year Chronological
Data of HIV and ART Cases in Pakistan. R package version 0.1.0.
https://CRAN.R-project.org/package=hivdata.

\bibitem{Ghysels2018}
Ghysels, E., \& Marcellino, M. (2018). Applied economic forecasting using time series methods. Oxford University Press.

\bibitem{Stock2001}
Stock, J. H. (2001). Forecasting economic time series. A Companion to Theoretical Econometrics, Blackwell Publishers, 562-84.

\bibitem{Brady2017}
Brady, G. L., \& Magazzino, C. (2017). Sustainability of Italian budgetary policies: A time series analysis (1862-2013). European Journal of Government and Economics, 6(2), 126-145.

\bibitem{Ampountolas2023}
Ampountolas, A. (2023). Comparative Analysis of Machine Learning, Hybrid, and Deep Learning Forecasting Models: Evidence from European Financial Markets and Bitcoins. Forecasting, 5(2), 472-486.

\bibitem{Horak2023}
Horak, J., \& Jannova, M. (2023). Predicting the Oil Price Movement in Commodity Markets in Global Economic Meltdowns. Forecasting, 5(2), 374-389.
\bibitem{Bobeica2023}
Bobeica, E., \& Hartwig, B. (2023). The COVID-19 shock and challenges for inflation modeling. International journal of forecasting, 39(1), 519-539.
\bibitem{Petropoulos2022}

Petropoulos, F., Makridakis, S., \& Stylianou, N. (2022). COVID-19: Forecasting confirmed cases and deaths with a simple time series model. International journal of forecasting, 38(2), 439-452.
\bibitem{Bracher2022}
Bracher, J., \& Held, L. (2022). Endemic-epidemic models with discrete-time serial interval distributions for infectious disease prediction. International Journal of Forecasting, 38(3), 1221-1233.
\bibitem{Lassila2014}
Lassila, J., Valkonen, T., \& Alho, J. M. (2014). Demographic forecasts and fiscal policy rules. International Journal of Forecasting, 30(4), 1098-1109.
\bibitem{Melchior2021}
Melchior, C., Zanini, R. R., Guerra, R. R., \& Rockenbach, D. A. (2021). Forecasting Brazilian mortality rates due to occupational accidents using autoregressive moving average approaches. International Journal of Forecasting, 37(2), 825-837.
\bibitem{Yu2022}
Yu, L., Liang, S., Chen, R., \& Lai, K. K. (2022). Predicting monthly biofuel production using a hybrid ensemble forecasting methodology. International Journal of Forecasting, 38(1), 3-20.

\bibitem{Ahmar2023}
Ahmar, A. S., Singh, P. K., Ruliana, R., Pandey, A. K., \& Gupta, S. (2023). Comparison of ARIMA, SutteARIMA, and Holt-Winters, and NNAR Models to Predict Food Grain in India. Forecasting, 5(1), 138-152.

\bibitem{Aliu2023}
Aliu, F., Kucera, J., \& Haskova, S. (2023). Agricultural Commodities in the Context of the Russia-Ukraine War: Evidence from Corn, Wheat, Barley, and Sunflower Oil. Forecasting, 5(1), 351-373.
\bibitem{Giunta2023}
Giunta, G., Ceppi, A., \& Salerno, R. (2023). An Extended Analysis of Temperature Prediction in Italy: From Sub-Seasonal to Seasonal Timescales. Forecasting, 5(4), 600-615.
\bibitem{Sun2022}
Sun, C., Hobday, A. J., Condie, S. A., Baird, M. E., Eveson, J. P., Hartog, J. R., ... \& Mongin, M. (2022). Ecological Forecasting and Operational Information Systems Support Sustainable Ocean Management. Forecasting, 4(4), 1051-1079.
\bibitem{Brown1956}
Brown, R.G., 1956. Exponential Smoothing for Predicting Demand. Arthur D. Little, Inc, Cambridge 42, Massachusetts.

\bibitem{Mitchell2023}
Mitchell O'Hara-Wild, Rob Hyndman and Earo Wang (2023). fable: Forecasting
  Models for Tidy Time Series. R package version 0.3.3.
  https://CRAN.R-project.org/package=fable.

\bibitem{Rami2023}
Rami Krispin (2023). TSstudio: Functions for Time Series Analysis and
Forecasting. R package version 0.1.7.
https://CRAN.R-project.org/package=TSstudio.

\bibitem{Diethelm2023}

Diethelm Wuertz, Tobias Setz and Yohan Chalabi (2023). timeSeries: Financial
Time Series Objects (Rmetrics). R package version 4030.106.
https://CRAN.R-project.org/package=timeSeries.


\bibitem{wang2023}
Wang, E, D Cook, and RJ Hyndman (2020). A new tidy data structure to support
exploration and modeling of temporal data, Journal of Computational and
Graphical Statistics, 29:3, 466-478, doi:10.1080/10618600.2019.1695624.

\bibitem{Rob2015}

Rob J Hyndman (2015). expsmooth: Data Sets from "Forecasting with
  Exponential Smoothing". R package version 2.3.
https://CRAN.R-project.org/package=expsmooth.

\bibitem{HyndmanRJ2023}
Hyndman RJ (2023). fma: Data sets from "Forecasting: methods and
applications" by Makridakis, Wheelwright \& Hyndman (1998). R package version
2.5. http://pkg.robjhyndman.com/fma.

\bibitem{Hyndman2002}
Hyndman RJ, Koehler AB, Snyder RD, et al. A state space framework for automatic forecasting
using exponential smoothing methods. International Journal of forecasting 2002;18(3):439-54.

\bibitem{Fayomi2022}
Fayomi, A., Nasir, J. A., Algarni, A., Rasool, M. S., Jamal, F., \& Chesneau, C. (2022). Best selected forecasting models for COVID-19 pandemic. Open Physics, 20(1), 1303-1312.
\end{thebibliography}




\chapter{Regression and Correlation}
\section{Introduction}
In this Chapter, we will cover the following: the fitting of simple regression model and its graphical representation, anova test, R-squared, adjusted R-squared, scatter diagram and its graphical illustration, Pearson's correlation with graphical demonstration, multiple regression model and understanding its output, assumptions associated to multiple regression analysis (linearity test, Breusch-Pagan test for heteroscedasticity, normality test, multicollinearity test, Durbin-Watson test, diagnostic plots with interpretation), model parameters estimation, fitting multiple regression model, making prediction for specific values, removing intercept from model, fitting of binary logistic regression and understanding its output, Wald test, multiple logistic regression with applications and understanding its output, Changing reference category, multinomial logistic regression using nnet package and multinomial logistic regression using mlogit package. This Chapter depends on the following R packages namely ggpubr~\cite{Alboukadel2023}, predict3d~\cite{Keon2023}, lmtest~\cite{Achim2002}, ggfortify~\cite{Yuan2016}, broom~\cite{David2023}, gtsummary~\cite{Sjoberg2021}, performance~\cite{Ludecke2021}, faraway~\cite{Julian2022}, nnet~\cite{Venables2002} and  mlogit~\cite{Croissant2020}.

\subsection{Regression analysis}
The idea of regression (initially it was reversion) was first given by the English scientist, Sir Francis Galton (1822-1911)~\cite{Galton1877}. In simple linear regression analysis two variables, $X$ and $Y$, are of interest. The variable $x$ is called as independent variable. it is not random. Usually, the value of independent variable is chosen by the experimenter. The independent variable is also known as the predictor or the regressor or the explanatory or regression variable. The other variable, $Y$, accordingly, is known as the dependent variable or the regressand or the predicted variable or the response variable or the explained variable. The dependent variable is a random variable, it is obtained using the information of independent variable. Generally, the regression is a process of estimating the dependent variable using known the value of independent variable. For example, the students academic performance (marks) depending upon teaching method. Here, $Y$ is student performance and $X$ is the teaching method. Similarly, the wheat yield per acre is depending upon the amount of fertilizer used. Here, $Y$ is the wheat yield per acre and $X$ is the uses of fertilizer. Two types of regression equations can be fitted, that is $Y$ on $X$, if $Y$ is estimated on the basis of $X$. The other is $X$ on $Y$, when $X$ is estimated using $Y$ variable information.

\subsection{Linear regression model}
The following is the population liner regression model
\begin{equation}
Y_{i}=\alpha+\beta X_{i}+e_{i},
\end{equation}
where $\alpha$ and $\beta$ are the population regression coefficients and $e_i$ represents the error term that is the difference between actual and predicted.
<<>>=
x <- c(50, 55, 60, 65, 70, 75, 80, 85)
y <- c(54, 52, 62, 61, 73, 74, 81, 82)
df <- as.data.frame(cbind(x, y))
@
\subsection{Least-squares lines of regression}
The method of least-squares is used to determine desire least-squares line. This method is capable to minimize the sum of squares of deviations between the observed values and the corresponding estimated values. The following is the least-squares line $Y$ on $X$, when the variable $Y$ is dependent is as follows:
\begin{equation}
y=a+bx,
\end{equation}
where $x$ is the independent variable and $y$ is the dependent variable. The $y$-intercept and slope are denoted by $a$ and $b$, respectively. The $a$ and $b$ can be computed using normal equations and as follows:
\begin{equation}
a=\bar{y}-b\bar{x}
\end{equation}
and
\begin{equation}
b=\frac{n\sum xy-(\sum x)(\sum y)}{n\sum x^{2}-(\sum x)^{2}}.
\end{equation}

The regression line $x$ on $y$ is given by
\begin{equation}
x=c+dy,
\end{equation}
where $c$ and $d$ are the x-intercept and regression coefficient, respectively. These unknown quantities can be computed by the following formula:
\begin{equation}
c=\bar{x}-d\bar{y}
\end{equation}
and
\begin{equation}
d=\frac{n\sum xy-(\sum x)(\sum y)}{n\sum y^{2}-(\sum y)^{2}}.
\end{equation}


\begin{table}[H]
  \centering
  \caption{Computation of regression slop and intercept.}
    \begin{tabular}{cccccccccc}
    \hline
    $x$     & $y$     & $xy$    & $x^2$  & $y^2$  & $\hat{y}$  & $e=(y-\hat{y})$     & $e^2$  & $(y-\bar{y})^2$ & $(\hat{y}-\bar{y})^2$ \\
    \hline
    50    & 54    & 2700  & 2500  & 2916  & 51.170 & 2.830 & 8.009 & 182.250 & 262.602 \\
    55    & 52    & 2860  & 3025  & 2704  & 55.801 & -3.801 & 14.448 & 240.250 & 133.957 \\
    60    & 62    & 3720  & 3600  & 3844  & 60.432 & 1.568 & 2.459 & 30.250 & 48.205 \\
    65    & 61    & 3965  & 4225  & 3721  & 65.063 & -4.063 & 16.508 & 42.250 & 5.345 \\
    70    & 73    & 5110  & 4900  & 5329  & 69.694 & 3.306 & 10.930 & 30.250 & 5.378 \\
    75    & 74    & 5550  & 5625  & 5476  & 74.325 & -0.325 & 0.106 & 42.250 & 48.303 \\
    80    & 81    & 6480  & 6400  & 6561  & 78.956 & 2.044 & 4.178 & 182.250 & 134.120 \\
    85    & 82    & 6970  & 7225  & 6724  & 83.587 & -1.587 & 2.519 & 210.250 & 262.829 \\
    \hline
    540   & 539   & 37355 & 37500 & 37275 & 539.028 & -0.0280 & 59.1549 & 960   & 900.7389 \\
    \hline
    \end{tabular}%
  \label{regl}%
\end{table}%



\noindent The computation of $a$ and $b$ using Table~\ref{regl} are given by
$$b=\frac{8\times37355-(540)(539)}{8\times37500-(540)^{2}}=0.9262$$
and
$$a=67.375-0.9262(67.5)=4.86.
$$

\noindent The computation of $c$ and $d$ using Table~\ref{regl} are given by
$$c=67.5-1.0132(67.375)=-0.7612$$
$$d=\frac{8\times37355-(540)(539)}{8\times37275-(539)^{2}}=1.0132.$$
\noindent The regression coefficients and intercepts in Table 6.1 are manually computed.
Whereas, We use the lm() function to the same data in order to fit the regression line y on x and given by
<<>>=
model_1 <- lm(y ~ x, data = df)
summary(model_1)
@
<<reg, fig.cap="Graphical representation of the fitted regression line.",fig.height=6, fig.width=13,fig.align='center', warning=FALSE>>=
library(ggpubr)
d1 <- ggplot(df, aes(x = x, y = y)) + geom_point(col = "red") +
    stat_ellipse(col = "black")
d2 <- ggplot(data = df, aes(x = x, y = y)) + geom_smooth(formula = y ~
    x, method = "lm") + geom_point(color = "red") +
    stat_regline_equation(label.x = 30, label.y = 75)
gridExtra::grid.arrange(d1, d2, nrow = 1)
@
\noindent We can only get the the coefficients along with test statistic of the fitted regression model
and given by

<<>>=
summary(model_1)$coefficients
@
\noindent The graphical representation of the fitted regression line is shown in Figure~\ref{fig:reg}. The ggPredict() function allows to get the graphical representation (as shown in Figure~\ref{fig:reg11}) along with fitted regression equation and given by:
<<reg11, fig.cap="Graphical representation of the fitted regression line using predict3d R package.",fig.align='center', warning=FALSE>>=
library(predict3d)
# regression line x on y
ggPredict(model_1, xpos = 0.75, vjust = 1.5, show.error = TRUE)
@


\subsection{ANOVA test}
The anova() function allows to perform anova test in order to check overall significance of the predictor and given by
<<>>=
anova(model_1)
@
\noindent To determine a confidence interval for one or more parameters of the fitted regression
model, the confint() function can be employed. It gives by default 95\%, but we can
change the confidence level and as follows
<<>>=
confint(model_1, level = 0.95)
@
\noindent The tidy() function from broom library can be handy to compute the fitted model along with lower and upper confidence interval and given by
<<>>=
broom::tidy(model_1, conf.int = TRUE)
@

\subsection{R-squared}
A measure of statistical significance known as goodness-of-fit is used to assess how well a regression model fits a dataset. For instance, if R-squares=0.90, that means 90\% variation is explained by the respective regression model. A higher R-squared value tends to better fitting. R-squared has a range of 0 to 1. A value of 0 denotes that there is no explanation of the response variable by the predictor or independent variables. On the other side, a value of 1 denotes a perfect explanation of the response variable by the predictor. R-squared always increases, when a regression model adds a new predictor. The additional variable may or may not be significant, that is the drawback of R-squared. The following expression can be used to evaluate $R^2$ based on Table~\ref{regl}
\begin{equation}
R^{2}=\frac{\text{Regression\,SS}}{\text{Total\,SS}}=\frac{\sum\left(\hat{y}-\bar{y}\right)^{2}}{\sum\left(y-\bar{y}\right)^{2}}=\frac{900.739}{960}=0.9383.
\end{equation}
\noindent Alternatively, we can compute the $R^2$ using Table~\ref{regl} as follows:
$$R^{2}=1-\frac{\sum e^{2}}{\sum(y-\bar{y})}=1-\frac{59.1549}{960}=0.9384.$$

\subsection{Adjusted R-square}
In a regression model, predictors that are not significant are taken into consideration by the adjusted R-squared. Alternatively, the adjusted R-squared indicates whether or not incorporating or adding extra independent variable enhances or improves a regression model. The adjusted R-square can be computed by the following formula
$$R_{adj}^{2}=1-\frac{\left(1-R^{2}\right)(n-1)}{\left(n-k-1\right)}=1-\frac{\left(1-0.9384\right)(8-1)}{(8-1-1)}=0.9281,$$
where the number of independent variables and the number of observations are denoted by $k$ and $n$, respectively.


\subsection{Scatter diagram}
Using scatter diagrams, one may determine whether or not there is a relationship between
the variables. It provides an overview of the form, direction, strength, and outliers of two
variables. A scatter plot, alternatively referred to as a scattergram, scatter diagram, scatterplot, scatter graph, or scatter chart. To generate the scatter diagram, the independent
variable is positioned on the horizontal axis and the dependent variable is on the vertical
axis. The following Figure~\ref{fig:scatter} shows the various types of relationships such as a (i) positive relationship in top left panel, (ii) negative relationship in top right panel, (iii) no no
relationship in bottom left panel and (iv) curvilinear relationship in bottom right panel
<<scatter, fig.cap="Visually inspection of form, direction, strength, and outliers using scatter plot.",fig.height=13, fig.width=13,fig.align='center', warning=FALSE,echo=FALSE>>=
set.seed(123)
x1 <- c(1:150)
y1 <- (2)*x1 + rnorm(150,0,20)
df <- data.frame(cbind(y1,x1))
scatter1<-ggplot(df, aes(x = x1, y = y1)) + geom_point(col = "red")
x2 <- c(1:150)
y2 <- (-2)*x2 + rnorm(150,0,30)
df1 <- data.frame(cbind(y2,x2))
scatter2<-ggplot(df1, aes(x = x2, y = y2)) + geom_point(col = "red")
x3 <- c(1:150)
y3 <- x + rnorm(150,0,10)
df2 <- data.frame(cbind(y3,x3))
scatter3<-ggplot(df2, aes(x = x3, y = y3)) + geom_point(col = "red")
x4<-c(40,35, 100,	120,	130,	140,	150,	160,	18,	38,	58,	78,	98,	118,	128,	138,	148,	158,	25,	45,	65,	85,	105,	125,	135)
y4<-c(10,8,	20,	16,	14,	12,	8,	0,	2, 14,	18,	23,	18,	14,	12,	10,	6,	1,	1,	13,	17,	22,	17,	13,	11)
length(x)
length(y)
df2 <- data.frame(cbind(y4,x4))
scatter4<-ggplot(df2, aes(x = x4, y = y4)) + geom_point(col = "red")
gridExtra::grid.arrange(scatter1, scatter2,scatter3,scatter4, nrow=2)
@



\subsection{Pearson's correlation}
A statistical measure called correlation describes how much two variables are related
linearly. Positive or negative correlations are possible. In a basic correlation, two variables
are said to be positively correlated when they move in the same direction, and negatively
correlated when they are moving in the opposite way. Zero correlation, or no correlation, is the state in which there is no relationship between two variables. The data vector of
the same length, x and y, is as follows:
<<>>=
x1 <- c(2, 4, 6, 8, 10, 12, 14, 16)
y1 <- c(20, 15, 13, 11, 10, 9, 7, 6)
@
\noindent The cor() function can be used to evaluate the correlation coefficient either choosing one of
the following pearson, kendall and spearman. A parametric correlation is calculated using
the Pearson Correlation Method. Non-parametric rank-based correlation tests include the
Kendall and Spearman correlation methods and as follows:
<<eval=FALSE>>=
cor(x1, y1, method = c("pearson", "kendall", "spearman"))
@

\noindent The Pearson correlation coefficient can be computed simply by using cor(x1, y1) and is
given by
<<>>=
cor(x1, y1, method = "pearson")
@
\noindent Test for association between paired samples or tests of no correlation. If the samples
have independent normal distributions, the test statistic is based on Pearson's product
moment correlation coefficient cor(x1, y1) and has a t distribution with length(x)-2 degrees
of freedom and given by
<<>>=
cor.test(x1, y1, method = "pearson")
@
\noindent The manual computation of the Pearson's correlation coefficient ($r$), when two variable
moving in opposite direction is given by

\begin{table}[H]
  \centering
  \caption{Computation of $r$, when two variable moving in opposite direction.}
    \begin{tabular}{c|cccccccc|c}
    \hline
    $x$     & 2     & 4     & 6     & 8     & 10    & 12    & 14    & 16    & 72 \\
    $y$     & 20    & 15    & 13    & 11    & 10    & 9     & 7     & 6     & 91 \\
    $xy$    & 40    & 60    & 78    & 88    & 100   & 108   & 98    & 96    & 668 \\
    $x^2$    & 4     & 16    & 36    & 64    & 100   & 144   & 196   & 256   & 816 \\
    $y^2$    & 400   & 225   & 169   & 121   & 100   & 81    & 49    & 36    & 1181 \\
    \hline
    \end{tabular}%
  \label{cor}%
\end{table}%

\noindent The correlation coefficient is ranging from $[-1, +1]$. A perfectly positive correlation is
represented by a value of 1, whereas a perfectly inverse or negative correlation is represented by a value of -1. In general, a correlation coefficient that falls between 0.4 to 0.7 is
regarded as moderate. When $r$ = 0, it indicates that the two variables are not correlated
or independent
\begin{equation}
r=\frac{n\sum xy-(\sum x)(\sum y)}{\sqrt{\left[n\sum x^{2}-(\sum x)^{2}\right]\left[n\sum y^{2}-(\sum y)^{2}\right]}}.
\end{equation}
$$r=\frac{8(668)-(72)(91)}{\sqrt{\left[8\left(816\right)-(72)^{2}\right]\left[8\left(1181\right)-(91)^{2}\right]}}=-0.9646$$
\noindent The presence of positive correlation is indicated when two variables move in the same
direction. Here is how r is manually computed as well as using R code and provided by:
<<>>=
x2 <- c(2, 4, 6, 8, 10, 12, 14, 16)
y2 <- c(12, 13, 16, 20, 23, 26, 29, 30)
cor(x2, y2)
@

\begin{table}[H]
  \centering
  \caption{Computation of correlation, when two variable moving in same direction.}
    \begin{tabular}{c|cccccccc|c}
    \hline
    $x$     & 2     & 4     & 6     & 8     & 10    & 12    & 14    & 16    & 72 \\
    $y$     & 12    & 13    & 16    & 20    & 23    & 26    & 29    & 30    & 169 \\
    $xy$    & 24    & 52    & 96    & 160   & 230   & 312   & 406   & 480   & 1760 \\
    $x^2$    & 4     & 16    & 36    & 64    & 100   & 144   & 196   & 256   & 816 \\
    $y^2$    & 144   & 169   & 256   & 400   & 529   & 676   & 841   & 900   & 3915 \\
    \hline
    \end{tabular}%
  \label{cor1}%
\end{table}%

$$r=\frac{8(1760)-(72)(169)}{\sqrt{\left[8\left(816\right)-(72)^{2}\right]\left[8\left(3915\right)-(169)^{2}\right]}}=0.9929.$$

<<corr, fig.cap="Scatter plot showing negative and positive correlation.",fig.height=6, fig.width=13, fig.align='center', warning=FALSE>>=
library(ggpubr)
df <- data.frame(cbind(x1, y1))
df1 <- df[c("x1", "y1")]
gs <- ggscatter(df1, x = "x1", y = "y1", color = "blue",
    add = "reg.line", add.params = list(color = "red",
        fill = "lightgreen"), conf.int = TRUE, xlab = "x",
    ylab = "y")
gs1 <- gs + stat_cor(method = "pearson", label.x = 3,
    label.y = 22)
df <- data.frame(cbind(x2, y2))
df2 <- df[c("x2", "y2")]
gs <- ggscatter(df2, x = "x2", y = "y2", color = "blue",
    add = "reg.line", add.params = list(color = "red",
        fill = "lightgreen"), conf.int = TRUE, xlab = "x",
    ylab = "y")
gs2 <- gs + stat_cor(method = "pearson", label.x = 3,
    label.y = 33)
gridExtra::grid.arrange(gs1, gs2, nrow = 1)
@

\subsection{Multiple linear regression}
Simple linear regression model deals with one regressor. On the other side, multiple regression model is an extension of simple linear regression model. It has at least two regressors. The following is the multiple regression model with $m$ regressor and as follows:

\begin{equation}\label{reg1}
y_{i}=\beta_{0}+\beta_{1}x_{1i}+\beta_{2}x_{2i}+\cdots+\beta_{m}x_{mi}+e_{i},
\end{equation}
\noindent where $i=1,2,\cdots,n.$ The Eq.~\eqref{reg1} can be written and follows:
\begin{equation}\label{reg11}
y_{i}=\beta_{0}+\sum_{z=1}^{m}\beta_{z}x_{zi}+e_{i},
\end{equation}
the parameters $\beta_0$ and $\beta_z$ are the $y$-intercept and regression coefficients, respectively. Moreover, the parameter $\beta_z$ refer to the expected or mean change in the dependent variable $y$ one or per unit change in $x_z$ holding all other remaining regressor fixed. Therefore, the regression coefficients are often known as partial regression coefficients.

\subsection{Assumptions associated to multiple regression analysis}
Multiple regression models require the fulfillment of five fundamental assumptions: (1) linearity (it is assumed that there is a linear relationship between the predictor (x) and the outcome (y)); (2) homoscedasticity (we assume that the variance of the residuals is constant); (3) independence of errors (this means that the residuals should therefore not be correlated); (4) normality (the residuals need to exhibit a normal distribution); and (5) independence of explanatory  variables (multicollinearity occurs when two or more of the variables have a high correlation with one another).
<<>>=
y <- c(162.2, 158, 157, 155, 156, 154.1, 169.1, 181,
    174.9, 180.2, 174)
x1 <- c(51, 52.9, 56, 56.5, 58, 60.1, 58, 61, 59.4,
    56.1, 61.2)
x2 <- c(108, 111, 115, 116, 117, 120, 124, 127, 122,
    121, 125)
df <- as.data.frame(cbind(y, x1, x2))
model <- lm(y ~ x1 + x2, data = df)
@

\subsection{Linearity test}
The linearity test determines whether or not a relationship is linear. We have a linear relationship, which is the null hypothesis. The evidence of non-linearity is demonstrated if the $p$-value is significant.
\begin{itemize}
\item $H_0$: There is linear relationship.
\item $H_1$: There is non-linear relationship.
\item Conclusion: If we have a significant $p$-value ($p$-value $<$ 0.05), then we have a nonlinear relationship.
\end{itemize}
We can perform the rain test using lmtest library with raintest() function and as follows:
<<>>=
lt <- lmtest::raintest(y ~ x1 + x2)
lt
@
Here, the $p$-value is not significant. There is an evidence of linearity.

\subsection{Breusch-Pagan test for heteroscedasticity}
The Breusch-Pagan test can be used to determine whether or not our errors have constant variance.

\begin{itemize}
\item $H_0$: Error terms have constant variance (homoscedasticity).
\item $H_1$: Error terms have no constant variance (heteroscedastic).
\item Conclusion: The data is considered significantly heteroscedastic if the p-value is less than 5\%.
\end{itemize}
We can perform the Breusch-Pagan test using lmtest library with bptest() function and as follows:
<<>>=
lmtest::bptest(model)
@
The $p$-value 0.2612 indicates that there is not enough evidence to rule out the null hypothesis.

\subsection{Normality test}
The normality of the model residuals, or their normal distribution, is assessed using the Shapiro-Wilk test.


\begin{itemize}
\item $H_0$: Data come form normal distribution.
\item $H_1$: Data do not come form normal distribution.
\item Conclusion: The small $p$-value tends to reject the null hypothesis of normality.
\end{itemize}
We can perform the normality test using stats library with shapiro.test () function and standardized residual (the residual divided by an estimate of the standard deviation of the residuals) can be obtained using rstandard() function and as follows:
<<>>=
z <- rstandard(model)
shapiro.test(z)
@
\noindent The $p$-value 0.5219 indicates that there is not enough evidence to reject the null hypothesis that is the data come from normal distribution.


\subsection{Multicollinearity test}
Multicollinearity is the term for the significant correlation that exists between two or more variables. Higher standard errors are produced by interrelated independent variables.

\begin{itemize}
\item If vif less than 5 shows low correlation of that predictor with other predictors.
\item If vif falls between 5 and 10 indicates moderate correlation.
\item If vif grater than 10 shows high correlation.
\end{itemize}
<<>>=
y <- c(162.2, 158, 157, 155, 156, 154.1, 169.1, 181,
    174.9, 180.2, 174)
x1 <- c(51, 52.9, 56, 56.5, 58, 60.1, 58, 61, 59.4,
    56.1, 61.2)
x2 <- c(15, 19, 16, 16.5, 17, 20, 10, 12, 22, 21, 35)
x3 <- c(10, 29, 20, 30, 10, 20, 10, 20, 20, 20, 30)
df <- as.data.frame(cbind(y, x1, x2, x3))
model1 <- lm(y ~ x1 + x2 + x3, data = df)
@

<<>>=
car::vif(model1)
@

\subsection{Durbin-Watson test}
The errors (residuals) are assumed to be independent of one another under the assumption of residual independence. The Durbin-Watson test can be used to verify this assumption. The Durban Watson statistic is always going to be between the range of 0 and 4. There is no autocorrelation, when the value is DW = 2. A negative serial correlation is indicated if DW$>$2, and a positive autocorrelation is shown by if DW$<$2.

\begin{itemize}
\item $H_0$: there is no autocorrelation.
\item $H_1$: there is autocorrelation.
\item Conclusion: if $p$-value greater than 5\%, we fail to reject the null hypothesis that means there is no autocorrelation.
\end{itemize}
<<>>=
dw <- lmtest::dwtest(model)
@

\subsection{Diagnostic plots with interpretation}

A scatter plot is a quick visual inspection to know the pattern of dependent and the predictor. The following Figure~\ref{fig:LIN} showing the linear (left) and non-linear (right) pattern. Based on Figure~\ref{fig:LIN}, one can decide to use adequate model for better output.
<<LIN, fig.cap="Diagnostic plots with linear (left) and non-linear (right) pattern.",fig.height=6, fig.width=13,fig.align='center', warning=FALSE, message=FALSE>>=
set.seed(123)
x <- c(1:150)
y <- x^2 + rnorm(150, 0, 20)
y1 <- 2 * x + rnorm(150, 0, 20)
df <- data.frame(cbind(y, x))
model <- lm(y ~ x, data = df)
ln1 <- ggplot(df, aes(x = x, y = y)) + geom_point() +
    geom_smooth(method = "lm")
df1 <- as.data.frame(cbind(y1, x1))
model1 <- lm(y1 ~ x, data = df1)
ln2 <- ggplot(df1, aes(x = x, y = y1)) + geom_point() +
    geom_smooth(method = "lm")
gridExtra::grid.arrange(ln2, ln1, nrow = 1)
@
\noindent Figure~\ref{fig:Dia} shows the set of four diagnostic plots namely a residuals versus fitted values plot (top left), a normal quantile-quantile (Q-Q) plot of the residuals (top right), scale-location plot (bottom left) and the residual versus  leverage plot (bottom right).

\begin{itemize}
\item A residuals versus fitted values plot: This plot is commonly used to assess assumption of linearity and homoscedasticity (constant variance).
\item The assumptions may be fulfilled if the points are randomly distributed along a horizontal (blue) line close to zero (as shown in Figure~\ref{fig:LI}, top (left)).
\item On the other side, clearly violation of linearity and homoscedasticity assumption as shown in Figure~\ref{fig:Dia}. A residuals versus fitted values plot showing a non-linear pattern.
\item A normal Q-Q plot of the residuals. It is used to assess the normality of the residuals. A straight line's deviations could indicate deviations from normality.
\item Scale-location plot: this plot is used to assess homoscedasticity and check for constant variance of residuals. A horizontal (blue) line with uniformly spaced dots is the best representation of a spread-location plot (as shown in Figure~\ref{fig:LI}.
\item The purpose of this plot is to highlight the outliers that have a significant impact on the regression model and a high leverage. A significant number of points in an appropriate model should be grouped close to the centre (as shown in Figure~\ref{fig:LI}.
\end{itemize}
\noindent The diagnostic plots can be obtained using  autoplot function from \@{ggfortify library}. The individual plots can be obtained and as follows:
<<eval=FALSE>>=
library(ggfortify)
# a residuals versus fitted values plot
autoplot(model, 1)
# a normal Q-Q plot of the residuals
autoplot(model, 2)
# scale-location plot
autoplot(model, 3)
# residual versus leverage plot
autoplot(model, 4)
@
\noindent A set of all four plots can also be obtained using {\sf autoplot ()} function and given by
<<Dia, fig.cap="Diagnostic plots with non-linear pattern.",fig.align='center', warning=FALSE, message=FALSE>>=
library(ggfortify)
autoplot(model)
@

<<LI, fig.cap="Diagnostic plots linear pattern.",fig.align='center', warning=FALSE, message=FALSE>>=
library(ggfortify)
autoplot(model1)
@

\subsection{Model parameters estimation}
For simple illustration of the multiple regression model, we take $m=2$, in Eq.~\eqref{reg11} and model reduces to

\begin{equation}
y_{i}=\beta_{0}+\beta_{1}x_{1i}+\beta_{2}x_{2i}+e_{i},
\end{equation}
\noindent the normal equations can be obtained by using the definition of least-square method as follows:

\begin{equation}
L(\beta_{0},\beta_{1},\beta_{2})=\sum_{i=1}^{n}\left(y_{i}-\beta_{0}+\beta_{1}x_{1i}+\beta_{2}x_{2i}\right)^{2},
\end{equation}
the estimators of $\beta_{0}, \beta_{1}$ and $\beta_{2}$, say $\hat{\beta_0}$, $\hat{\beta_1}$ and $\hat{\beta_2}$ satisfy the

\begin{equation}\label{rege1}
\frac{\partial L}{\partial\beta_{0}}=\sum_{i=1}^{n}\left(y_{i}-\hat{\beta_{0}}+\hat{\beta_{1}}x_{1i}+\hat{\beta_{2}}x_{2i}\right)=0,
\end{equation}

\begin{equation}\label{rege2}
\frac{\partial L}{\partial\beta_{1}}=\sum_{i=1}^{n}\left(y_{i}-\hat{\beta_{0}}+\hat{\beta_{1}}x_{1i}+\hat{\beta_{2}}x_{2i}\right)x_{1i}=0,
\end{equation}
and
\begin{equation}\label{rege3}
\frac{\partial L}{\partial\beta_{2}}=\sum_{i=1}^{n}\left(y_{i}-\hat{\beta_{0}}+\hat{\beta_{1}}x_{1i}+\hat{\beta_{2}}x_{2i}\right)x_{2i}=0.
\end{equation}
By solving the above Eqs.~\eqref{rege1}-\eqref{rege3} simultaneously, we can obtain the estimators $\hat{\beta_0}$, $\hat{\beta_1}$ and $\hat{\beta_2}$ and given by


\begin{equation}\label{b0}
\hat{\beta_{0}}=\bar{y}-\hat{\beta_{1}}\bar{x_{1}}-\hat{\beta_{2}}\bar{x}_{2},
\end{equation}

\begin{equation}\label{b1}
\hat{\beta_{1}}=\frac{\left(\sum x_{2i}^{2}\right)\left(\sum x_{1i}y_{i}\right)-\left(\sum x_{1i}x_{2i}\right)\left(\sum x_{2i}y_{i}\right)}{\left(\sum x_{1i}^{2}\right)\left(\sum x_{2i}^{2}\right)-\left(\sum x_{1i}x_{2i}\right)^{2}},
\end{equation}
and
\begin{equation}\label{b2}
\hat{\beta_{2}}=\frac{\left(\sum x_{1i}^{2}\right)\left(\sum x_{2i}y_{i}\right)-\left(\sum x_{1i}x_{2i}\right)\left(\sum x_{1i}y_{i}\right)}{\left(\sum x_{1i}^{2}\right)\left(\sum x_{2i}^{2}\right)-\left(\sum x_{1i}x_{2i}\right)^{2}}.
\end{equation}

\subsection{Fitting multiple regression model on small scale data by hand}
Here, we illustrate the multiple regression model and estimating its parameters using a small scale data and as follows in Table~\ref{tab:mrh}:
\begin{table}[H]
  \centering
  \caption{Computational summary of fitting multiple regression model.}
    \begin{tabular}{ccccccccccc}
    \hline
    $y_i$     & $x_{1i}$    & $x_{2i}$    & $x_{1i}y_i$   & $x_{2i}y_i$   & $x_{1i}x_{2i}$  & $x_{1i}^2$ & $x_{2i}^2$ & $\hat{y_i}$  & $(\hat{y_i}-\bar{y})^2$ & $(y_i-\bar{y})^2$ \\
    \hline
    40    & 10    & 14    & 400   & 560   & 140   & 100   & 196   & 40.83 & 23.29 & 16 \\
    38    & 12    & 16    & 456   & 608   & 192   & 144   & 256   & 37.04 & 1.08  & 4 \\
    35    & 14    & 24    & 490   & 840   & 336   & 196   & 576   & 38.73 & 7.47  & 1 \\
    42    & 16    & 25    & 672   & 1050  & 400   & 256   & 625   & 34.03 & 3.88  & 36 \\
    25    & 18    & 26    & 450   & 650   & 468   & 324   & 676   & 29.33 & 44.50 & 121 \\
    \hline
    180   & 70    & 105   & 2468  & 3708  & 1536  & 1020  & 2329  & 179.96 & 80.21 & 178 \\
    \hline
    \end{tabular}%
  \label{tab:mrh}%
\end{table}%

\noindent The following formulas can be used to evaluate the coefficients of multiple regression model given by in Eq.~\eqref{b0}-\eqref{b2}. Using Table~\ref{tab:mrh}, one can obtain the following numerical computation and as follows:
$$\sum x_{1i}^{2}=\sum x_{1i}^{2}-\frac{\left(\sum x_{1i}\right)^{2}}{n}=1020-\left(70\right)^{2}/5=40,$$
similarly,
$$\sum x_{2i}^{2}=\sum x_{2i}^{2}-\frac{\left(\sum x_{2i}\right)^{2}}{n}=2329-\left(105\right)^{2}/5=124.$$
After simplification, it yields
$$\sum x_{1i}y_{i}=\sum x_{1i}y_{i}-\frac{\left(\sum x_{1i}\right)\left(\sum y_{i}\right)}{n}=2468-\frac{\left(105\right)(180)}{5}=-52.$$


$$\sum x_{2i}y_{i}=\sum x_{2i}y_{i}-\frac{\left(\sum x_{2i}\right)\left(\sum y_{i}\right)}{n}=3708-\frac{\left(105\right)(180)}{5}=-72.
$$

$$\sum x_{1i}x_{2i}=\sum x_{1i}x_{2i}-\frac{\left(\sum x_{1i}\right)\left(\sum x_{2i}\right)}{n}=1536-\frac{\left(70\right)(105)}{5}=66.$$

\noindent By substituting the above computed numeric values in Eq.~\eqref{b0}-\eqref{b2} and simplification, we get $\hat{\beta_{0}}$, $\hat{\beta_{1}}$ and $\hat{\beta_{2}}$ and as follows:

$$\hat{\beta_{1}}=\frac{\left(124\right)\left(-52\right)-\left(66\right)\left(-72\right)}{\left(124\right)\left(40\right)-\left(66\right)^{2}}=2.8079,$$

$$\hat{\beta_{2}}=\frac{(40)(-72)-(66)(-52)}{\left(124\right)\left(40\right)-\left(66\right)^{2}}=0.9139,$$
and
$$\hat{\beta_{0}}=36+2.8079(14)-0.9139(21)=56.119.$$
\noindent The following R codes can be used to compute the multiple regression model parameters
<<>>=
y <- c(40, 38, 35, 42, 25)
x1 <- c(10, 12, 14, 16, 18)
x2 <- c(14, 16, 24, 25, 26)
df1 <- data.frame(cbind(y, x1, x2))
mr1 <- lm(formula = y ~ x1 + x2, data = df1)
mr1$coefficients
@

\subsection{Understanding the multiple regression analysis output}

The model has an overall F-statistic of 11.58 and a corresponding $p$-value of 0.0043. This suggests that the fitted model is statistically significant. In other words, the overall fitted model is useful.
<<message=FALSE, warning=FALSE>>=
# serum cholesterol
sc <- c(162.2, 158, 157, 155, 156, 154.1, 169.1, 181,
    174.9, 180.2, 174)
# weight in kilograms
wkg <- c(51, 52.9, 56, 56.5, 58, 60.1, 58, 61, 59.4,
    56.1, 61.2)
# systolic blood pressure
sbp <- c(108, 111, 115, 116, 117, 120, 124, 127, 122,
    121, 125)
df <- data.frame(cbind(sc, wkg, sbp))
@
\noindent The following is the output summary of the multiple linear regression model:
<<>>=
mr <- lm(formula = sc ~ wkg + sbp, data = df)
summary(mr)
@
\noindent The first thing is to look at the bottom of the output summary of the fitted model that is global test of model adequacy or overall test for significance of regression. It is a test which determine the linear relationship between the response variable and any of the regressors. The following are the null and alternative hypothesis:$H_{0}=\beta_{1}=\beta_{2}=\cdots=\beta_{k}=0$ and $H_{1}=\text{At least one of the regression slope \ensuremath{\ne0} }$. In our case, we have two regressors that is $H_{0}=\beta_{1}=\beta_{2}=0$ and $H_{1}=\text{At least one of the regression slope \ensuremath{\ne0} }.$ The following the F-statistic that is 11.58 whereas the critical value (table value) with degree of freedom $v_1$=2 and $v_2$=8 at 5$\%$ level of significance can be computed using the following R code:
<<>>=
qf(0.05,2,8,lower.tail=FALSE)
@
\noindent It is 4.45897 and we can conclude that as the test statistic value=11.58 > than the critical value=4.45897, so therefore, we can reject the null hypothesis and conclude that $H_{1}=\text{At least one of the regression slope \ensuremath{\ne0} }$. On the other side, using $p$-value=0.0043, we can reject the null hypothesis (smaller $p$-value tends to reject the null hypothesis). The F-statistic can be evaluated and given by
\begin{equation}
F=\frac{R^{2}}{\left(1-R^{2}\right)}\left(\frac{n-k-1}{k}\right)=\frac{0.7433}{1-0.7433}\left(\frac{11-2-1}{2}\right)=11.58,
\end{equation}
where $R^2$, $n$ and $k$, respectively, are the multiple R-squared, the number of classes and the number of regressors that is $R^2$=11.58, $n=11$ and $k=2,$ in our case. The least-square line is given by
\begin{equation}
\text{Serum cholesterol}=18.5219-4.0659\times\text{weight}+3.2007\times\text{Systolic blood pressure}
\end{equation}


\begin{itemize}
\item The finding of the analysis showed that significant relationships between the weight ($x_1$) and the serum cholesterol ($y$) and the systolic blood pressure ($x_2$) and serum cholesterol ($y$).
\item A 4.0659\% decrease in the serum cholesterol for every 1\% increase in weight.
\item A 0.178\% increase in the serum cholesterol for every 1\% increase in systolic blood pressure
\end{itemize}
\noindent The multiple regression model is represented graphically (as shown in Figure~\ref{fig:multip}), using two regressors, such as weight in kilograms and systolic blood pressure (sbp), and
dependent variable, serum cholesterol (SC) is given by
<<multip, fig.cap="Graphical representation of multiple regression model with two regressors.",fig.align='center', warning=FALSE, message=FALSE>>=
library(predict3d)
ggPredict(mr, xpos = 0.75, vjust = 1.5, show.error = TRUE)
@
\noindent The anova test shows that the both regressors are significant and making contribution in
the model and as follows:
<<>>=
anova(mr)
@
\noindent One other helpful function () from the gtsummary [6] library is tbl regression, which yields
p-value, odd ratios, and 95\% lower and upper confidence interval in tabulation format and
given by
<<message=FALSE, warning=FALSE>>=
gtsummary::tbl_regression(mr)
@
\subsection{Making prediction for specific values}
R allows to make perdition by specifying the independent variable. The predict() function
and as follows:
<<>>=
v<-data.frame(wkg = 1, sbp = 1)
prediction <- predict(mr, v)
prediction
@
\noindent These two arguments are passed to the predict() function. The first one is the fitted
regression model that is "$mr$" in our case and the other is "$v$" specifying the independent
variable

<<>>=
v1 <- data.frame(wkg = wkg, sbp = 124.66)
prediction <- predict(mr, v1)
prediction
@
\noindent We can make a prediction either specifying the certain value of regressors (as in above case sbp=124.66). The complete prediction can be obtained as follows:
<<>>=
v2 <- data.frame(wkg = wkg, sbp = sbp)
prediction <- predict(mr, v2)
prediction
@




\subsection{Removing intercept from model}
We can eliminate the intercept from our regression model as in the above example we have insignificant intercept. It can be eliminated using $\text{lm(sc\,\textasciitilde\,0\,+\,wkg\,+\,sbp,\,data\,=\,df)}$, which indicates just adding zero before regressors. The following is the output summary of the multiple linear regression model without intercept:
<<>>=
mr1 <- lm(sc ~ 0+wkg + sbp, data = df)
summary(mr1)
@
<<multip1, fig.cap=" Graphical representation of multiple regression model without intercept.",fig.align='center', warning=FALSE, message=FALSE>>=
library(predict3d)
ggPredict(mr1, xpos = 0.75, vjust = 1.5, show.error = TRUE)
@
On the other side, after removing the intercept, the resulting model improved with higher R-squared.


\subsection{Binary logistic regression}
Whenever we use a regression analysis, first we look at the response variable. If the the response variable is categorical and has two category (dichotomous), then the binary logistic regression~\cite{Agresti2012} is the altimate choice. The binary logistics regression can be simple and multiple. In simple logistic regression: It has one response variable (dichotomous) and one regressor (either continuous or categorical). In multiple logistic regression: It has one response variable (dichotomous) and at least two regressors (either continuous or categorical). The premise of linear regression is that the variables have a linear relationship. If the response variable is dichotomous, binary, or categorical, then this assumption is devastated. In other words, when there are two categories for the response variable-yes or no, etc. We employ logistic regression to predict the binary outcomes. Let's say we want to predict the level of HIV-AIDS awareness among respondents who live in rural areas and are between the ages of 18 and 50. If we question the respondents, "Have you heard about HIV-AIDS? " our dependent variable might be categorized as "yes" or "no".  The logistic regression model is a nonlinear transformation of the linear regression. Its linear form after the logit transformation is given by


\subsection{Simple logistic regression with application}
In simple logistic regression~\cite{Campbell1991}, we deal one dependent variable and only one regressor as given by in Eq.~\eqref{bin1}
\begin{equation}
\log\left(\frac{p}{1-p}\right)=a+bx,
\end{equation}
taking exponentiation on both sides
\begin{equation}
\frac{p}{1-p}=e^{a+bx}.
\end{equation}
Hence
\begin{equation}\label{bin1}
p=\frac{e^{a+bx}}{1+e^{a+bx}}\,\,\,\,\text{or\,\,\,\,}p=\frac{1}{1+e^{-\left(a+bx\right)}}.
\end{equation}
Here, we take a small hypothetical data set that represents the academic performance of the students ($y$), and codes it as follows: 0 means the student fails the examination, and 1 means they pass. On the other side, we utilize one regressor, which is the number of study hours ($x$). The data set is as follows:
<<>>=
# students performance 0 for fail and 1 for pass
y <- c(1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1)
# study hours
x <- c(25, 19, 35, 20, 18, 25, 28, 30, 26, 40, 32, 28, 24, 23, 33,
    27, 25, 24, 29)
df1 <- data.frame(cbind(x, y))
@
\noindent The binary logistic regression model can be fitted using glm () function from stats library specifying family as "binomial" and as follows:
<<>>=
bl <- glm(y ~ x, data = df1, family = "binomial")
@
\noindent The estimated values, standard error of the estimates, $z$-value, and significant status of the intercept and slope are included in the output summary of the fitted bl model, which is provided below
<<>>=
summary(bl)
@
\noindent Several model performance related measures of the fitted bl model can be obtained using model\_performance() function from performance~\cite{Ludecke2021} library and given by

<<eval=FALSE>>=
library(performance)
model_performance(bl)
@
\noindent The Hosmer-Lemeshow goodness-of-fit test of the fitted bl model can be obtained using performance\_hosmer () function from performance~\cite{Ludecke2021} library. The function provides the model summary like "model seems to fit well". Generally, the higher $p$-value tend to better fitting.
<<>>=
library(performance)
performance_hosmer(bl)
@
\noindent The following functions namely the coef(), exp(coef()) and exp(confint()) provide the coefficients, odd ratios and 95\% confidence interval of the odd ratios of the fitted model and given by

<<message=FALSE,warning=FALSE>>=
round(cbind(coef(bl), odds_ratio = exp(coef(bl)), exp(confint(bl))),
    4)
@
\noindent The performance\_pcp function gives the percentage of correct predictions from logistic regression model, likelihood-ratio-test. In our case the percentage of correct predictions from logistic regression model is 72.25\% and the likelihood-ratio-test shows the model is significant as $p$-value is less than 5\%.
<<>>=
library(performance)
performance_pcp(bl)
@
\noindent An other useful function () from gtsummary~\cite{Sjoberg2021} library that is tbl\_regression which is used to get odd ratios, 95\% and $p$-value in tabulation format and given by
<<message=FALSE,warning=FALSE>>=
library(gtsummary)
tbl_regression(bl, exp = TRUE)
@


\noindent The interpretation of results based on the above fitted model is a follows: An hour increase in study timing increases the the odds of passing examination by 58\%.

\subsection{Wald test}
In the context of logistic regression, models are compared using the Wald test's best fit criteria. The significant variables are identified using this technique from the set of predictors utilized in model with continuous or binary variables. The Anova () function from car~\cite{Fox2019} library is used to perform the Wald test.
<<>>=
car::Anova(bl, type = 3, test.statistic = "Wald")
@
\noindent The Pseudo R-square measures  namely the Nagelkerke, Cox and Snell and McFadden indicate the amount of variance in response variable is explained by regressors. For instance the Nagelkerke R square of the fitted model tells that 52.78\% of the variance in academic performance of the student is explained by study hours.
<<>>=
library(performance)
r2_nagelkerke(bl)
r2_coxsnell(bl)
@
\noindent The graphical illustration of the fitted model can be obtained using ggPredict () function from predict3d~\cite{Keon2023} library. The function also displays the fitted equation on graph and given by
<<message=FALSE,warning=FALSE, fig.cap="The graphical illustration of academic performance and study hours.">>=
library(predict3d)
ggPredict(bl, xpos = 0.75, vjust = 1.5)
@




\subsection{Multiple logistic regression with applications}
In multiple logistic regression, we have more than one regressor. Here we use the spector data from faraway~\cite{Julian2022} library. The data, which is based on four variables (grade, psi, tuce, and gpa), is used to evaluate how well a novel approach to teaching economics works. For more detail, the readers are referred to Spector and Mazzeo~\cite{Spector1980}. The dependent variable is grade, and it is divided into two categories: 1 denotes improvement in exam grades, while 0 denotes no improvement. The regressor PSI, or personalized system of instruction, is a novel approach to education that falls into two distinct categories. A student is in the PSI group if their score is 1, and they are not in the PSI group if their score is 0. Cumulative grade point average (gpa) is a continuous regressor and tuce is a measure of ability when entering the class. The following is the data display


<<warning=FALSE, message=FALSE>>=
library(faraway)
head(spector, 8)
@
\noindent The binary logistic regression with three regressors is fitted and as follows:
<<>>=
bm <- glm(grade ~ factor(psi) + tuce + gpa, data = spector,
    family = "binomial")
summary(bm)
@
\noindent The output summary shows that the tuce variable is insignificant (making no contribution to model). So we remove it from the model and again run the model and given by

<<>>=
bm1 <- glm(grade ~ factor(psi) + gpa, data = spector,
    family = "binomial")
summary(bm1)
@

\noindent The findings derived from the multiple binary logistic regression model fitted above are interpreted as follows: Being the psi group student (a new teaching method) increase the odds of improving exam grades by 9.4\%. A unit increase in grade point average increases the odds of improving exam grades by 20.4\%. The fitted model is shown graphically by two equations: the blue equation indicates when a student in the Psi group is taken into consideration as the reference category, and the black equation indicates when no Psi group student is taken into consideration.

<<message=FALSE,warning=FALSE>>=
round(cbind(coef(bm1), odds_ratio = exp(coef(bm1)),
    exp(confint(bm1))), 4)
@

<<message=FALSE,warning=FALSE,fig.cap="The fitted multiple binary logistic regression model.">>=
library(predict3d)
ggPredict(bm1, interactive = TRUE, digits = 4)
@



\subsection{Changing reference category}
The following functions allow to change the reference category of categorical independent variable. By default glm () function section first category as reference category. We can change the reference category for instance, here we have psi as a categorical independent variable with two level 0 and 1. We can select either 0 or 1 using factor(psi,c(1,0)) function, here 1 is treated as a reference category.
<<>>=
bl2 <- glm(grade ~ factor(psi, c(1, 0)) + gpa, data = spector,
    family = "binomial")
summary(bl2)$coefficients
@
\noindent Using factor(psi,c(0,1)), here 0 is treated as a reference category.
<<>>=
bl3 <- glm(grade ~ factor(psi, c(0, 1)) + gpa, data = spector,
    family = "binomial")
summary(bl3)$coefficients
@

\subsection{Multinomial logistic regression}
Multinational logistic regression~\cite{Chatterjee2013} is an extension of the binary logistic regression~\cite{Agresti2012}. In binary logistic regression, the dependent variable has two nominal category, whereas in multinational logistic regression the dependent variable has at least three categories. For example, in the following hypothetical data, we have $y$, the dependent variable, $x$ and $z$ are two independent variables.

\begin{itemize}
\item we have one dependent variable $y$ and two regressors ($x$ and $z$).
\item Here $y$ is the dependent variable that is casting a vote to a either of the three political parties. The zero represents party A, one represents party B and two represents party C.
\item $x$ denotes the age in years of respondent.
\item $z$ is a categorical independent variable and has two levels, 1 for male and 2 for female.
\end{itemize}
<<>>=
# the dependent variable (three categories 0, 1 and 2)
y <- c(0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 1, 1, 1, 1, 1,
    2, 1, 1)
# age in years (independent variable)
x <- c(30, 33, 36, 19, 19, 30, 30, 30, 30, 19, 19, 19, 30, 30, 33,
    36, 19, 19, 30, 30, 36, 40, 20, 22, 43, 42, 22, 23, 18, 19, 20,
    22, 30, 33, 36, 19, 19, 30, 30, 30, 30, 19, 19, 19)
# gender 1 for male and 2 for female (independent variable)
z <- c(1, 1, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1,
    1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1,
    1, 2, 2)
df1 <- data.frame(cbind(x, y, z))
@
\subsection{Multinomial logistic regression using nnet package}
Here we demonstrate the two different R packages to fit the multinational regression analysis, the first one is multinom () function from nnet~\cite{Venables2002} library.
<<>>=
library(nnet)
multi <- multinom(formula = y ~ x + factor(z, c(2,
    1)), data = df1)
summary(multi)
@
\noindent The following is the output summary of multinational logistic regression based on multinom () function. The Pseudo R-square measures  namely the Nagelkerke, Cox and Snell and McFadden indicate the amount of variance in response variable is explained by regressors. For instance the Nagelkerke R square of the fitted model tells that 30.59\% of the variance in casting vote is explained by age and gender.
<<>>=
r2_nagelkerke(multi)
r2_coxsnell(multi)
r2_mcfadden(multi)
@
\noindent The likelihood ratio test indicates the regressor age and gender both are significant as p-value is less the 5\%.
<<>>=
car::Anova(multi, type = 3, test.statistic = "Wald")
@
\noindent Here, we interpret the results of the fitted model, the following is the odd ratios and indicating that a year increase in age decreases the odds of voting parity B by 8.89\% and being a male increases the odds to vote parity B by 11.25\% compared to party A. The odd ratios for other category (that is category 2) can be interpreted in a similar manner.

<<>>=
odds_ratio = exp(coef(multi))
odds_ratio
@


\subsection{Multinomial logistic regression using mlogit package}

The multinational logistic regression model can be fitted using mlogit () function form mlogit~\cite{Croissant2020} library. The mlogit () function summary provides more information compare to multinom () function from nnet~\cite{Venables2002} library. A minor complication using the mlogit () function is first to covert data into mlogit format using mlogit.data () function and as follows:
<<message=FALSE, warning=FALSE>>=
library(mlogit)
cont = mlogit.data(df1, shape = "wide", choice = "y")
@
<<>>=
mlo = mlogit(y ~ 0 | x + factor(z, c(2, 1)), data = cont,
    reflevel = "0")
summary(mlo)
@
\noindent The mlogit () function is more optional for instance, we can select or change the reference category of response variable using reflevel function in our case we use zero as a reference or base line category as reflevel$=0$.
<<>>=
round(cbind(coef(mlo), odds_ratio = exp(coef(mlo)),
    exp(confint(mlo))), 4)
@


\begin{thebibliography}{99}
\bibitem{Alboukadel2023}
Alboukadel Kassambara (2023). ggpubr: 'ggplot2' Based
Publication Ready Plots. R package version 0.6.0.
https://CRAN.R-project.org/package=ggpubr
\bibitem{Keon2023}
Keon-Woong Moon (2023). predict3d: Draw Three
Dimensional Predict Plot Using Package 'rgl'. R package
version 0.1.4.
https://CRAN.R-project.org/package=predict3d
\bibitem{Achim2002}
Achim Zeileis, Torsten Hothorn (2002). Diagnostic Checking in
Regression Relationships. R News 2(3), 7-10. URL
https://CRAN.R-project.org/doc/Rnews/
\bibitem{Yuan2016}
  Yuan Tang, Masaaki Horikoshi, and Wenxuan Li. "ggfortify:
  Unified Interface to Visualize Statistical Result of Popular R
  Packages." The R Journal 8.2 (2016): 478-489.
  \bibitem{David2023}
  David Robinson, Alex Hayes and Simon Couch (2023). broom:
    Convert Statistical Objects into Tidy Tibbles. R package version
  1.0.5. https://CRAN.R-project.org/package=broom
  \bibitem{Sjoberg2021}
  Sjoberg DD, Whiting K, Curry M, Lavery JA, Larmarange
  J. Reproducible summary tables with the gtsummary
  package. The R Journal 2021;13:570-80.
  https://doi.org/10.32614/RJ-2021-053.
  \bibitem{Ludecke2021}
  Ludecke et al., (2021). performance: An R Package for
  Assessment, Comparison and Testing of Statistical Models.
  Journal of Open Source Software, 6(60), 3139.
  \bibitem{Julian2022}
  Julian Faraway (2022). faraway: Functions and Datasets
  for Books by Julian Faraway. R package version 1.0.8.
  https://CRAN.R-project.org/package=faraway.
  \bibitem{Venables2002}
  Venables, W. N., \& Ripley, B. D. (2002). Modern Applied Statistics with S, Springer, New York: ISBN 0-387-95457-0.
  \bibitem{Croissant2020}
  Croissant, Y. (2020). Estimation of random utility models in R: the mlogit package. Journal of Statistical Software, 95, 1-41.
\bibitem{Galton1877}
Galton, F. (1877). Typical Laws of Heredity 1. Nature, 15(389), 512-514.
\bibitem{Campbell1991}
Campbell, M. J., \& Dobson, A. J. (1991). An Introduction to Generalized Linear Models. Biometrics, 47(1), 347.
\bibitem{Agresti2012}
Agresti, A. (2012). Categorical data analysis (Vol. 792). John Wiley \& Sons.
\bibitem{Chatterjee2013}
Chatterjee, S., \& Simonoff, J. S. (2013). Handbook of regression analysis (Vol. 5). John Wiley \& Sons.
\bibitem{Fox2019}
Fox J, Weisberg S (2019). An R Companion to Applied
Regression, Third edition. Sage, Thousand Oaks CA.
\bibitem{Spector1980}
Spector, L. C., \& Mazzeo, M. (1980). Probit analysis and economic education. The Journal of Economic Education, 11(2), 37-44.
\end{thebibliography}










\chapter{Creating R Package: A Minimal Example }\label{CH7}

\section{Introduction}
This Chapter covers the essential settings related to R package creation and as follows: Some important setting in RStudio, installing the roxygen2 R package and its implementation to package development, deleting NAMESPACE file and again creation with roxygen2 setting, fixing errors, warnings and notes, testing and installing package into library and finally, building a pdf reference manual of the developed package.


\subsection{Creating a new package}
Using R-studio setting, the R package can be created. First of all, from the top left of your R-studio interface (see: Figure~\ref{S2_C1}), Go to the \textbf{file} and click on the \textbf{New project} (see: Figure~\ref{S2_C1}).

\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=14.0cm]{S2_C1}~
\caption{Development of R package, step-1.}\label{S2_C1}
\end{center}
\end{figure}

\noindent After that new window is opened with three options (New Directory, Existing Directory, and Visual Control), click on \textbf{New Directory} (starting a project in a brand new working directory), and then click on \textbf{R package}. A new window is opened and click on \textbf{Browse} and save your package in the new working directory, carefully choose your package name (in this case we create a package and named it "des" as highlighted at bottom right as shown in Figure~\ref{S12_C1}) and click on the bottom right on \textbf{create package} tab.

\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=14.0cm]{S12_C1}~
\caption{Development of R package, step-2.}\label{S12_C1}
\end{center}
\end{figure}
\noindent The pictorial illustrations for creating R package are given by Figures.~\ref{S2_C1}--\ref{S12_C1}. Finally, the bottom left red rectangle (see: Figure~\ref{S12_C1}) shows the initially created R package containing the following files and folders

\begin{itemize}
\item R folder
\item man folder
\item DESCRIPTION file
\item NAMESPACE
\item desRproj, the package name
\end{itemize}

\subsection{Important configuration settings}
\noindent Before starting the following settings are required and as follows: Go to Build and then click Configure Build Tools (see Figure~\ref{S13_C1}), and click on the following as shown in Figure~\ref{S15_C1} and click OK.
\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=14.0cm]{S13_C1}~
\caption{Development of R package and important configuration settings, step-1.}\label{S13_C1}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=14.0cm]{S14_C1}~
\caption{Development of R package and important configuration settings, step-2.}\label{S14_C1}
\end{center}
\end{figure}
\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=14.0cm]{S15_C1}~
\caption{Development of R package and important configuration settings, step-3.}\label{S15_C1}
\end{center}
\end{figure}


\subsection{Installing and loading important R packages}
roxygen2 R package helps in writing 'NAMESPACE' file, your Rd documentation, and the collation field. It is simpler to maintain updated documentation when your needs change when documentation is written concurrently with code. It is simply installed by executing install.packages("package name") command and given by
<<chp7, eval=FALSE>>=
install.packages("roxygen2")
install.packages("devtools")
install.packages("usethis")
install.packages("testthat")
@
\noindent The other way of installing packages using RStudio as follows: Go to Packages, then click on Install tab, and write roxygen2 in a blank tab it will automatically search the required package, and finally, click on roxygen2. The automatic installation will start and take few seconds (as shown in subsection~\ref{ins}). When the installation is completed, then load the package. Similarly, the other packages can be installed (make sure your internet connection).

<<chp77, eval=FALSE>>=
library("roxygen2")
library("devtools")
library("usethis")
library("testthat")
@

\subsection{Deleting NAMESPACE file}
Before writing your R code and documentation, delete "NAMESPACE" file as shown in Figure~\ref{S20_C1}. After deleting Go to Build and click on More and then click on Document (see Figure~\ref{S21_C1}). The roxygen2 will create new NAMESPACE file automatically, to this end, Go to Build click on configure and click on NAMESPACE file and then OK. One more time Go to Build and click on More and then click on Document, the new roxygen2- based NAMESPACE file is created as shown in Figure~\ref{S23_C1}
\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=14.0cm]{S20_C1}~
\caption{Deleting NAMESPACE file and creating it again, step-1.}\label{S20_C1}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=14.0cm]{S21_C1}~
\caption{Deleting NAMESPACE file and creating it again, step-2.}\label{S21_C1}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=14.0cm]{S22_C1}~
\caption{Deleting NAMESPACE file and again creating it, step-3.}\label{S22_C1}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=14.0cm]{S23_C1}~
\caption{Deleting NAMESPACE file and creating it again, step-4.}\label{S23_C1}
\end{center}
\end{figure}


\section{Fixing errors, warnings and notes}
The one of the essential part of creating R package is to check and fix error, so that the created package works smoothly and yields correct results. The created package can be checked for possible errors, warnings and notes so many times. To do this, first install devtools using the following commands: install.packages ("devtools") and then load package using library (devtools). To this end, Go to Check after clicking on it as shown in Figure~\ref{S24_C1}.

\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=14.0cm]{S24_C1}~
\caption{Fixing errors, warnings and notes, step-1.}\label{S24_C1}
\end{center}
\end{figure}

\noindent An automatic process will start, it will take few seconds and automatically checks your R codes and all other documentation including Rd files, man files and DESCRIPTION file. After the completion of the process, the possible eroors, warnings and notes, message appears as shown in Figure~\ref{S23_C1}. The following Figure~\ref{S23_C1}, shows that we have one error, one warning and zero notes.

\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=14.0cm]{S25_C1}~
\caption{Fixing errors, warnings and notes, step-2.}\label{S25_C1}
\end{center}
\end{figure}
\noindent To see the description of error and warning message, based on the Figure~\ref{S24_C1}. Scroll up and checking outcomes window as shown in Figure~\ref{S25_C1} that is Non-standard license specification.

\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=14.0cm]{S26_C1}~
\caption{Fixing errors, warnings and notes, step-3.}\label{S26_C1}
\end{center}
\end{figure}
\noindent To fix this Go to DESCRIPTION file and replace "License: What license is it under" with "License: GPL-2" (see Figure~\ref{S27_C1}).

\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=14.0cm]{S27_C1}~
\caption{Fixing errors, warnings and notes, step-4.}\label{S27_C1}
\end{center}
\end{figure}

\noindent After that save all changes (see Figure~\ref{S28_C1}) and Go to Check again for possible errors and warnings. Fig~\ref{S29_C1} shows that there is no more warning message after replacing License: GPL-2 information, but still we we one error message.

\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=14.0cm]{S28_C1}~
\caption{Fixing errors, warnings and notes, step-5.}\label{S28_C1}
\end{center}
\end{figure}
\noindent Scroll up the checking outcomes window as shown in Figure~\ref{S30_C1}. To fix this error, there is need to export function in NAMESPACE, click on R folder as shown in Figure~\ref{S31_C1} and use \#' @export" function (see Figure~\ref{S32_C1}), under roxygen2 settings and save the changes and again click on Check for possible errors. Finally, we successfully, fix all the errors and warning messages as shown in Figure~\ref{S33_C1}.
\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=14.0cm]{S29_C1}~
\caption{Fixing errors, warnings and notes, step-6.}\label{S29_C1}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=14.0cm]{S30_C1}~
\caption{Fixing errors, warnings and notes, step-7.}\label{S30_C1}
\end{center}
\end{figure}


\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=14.0cm]{S31_C1}~
\caption{Fixing errors, warnings and notes, step-8.}\label{S31_C1}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=14.0cm]{S32_C1}~
\caption{Fixing errors, warnings and notes, step-9.}\label{S32_C1}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=14.0cm]{S33_C1}~
\caption{Fixing errors, warnings and notes, step-10.}\label{S33_C1}
\end{center}
\end{figure}

\subsection{Testing and installing package into library}
Once you create your R package, after fixing all errors, warnings, and notes. You can test it and install it in your library. To do this, click on Install (see Figure~\ref{S34_C1}), it will take few seconds to install your "des" package into library for use. It is successfully installed to library, now we can use it. After hitting the Run tab, it print the "Hello, world!".


\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=14.0cm]{S34_C1}~
\caption{Development of R package, step-34.}\label{S34_C1}
\end{center}
\end{figure}





\subsection{Building a PDF manual}
A PDF manual of the created R package can be built using devtools R package. It is obtained running the command in your R studio script as shown below. The build PDF manual can be found in your save package directory.
<<cvc, eval=FALSE>>=
devtools::build_manual (pkg = ".", path = NULL)
@







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%         Chapter 8              %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{The Metadata: An Overview}\label{CH8}
\section{Introduction}
In Chapter~\ref{CH7}, we illustrate the basic steps that involves in creating R package, handling errors and warnings, roxigen2-based NAMESPACE file in a brief manner. Chapter~\ref{CH8} devotes to the created R package metadata information. Usually, the first page of pdf reference manual of R package contains the metadata information. In this Chapter, we illustrate handling the DESCRIPTION file of R package, as the DESCRIPTION and NAMESPACE file is responsible to store the important metadata of your package. It contains several things and is as follows:
\begin{itemize}
\item Package: des
\item Type: Package
\item Title: What the Package Does (Title Case)
\item Version: 0.1.0
\item Author: Who wrote it
\item Maintainer: The package maintainer $<\text{yourself@somewhere.net}>$
  \item Description: More about what it does (maybe more than one line) Use four spaces when indenting paragraphs within the Description.
\item License: GPL-2
\item Encoding: UTF-8
\item LazyData: true
\item RoxygenNote: 7.2.3.
\end{itemize}

\noindent In Chapter 1, the pdf manual of "des" created R package is built. The first, page of the PDF manual is shown in Figure~\ref{S1_C2}. It consists of Package, Type, Title, Version, Author, Maintainer, Description of the package, License, Encoding, Lazydata and RoxygenNote.











\subsection{Give your package a name}\label{available}
In the DESCRIPTION file, \textbf{Package} indicates the name of your created package. For example in the previous Chapter, we created a \textbf{"des"} that refers to Package: des. When you created your package and save to directory, the name of the package and \textbf{Type} (Type: Package) automatically link to DESCRIPTION file (see Figure~\ref{S1_C2}).
\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=14.0cm]{S1_C2}~
\caption{Understanding the metadata based on the DESCRIPTION file.}\label{S1_C2}
\end{center}
\end{figure}
\noindent Selecting a suitable package name is crucial. That is easy to remember and pronounce. It's not previously listed on CRAN, GitHub, Wikipedia, or Wiktionary.  The name you gave your package cannot be ended with a period (.). There can be no hyphens (-) or underlines (\_). There must be a letter or a number as the first character. Use of both capital and lowercase characters should be avoided as they cause difficulty when typing and remembering (the package name in lowercase is preferred).
The CRAN package \textbf{"available"}~\cite{2Carl20221} is quite helpful about the name of your R package. It helps us to verify the validity of the package name you created. It can quickly check if the package name you created is already available on GitHub, Wikipedia, CRAN, or Wiktionary. Additionally, it assists by automatically suggesting an appropriate package name using the DESCRIPTION file. Here, we illustrate how the available package is useful in naming your package. The commands that are used are as follows:
\begin{itemize}
\item To check if the name is valid, use the command \textbf{valid\_package\_name ("purr")}.
\item To check if the name is already available, use the command \textbf{available ("purr")}.
\item To check if the name is already available on GitHub, use the command \textbf{available\_on\_github ("purr")}.
\item To check if the name is already available on CRAN, use the command \textbf{available\_on\_cran ("purr")}.
\item To check if the name is already available on wikipedia, use the command \textbf{get\_wikipedia ("purr")}.
\item To check if the name is already available on wiktionary, use the command \textbf{get\_wiktionary ("purr")}.
\end{itemize}
\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=14.0cm]{S2_C2}~
\caption{Selecting a suitable package name with the help of available R package, step-1.}\label{S2_C2}
\end{center}
\end{figure}
The package name \textbf{"purr"} is valid but we cannot use this name as already available on GitHub, Wikipedia, CRAN, and Wiktionary (see Figure~\ref{S2_C2}). Similarly, the package name \textbf{"des"} is also valid and we can use this name for our package as it is already not available on GitHub and CRAN(see Figure~\ref{S3_C2}).

\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=14.0cm]{S3_C2}~
\caption{Selecting a suitable package name with the help of available R package, step-2.}\label{S3_C2}
\end{center}
\end{figure}
\subsection{Title of package}
In the DESCRIPTION file, the \textbf{Title} indicates the title of your created R package. It should be compact and comprehensive. The important thing about \textbf{Title} is as follows:
\begin{itemize}
\item Its length should remain under \textbf{65 words} (CRAN recommended).
\item It should be \textbf{Title Case} (Minor words are lowercase in title case, but all major terms are capitalized. Important Properties of the Distribution is an easy illustration.)
\end{itemize}

\subsection{Version}
The DESCRIPTION file contains details of version information. Usually, the format of the version consists of three components (major, minor, and patch) and each component has some information and meaning about Package status. For example, a package release version consists of three numbers that are 1.6.2 (1 refers to major, 6 refers to minor and 2 refers to patch).
\begin{itemize}
\item Major: When a package with version 1.6.2 is significantly changed and a new version 2.0.0 of the same package is released, compatibility may suffer.
\item Minor: This indicates that your code should continue to function with the new version of the package even if you update to a new minor version. For example, new features have been added to the package version 1.6.2, backward compatibility is still worked if the developer releases a new version with version number 1.7.0.
\item Patch: Bug fixes or other minor changes to the package are indicated by a patch version change. This indicates that even if you update to a new patch version, the code should continue to function with the updated package.
\end{itemize}

\noindent With each new package release, your package's version number increases. If you have created your first package, it will initially have the version 0.0.0.9000. Readers are referred to for more information about the package version cite~\cite{2Wickham2023}

\subsection{Author and maintainer detail}

Both author and maintainer information can be found in the DESCRIPTION file. The individual who contributed significantly to the R package is known as the author ("aut"). While the package and its documentation are maintained by the package's maintainer ("cre"). The contributor ("ctb") stands for someone who contributes less to the package. Whereas "cph" and "fnd" represent, the copyright holder (usually a company) and funder who provides funds for the writing package, respectively \cite{2Hornik2012}.

\subsection{Description}
The description includes a concise, comprehensive, one-paragraph overview, but short. The description provides more information than the package's title. It typically contains information about the package you developed, such as its goal. Your package evaluates what and who can utilize it. The important thing about description is as follows: Your description cannot exceed 80 characters per line, even if it has multiple lines and Next lines should be indented by four spaces.

\section{R objects documentation}\label{Rd file}
R documentation (Rd) is carried out in Rd files format that is similar to (La)TeX and can be converted into several forms, including LaTeX, HTML, and plain text. The \textbf{man} directory of a R package is where Rd files are located. Here, we illustrate, the Rd files using one of our recently presented R package \textbf{dprop}~\cite{2Christophe2023}. Figures~\ref{S19_C2}--\ref{S20_C2} highlight the man folder and Rd files of exp distribution (see Fig~\ref{S20_C2}). The new Rd file can be created using the following command in RStudio, \textbf{Go to File --$>$ New File --$>$ R Documentation}, then write topic name and finally click OK. The new Rd file is opened and you can save it in your package \textbf{man} folder for further use.
The Rd files can be written by two ways: the first way is using man directory as shown in Figures~\ref{S19_C2}-\ref{S20_C2} and the second way is by using roxygen2.
\begin{figure}[H]
\begin{center}
\includegraphics[width=16.5cm,height=14.0cm]{S19_C2}~
\caption{Writing and understanding the all different parts of the Rd files, step-1.}\label{S19_C2}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[width=16.5cm,height=14.0cm]{S20_C2}~
\caption{Writing and understanding the all different parts of the Rd files, step-2.}\label{S20_C2}
\end{center}
\end{figure}

\subsection{Where it appears in the PDF manual, writing in Rd files}
\begin{itemize}
\item \textbf{Name:} It represents the name of your topic (not package) and it places in your PDF reference manual of the CRAN package (compare Figure~\ref{S6_C2} with Figure~\ref{S5_C2}).
\item \textbf{Alias:} It represents the name of the function you define in the R code file of your created R package (compare Figure~\ref{S6_C2} and Figure~\ref{ALI2_C2}).
\item \textbf{Title:} It represents the tile of your topic and it places in your PDF reference manual of the CRAN package (compare Figure~\ref{S6_C2} with Figure~\ref{S5_C2}).
\item \textbf{Usage:} It represents the use of your created function and it places in your PDF reference manual of the CRAN package (compare Figure~\ref{S6_C2} with Figure~\ref{S5_C2}).
\item \textbf{Description:} It represents the description of your topic and it places in your PDF reference manual of the CRAN package (compare Figure~\ref{S6_C2} with Figure~\ref{S5_C2}).
\item \textbf{Arguments} It represents the parameters passed to a function in a programming language are known as arguments (compare Figure~\ref{S6_C2} with Figure~\ref{S5_C2}).
\item \textbf{Details:} It represents the detail regarding your function (mathematical equations or an explanation of the functionality etc.)(compare Figure~\ref{S6_C2} with Figure~\ref{S5_C2}).
\item \textbf{Value:} It gives an explanation of the return value of the function (compare Figure~\ref{S8_C2} with Figure~\ref{S7_C2}).
\item \textbf{References:} It represents the references from the literature related to your topic (compare Figure~\ref{S8_C2} with Figure~\ref{S7_C2}).
\item \textbf{Author:} It represents the details regarding the author(s) of the Rd file along with the email address (compare Figure~\ref{S9_C2} with Figure~\ref{S10_C2}).
\item \textbf{seealso:} It represents the R objects connected or related to your topic or function (compare Figure~\ref{S9_C2} with Figure~\ref{S10_C2}).
\item \textbf{Examples:} It represents examples of the function's use (compare Figure~\ref{S9_C2} with Figure~\ref{S10_C2}).
\end{itemize}

\begin{figure}[H]
\begin{center}
\includegraphics[width=16.5cm,height=14.0cm]{S6_C2}~
\caption{Where it appears in the PDF manual, writing in Rd files, step-1.}\label{S6_C2}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[width=16.5cm,height=14.0cm]{S8_C2}~
\caption{Where it appears in the PDF manual, writing in Rd files, step-2.}\label{S8_C2}
\end{center}
\end{figure}


\begin{figure}[H]
\begin{center}
\includegraphics[width=16.5cm,height=14.0cm]{S9_C2}~
\caption{Where it appears in the PDF manual, writing in Rd files, step-3.}\label{S9_C2}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[width=16.5cm,height=12.0cm]{S5_C2}~
\caption{Where it appears in the PDF manual, writing in Rd files, step-4.}\label{S5_C2}
\end{center}
\end{figure}


\begin{figure}[H]
\begin{center}
\includegraphics[width=18cm,height=12cm]{S7_C2}~
\caption{Where it appears in the PDF manual, writing in Rd files, step-5.}\label{S7_C2}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[width=16.5cm,height=14cm]{S10_C2}~
\caption{Where it appears in the PDF manual, writing in Rd files, step-6.}\label{S10_C2}
\end{center}
\end{figure}

\subsection{Writing R codes in R folder}
Upon creation of the initial package, the R folder holds the R codes, and the man folder contains the Rd files. The R folder contains all of the package-related code as shown in Figure~\ref{ALI2_C2}


\begin{figure}[H]
\begin{center}
\includegraphics[width=16.5cm,height=14cm]{ALI2_C2}~
\caption{Writing R codes in R folder.}\label{ALI2_C2}
\end{center}
\end{figure}

\subsection{Writing mathematical equations in Rd files}
Rd files accommodate the \LaTeX  syntax. The mathematical expression can also be added to the Rd files using \LaTeX just by replacing the $\setminus\text{begin}\{\text{equation}\}$ and $\setminus\text{end}\{\text{equation}\}$ with $\setminus\text{deqn}\{\}$. On the other side, if you want to add that $\alpha>0$, just eliminate the two dollar sign and use $\setminus\text{eqn}\{\setminus\text{alpha}\}.$ The following is the probability density function of the exponential distribution:
\begin{equation}
f(x)=\alpha e^{-\alpha x},
\end{equation}
where $x > 0$ and $\alpha > 0.$
The above exponential distribution can be added to Rd file as shown by Figure~\ref{S21_C2}
\begin{figure}[H]
\begin{center}
\includegraphics[width=16.5cm,height=14.0cm]{S21_C2}~
\caption{Writing mathematical equations in Rd files.}\label{S21_C2}
\end{center}
\end{figure}

\subsection{Do not edit by hand use roxygen2}\label{roxyen2}
We have already briefly demonstrate, the Rd file documentation using man directory (we separately write the Rd files in man directory similar to Latex format and R codes in R directory). Now based on the Figures~\ref{S5_C2}--\ref{S10_C2}, we will show roxygen2-based Rd file documentation using an other R directory (see Figure~\ref{S22_C2}).
%\begin{figure}[H]
%\begin{center}
%\includegraphics[page=10,width=1.2\textwidth,height = 1.1\textheight]{dprop.pdf}~
%\caption{Understanding the metadata, step-1.}\label{S0_C2}
%\end{center}
%\end{figure}
After completion of roxygen2-based Rd files. The document can be roxygenized by run command roxygen2::roxygenise() or using Figure~\ref{S23_C2}. The resulting roxygen2-based Rd file is automatically updated in man directory as shown in Figure~\ref{S24_C2}. The Roxygen comments start with \#' and as follows:
\begin{itemize}
\item \#'  $@$name Exponential Distribution
\item \#'  $@$aliases d\_exp
\item \#'  $@$title Compute the distributional properties of the exponential distribution
\item \#'  $@$description Compute the first four ordinary moments, central moments, mean, and variance, Pearson's coefficient of skewness and kurtosis, coefficient of variation, median and quartile deviation based on the selected parametric values of the exponential distribution.
\item \#'  $@$param Alpha The strictly positive scale parameter of the exponential distribution.
\item \#'  $@$details The following is the probability density function of the exponential distribution:
\item \#'  $@$return d\_exp gives the first four ordinary moments, central moments, mean, and variance, Pearson's coefficient of skewness and kurtosis, coefficient of variation, median and quartile deviation based on the selected parametric values of the exponential distribution. Return denotes the return value.
\item \#'  $@$references Balakrishnan, K. (2019). Exponential distribution: theory, methods and applications. Routledge.\\
Singh, A. K. (1997). The exponential distribution-theory, methods and applications, Technometrics, 39(3), 341-341.
\item \#'  $@$author Muhammad Imran. R implementation and documentation: Muhammad Imran $\smallsetminus \text{email}$
$\{\text{imranshakoor84@yahoo.com}\}$.
\item \#'  $@$seealso $\smallsetminus \text{link}\{\text{d\_wei}\}$, $\smallsetminus \text{link}\{\text{d\_EE}\}$
\item \#'  $@$examples d\_exp(2)
\item \#'  $@$export
\end{itemize}

\begin{figure}[H]
\begin{center}
\includegraphics[width=16.5cm,height=14.0cm]{S22_C2}~
\caption{Writing roxygen2-based Rd files, step-1.}\label{S22_C2}
\end{center}
\end{figure}
\begin{figure}[H]
\begin{center}
\includegraphics[width=16.5cm,height=14.0cm]{S23_C2}~
\caption{Writing roxygen2-based Rd files, step-2.}\label{S23_C2}
\end{center}
\end{figure}
\begin{figure}[H]
\begin{center}
\includegraphics[width=16.5cm,height=15.0cm]{S24_C2}~
\caption{Writing roxygen2-based Rd files, step-3.}\label{S24_C2}
\end{center}
\end{figure}

\section{Format your R codes}\label{formatR}
With the formatR~\cite{Yihui2023} package, we can create elegant R scripts even from scratch. As a user interface, this package also comes with a Shiny app in order to format R codes. After go through a Shiny app, the R code will be more readable and organized by default, adding spaces and indents as needed. The Shiny app can be opened using tidy\_app() function (as shown in Figures~\ref{S13_C2}-\ref{S14_C2}). Just execute the tidy\_app() in RStudio after installing the formatR package, as given by
<<zxzxzx, eval=FALSE>>=
formatR::tidy_app()
@

\noindent Following the execution of tidy\_app(), the Shiny application will launch in a new window when you select "open in browser." Next, just copy the codes, then select "format my code" to format the copied R codes.
\begin{figure}[H]
\begin{center}
\includegraphics[width=16.5cm,height=14.0cm]{S13_C2}~
\caption{Format your R codes using formatR package, step-1.}\label{S13_C2}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[width=16.5cm,height=14.0cm]{S14_C2}~
\caption{Format your R codes using formatR package, step-2.}\label{S14_C2}
\end{center}
\end{figure}
\subsection{Poorly written R codes}
As demonstrated in the example below, the shoddy R scripts fail to appropriately account for spaces and indents.
<<fdfdf, eval=FALSE>>=
x<-DataSetsUni::data_bank
TTT(x,col=4,lwd=4)
boxplot(x,col=4,lwd=2)
@
\noindent Using Shiny app as shown in the following Figure~\ref{S15_C2}, we can format the R codes
\begin{figure}[H]
\begin{center}
\includegraphics[width=16.5cm,height=14.0cm]{S15_C2}~
\caption{Format your R codes using formatR package, step-3.}\label{S15_C2}
\end{center}
\end{figure}

\noindent Following a Shiny app review, the R code will be automatically more readable and structured, with spaces and indents added as needed (as shown in Figure~\ref{S16_C2}) and provided by

<<fgtrt, eval=FALSE>>=
x <- DataSetsUni::databank
TTT(x, col = 4, lwd = 4)
boxplot(x, col = 4, lwd = 2)
@

\begin{figure}[H]
\begin{center}
\includegraphics[width=16.5cm,height=14.0cm]{S16_C2}~
\caption{Format your R codes using formatR package, step-4.}\label{S16_C2}
\end{center}
\end{figure}
\newpage


\begin{thebibliography}{99}

\bibitem{2Carl20221}
Carl Ganz, Gabor Csardi, Jim Hester, Molly Lewis and
  Rachael Tatman (2022). available: Check if the Title
  of a Package is Available, Appropriate and
  Interesting. R package version 1.1.0.
  https://CRAN.R-project.org/package=available
\bibitem{2Wickham2023}
Wickham, H., and Bryan, J. (2023). R packages. " O'Reilly Media, Inc.".


\bibitem{2Hornik2012}
Hornik, K., Murdoch, D., and Zeileis, A. (2012). Who Did What? The Roles of R Package Authors and How to Refer to Them. The R Journal, 4(1), 64-69.

\bibitem{2Christophe2023}
Christophe Chesneau, Muhammad Imran, M.H Tahir and
  Farrukh Jamal (2023). dprop: Computation of Some
  Important Distributional Properties. R package
  version 0.1.0.
  https://CRAN.R-project.org/package=dprop
  \bibitem{Yihui2023}
  Yihui Xie (2023). formatR: Format R Code
  Automatically. R package version 1.14.
  https://CRAN.R-project.org/package=formatR
\end{thebibliography}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%         Chapter 1              %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Creating R Package: A moderate Level}
\section{Introduction}
This Chapter illustrates the important steps  involving in the development of new R package such as designing R project, package development using RStudio and checking Package name validity, roxygen2 based creation of NAMESPACE file, writing and formatting desire R codes, importing other packages to link with your Package, writing Rd files, writing DESCRIPTION file(package name, title, version, information about the authors and maintainers, a brief explanation of your package, license details, package dependence, suggests, etc.), testing created package for possible errors, warnings and notes, Removing possible errors and importing dependencies, package installation, creating pdf reference manual using devtools, building a source package and finally package submission to CRAN.

\section{Designing a project (Step-1)}
Here, we design a R package that evaluate the ML estimate, the SE of the corresponding estimate and Graphical illustration of PDF, CDF and TTT plot.
\begin{itemize}
\item To evaluate the maximum likelihood estimate.
\item To evaluate the SE of the corresponding estimate.
\item Graphical illustration of PDF.
\item Graphical illustration of CDF.
\item TTT plot of the data
\end{itemize}

\section{Package development using RStudio (Step-2)}
The available~\cite{2Carl2022} R package permits whether the package name you have suggested is already available. The Package name related several things such as verify the availability of a specific package name for usage. On the other hand, it checks the validity of the name, its usage on "GitHub," "CRAN", "Bioconductor", 'Wiktionary' and Wikipedia. To see the package name and its validation on CRAN, run the following commands (see section~\ref{available}).

<<yutyui, eval=FALSE>>=
available::available_on_cran("expm")
available::valid_package_name("expm")
@
\noindent In Chapter~\ref{CH7}, we show the basic steps of creating new package using RStudio. We do repeat the same steps (as shown in Figures.~\ref{S2_C1}--\ref{S12_C1}) to create a new package called "expm". The package is created and saved in a folder "My R Package" in "D" drive. The initially created package "expm" is displayed in Figure~\ref{S2_C5}, with the bottom right corner highlighted in a red rectangle.

\section{NAMESPACE using roxygen2 (Step-3)}
The NAMESPACE file can be created using roxygen2 as shown in Chapter~\ref{CH7}. The same procedure is adopted here for "expm" package and new roxygen2-based NAMESPACE file is created by the repeating the same steps shown from Figures~\ref{S13_C1}--\ref{S23_C1}. Figure~\ref{S3_C5} presents the roxygen2-based NAMESPACE file (generated by roxygen2: do not edit by hand)
\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=15.0cm]{S2_C5}~
\caption{Development of R package.}\label{S2_C5}
\end{center}
\end{figure}


\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=15.0cm]{S3_C5}~
\caption{Creating roxygen2-based NAMESPACE file.}\label{S3_C5}
\end{center}
\end{figure}


\section{Writing desire R codes (Step-4)}

One of the most important thing is to write R codes adequately. So that the users can use your R package and yield correct results. In this Chapter, we emphasis on creating R package (not on writing R codes). The codes are written in R directory as shown in Figure~\ref{S5_C5}.

\subsection{Format your codes}
We write the following R codes to find the ML estimate, the SE of the corresponding estimate and Graphical illustration of PDF, CDF and TTT plot. The following codes are formatted using \textbf{formatR}~\cite{5Yihui2023} R package with the following command
<<vbgftr, eval=FALSE>>=
formatR::tidy_app()
@
\noindent The detailed description are presented in section~\ref{formatR}. The resulting formatted codes are as follows:
<<sderthb>>=
mExp <- function(x, alpha, method = "B") {
    par(mfrow = c(1, 3))
    AdequacyModel::TTT(x, col = 2, lwd = 3)
    pdf_E <- function(par, x) {
        alpha = par[1]
        F = (1 - exp(-alpha * x))
        f = alpha * exp(-alpha * x)
        return(f)
    }
    cdf_E <- function(par, x) {
        alpha = par[1]
        F = (1 - exp(-alpha * x))
        f = alpha * exp(-alpha * x)
        return(F)
    }
    res = suppressWarnings(AdequacyModel::goodness.fit(pdf = pdf_E,
    cdf = cdf_E, starts = c(alpha),
    data = x, method = method, mle = NULL))
    aux = cbind(res$mle, res$Erro)
    colnames(aux) = c("MLE", "SE")
    aux1 = cbind(if (res$Convergence == 0) {
        "Converged"
    } else {
        "Not Converged"
    })
    hist(x, prob = T, main = "", ylab = "pdf")
    curve(pdf_E(res$mle, x), add = T, lty = 1, col = 2, lwd = 4)
    legend("topright", legend = c("E"), col = 2, lty = 1,
    lwd = 2, cex = 0.85)
    plot(ecdf(x), main = "", ylab = "cdf")
    curve(cdf_E(res$mle, x), add = T, lty = 1, col = 2, lwd = 5)
    legend("bottomright", legend = c("E"), col = 2, lty = 1,
     lwd = 2, cex = 0.85)
    list(Estimates = aux, `Convergence Status` = aux1)
}
@
\noindent Paste the above formatted R codes in R directory of your created package and rename the R file as "exponential" . To do this, Go to File and then click on Save as and change the name as "exponential" (see Figure~\ref{S5_C5}), and delete the hello.R file. Here, we import two R packages into the R codes files, the \textbf{AdequacyModel}~\cite{5Pedro2016} and \textbf{graphics}~\cite{5R Core Team2021}.

\subsection{Import other packages to link your Package}

The @imports function is used to link any packages that your package needs but that library() doesn't have to load. The packages to which @import or @importFrom refers should be those from your roxygen2 comments or those whose functions may be called using the \textbf{::} operator.
\subsection{Suggests}
Suggests refers to packages that are being used in examples but are not actually necessary to your package. In other words, the packages listed in Suggests do not require installation with your package, any package listed in Imports will require installation along with your product.

\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=15.0cm]{S5_C5}~
\caption{Writing desire R codes and importing packages in file(as given in top left).}\label{S5_C5}
\end{center}
\end{figure}

\section{Writing Rd files (Step-5)}
The Rd files are important that express R package and its functionalities. It consists of several things and based on these files the users can effectively use and understand the package. These files can be written either using man directory (see section~\ref{Rd file}) or using roxygen2 directly in R directory (see subsection~\ref{roxyen2}). Both methods are discussed in detailed in Chapter~\ref{CH8}. Here, we use man directory to write Rd file of our package "expm". The following Rd file can be amended with respect to our created R package "expm", replace name$\{\text{hello}\}$ with name$\{\text{Exponential distribution}\}$, alias$\{\text{hello}\}$ with alias$\{\text{mExp}\}$, title$\{\text{hello}\}$ with title $\{\text{Evaluates the MLE of exponential distribution}\}$, usage$\{\text{hello}\}$ with usage $\{\text{mExp(x, alpha, method = "B")}\}$ and so on.

\begin{figure}[H]
\begin{center}
\includegraphics[width=17cm,height=16.0cm]{S8_C5}~
\caption{Developing a R package and writing Rd files, step-1.}\label{S8_C5}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[width=17cm,height=16.0cm]{S9_C5}~
\caption{Developing a R package and writing Rd files, step-2.}\label{S9_C5}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[width=17cm,height=16.0cm]{S10_C5}~
\caption{Developing a R package and writing Rd files, step-3.}\label{S10_C5}
\end{center}
\end{figure}

\section{Writing DESCRIPTION file (Step-6)}
The description file contains the package name, title, version, information about the authors and maintainers, a brief explanation of your package, license details, package dependence, suggests, etc. In Chapter~\ref{CH8}, we explain the basic awareness regarding metadata (DESCRIPTION file).

\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=15.0cm]{S11_C5}~
\caption{Completing metadata information in DESCRIPTION file, step-1.}\label{S11_C5}
\end{center}
\end{figure}


\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=16.0cm]{S12_C5}~
\caption{Completing metadata information in DESCRIPTION file, step-2.}\label{S12_C5}
\end{center}
\end{figure}

\section{Testing (Step-7)}

After updating your metadata in DESCRIPTION file (see Figure~\ref{S12_C5}), now its time to test your package for possible errors, warnings and notes. The R CMD check can be performed (CtrL+Shift+E) as shown in Figure~\ref{S13_C5}.

\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=16.0cm]{S13_C5}~
\caption{Examining and testing the created R package to look for any possible notes, warnings, or errors, step-1.}\label{S13_C5}
\end{center}
\end{figure}

\subsection{Removing errors}

The R CMD checks yields a output of all possible errors, warnings and notes as shown in Figure~\ref{S14_C5}. It shows a one possible error regarding package dependencies because our package uses a \textbf{AdequacyModel} that must be mentioned in DESCRIPTION file.
\subsection{Import package dependencies}
We import package \textbf{AdequacyModel} to description file and redo the R CMD checks as shown in Figure~\ref{S15_C5}
\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=14.0cm]{S14_C5}~
\caption{Examining and testing the created R package to look for any possible notes, warnings, or errors, step-2.}\label{S14_C5}
\end{center}
\end{figure}
\noindent In the following Figure~\ref{S15_C5}, we have one error (you have to define the object $x$ in man directory in example, so that the function takes the values of $x$ to further proceeds) and one note (in our R codes, we use curve, hist, legend and par functions all theses should be import from graphics package). One can overcome the following error and note by importing curve, hist, legend and par functions from graphics R package as follows: importFrom graphics curve hist legend par. To do this go to R directory file and place the following function into your R codes file importFrom graphics curve hist legend par as shown in Figure~\ref{S16_C5}. On the other side, be careful imports graphics in to your DESCRIPTION file as we did before for AdequacyModel. After this again redo the R CMD check.
\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=14.0cm]{S15_C5}~
\caption{Examining and testing the created R package to look for any possible notes, warnings, or errors, step-3.}\label{S15_C5}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[width=17cm,height=16.0cm]{S16_C5}~
\caption{Examining and testing the created R package to look for any possible notes, warnings, or errors, step-4.}\label{S16_C5}
\end{center}
\end{figure}
\subsection{Defining object $x$ or supplying the data vector}
In the following Figure~\ref{S17_C5}, one error exists (highlighted in red rectangle) that is object $x$ is not found which refers to providing the data vector. Here, we use DataSetUni R package (which consists of data sets) to define the object $x$. For this, Go to man directory and define $x$ as follows:
<<DataSetsUnicf>>=
x <- DataSetsUni::data_Taxes
@
\noindent and do not forget to import DataSetsUni in DESCRIPTION file as highlighted in red rectangle in Figure~\ref{S19_C5} and again redo the R CMD check examining and testing the created R package to look for any possible notes, warnings, or errors. Finally, the all errors, warnings and notes are vanished and we have created error free package as shown in Figures~\ref{S18_C5}-\ref{S19_C5}.
\begin{figure}[H]
\begin{center}
\includegraphics[width=17cm,height=16.0cm]{S17_C5}~
\caption{R CMD inspection and elimination of possible errors, step-1.}\label{S17_C5}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[width=17cm,height=16.0cm]{S18_C5}~
\caption{R CMD inspection and elimination of possible errors, step-2.}\label{S18_C5}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[width=17cm,height=16.0cm]{S19_C5}~
\caption{R CMD inspection and elimination of possible errors, step-3.}\label{S19_C5}
\end{center}
\end{figure}


\subsection{Installation}
After successfully removing all errors, the next step is to install package in your library and text on different data sets. To this end, Go to install and click on clean and install tab as shown in Figure~\ref{S20_C5}. After installation Go to Package and load it after clicking on "expm" (see Figure~\ref{S21_C5}). Now the created R package "expm" is ready to use.

\begin{figure}[H]
\begin{center}
\includegraphics[width=17cm,height=16.0cm]{S20_C5}~
\caption{Installing the created package after successfully removing all errors, notes and warnings, step-1.}\label{S20_C5}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[width=17cm,height=16.0cm]{S21_C5}~
\caption{Installing the created package after successfully removing all errors, notes and warnings, step-2.}\label{S21_C5}
\end{center}
\end{figure}
\subsection{Output summary}
Once your package has been developed and installed successfully, you can test it repeatedly on various data sets to ensure the consistency and validity of the fitted model as well as its output. Upon running the following command, the output summary is provided by
<<DataSetsUnicfgt>>=
x <- DataSetsUni::data_Taxes
mExp(x, 0.2, method = "B")
@

\noindent where $x$ represents the data vector and the initial parameter value of exponential distribution that is $\alpha=0.2$ with optimization method "B". The ML estimates, SE, TTT plot, fitted PDF and CDF are provided via the output summary. Moreover, the output summary also yields the convergence status that is converged to zero which means the successful completion of the fitted exponential model.


\subsection{Creating PDF reference manual}
The following devtools command can be used to create a PDF documentation for an overview of a created R package:
<<DataSetsUni111, eval=FALSE>>=
devtools::build_manual (pkg = ".", path = NULL)
@

\noindent After run this command as shown in Figure~\ref{S23_C5} and Saving output to the following drive D:/My R Package/expm\_0.0.0.9000.pdf. The created PDF manual as shown in Figure~\ref{S24_C5} depicting error in Authors: c (person ("Muhammad", "Imran", role = c("aut", "cre"), email =
        "imranshakoor84@yahoo.com" )). To overcome this error just replace Author (see Figure~\ref{S25_C5}) with Authors@R c (person ("Muhammad", "Imran", role = c("aut", "cre"), email =
        "imranshakoor84@yahoo.com" )). in DESCRIPTION file.

\begin{figure}[H]
\begin{center}
\includegraphics[width=17cm,height=14.0cm]{S23_C5}~
\caption{Creating PDF reference manual using devtools, step-3.}\label{S23_C5}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[width=20cm,height=10.0cm]{S24_C5}~
\caption{Removing error from DESCRIPTION file, step-1.}\label{S24_C5}
\end{center}
\end{figure}
\begin{figure}[H]
\begin{center}
\includegraphics[width=16cm,height=16.0cm]{S25_C5}~
\caption{Removing error from DESCRIPTION file, step-2.}\label{S25_C5}
\end{center}
\end{figure}
\noindent The following is the three pages pdf reference manual of our created R package "expm". The first is related to metadata based on the DESCRIPTION file.
\includepdf[pages=-,pagecommand={\thispagestyle{plain}}]{expm_1.pdf}

\subsection{Building a source package}

In order to build a source package. To this end, click on Build-$>$ More -$>$ Build Source Package. The procedure of building source package is shown in the following Figures~\ref{S26_C5}--\ref{S27_C5}. The source package can also be build using devtools by executing the following command:
<<devtools, eval=FALSE>>=
devtools::build()
@

\noindent The created source package is saved to the following "D" drive with fodder name "My R Package" D:/My R Package/expm\_0.0.0.9000.tar.gz.

\begin{figure}[H]
\begin{center}
\includegraphics[width=17cm,height=16.0cm]{S26_C5}~
\caption{Building a source package, step-1.}\label{S26_C5}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[width=18cm,height=14.0cm]{S27_C5}~
\caption{Building a source package, step-2.}\label{S27_C5}
\end{center}
\end{figure}

\section{Submission to CRAN (Step-8)}
Finally, its time to submit your error free package to CRAN. The following CRAN link can be used to open the package submission page \textbf{https://cran.r-project.org/submit.html} as shown in Figure~\ref{S28_C5}. In the name tab put maintainer name and in email tab maintainer email as provided in description file. Browse your source file as shown in Figure~\ref{S28_C5} and submit the package. The CRAN team will touch you for further correspondence.

\begin{figure}[H]
\begin{center}
\includegraphics[width=18cm,height=14.0cm]{S28_C5}~
\caption{Submission to CRAN.}\label{S28_C5}
\end{center}
\end{figure}

\begin{thebibliography}{99}


\bibitem{2Carl2022}
Carl Ganz, Gabor Csardi, Jim Hester, Molly Lewis and
  Rachael Tatman (2022). available: Check if the Title
  of a Package is Available, Appropriate and
  Interesting. R package version 1.1.0.
  https://CRAN.R-project.org/package=available

\bibitem{5Yihui2023}
 Yihui Xie (2023). formatR: Format R Code
  Automatically. R package version 1.14.
  https://CRAN.R-project.org/package=formatR

  \bibitem{5Pedro2016}
  Pedro Rafael Diniz Marinho, Marcelo
  Bourguignon and Cicero Rafael Barros Dias
  (2016). AdequacyModel: Adequacy of
  Probabilistic Models and General Purpose
  Optimization. R package version 2.0.0.
  https://CRAN.R-project.org/package=AdequacyModel

   \bibitem{5R Core Team2021}
  R Core Team (2021). R: A language and
  environment for statistical computing. R
  Foundation for Statistical Computing,
  Vienna, Austria. URL
  https://www.R-project.org/.

\end{thebibliography}




































\end{document}








